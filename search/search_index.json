{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Welcome to Promptolution!","text":"<p>Promptolution is a modular library for optimizing prompts for large language models.</p> <p>Promptolution makes advanced prompt tuning techniques available to everybody In an intuitive design, it facilitates the application of prompt optimization techniques both for real-world problems and research projects. The library's modular design ensures extensibility, allowing for seamless integration of new prompt optimizers, LLMs, and tasks. By providing a common framework, it helps standardize prompt tuning approaches, facilitating comparison of different methods and results across studies, and enhancing reproducibility in prompt optimization research.</p>"},{"location":"#features","title":"Features","text":"<ul> <li>Various LLM implementations</li> <li>Optimization algorithms for prompt tuning</li> <li>Task-specific modules</li> <li>Flexible configuration options</li> </ul>"},{"location":"#getting-started","title":"Getting Started","text":"<p>Install via pip:</p> <pre><code>pip install promptolution\n</code></pre> <p>Or clone our GitHub repository:</p> <p>https://github.com/finitearth/promptolution</p>"},{"location":"#api-reference","title":"API Reference","text":"<ul> <li>LLMs</li> <li>Optimizers</li> <li>Predictors</li> <li>Tasks</li> <li>Callbacks</li> <li>Config</li> </ul>"},{"location":"release-notes/","title":"Overview","text":"<p>Welcome to the release notes of Promptolution! Please find the release notes for the corresponding versions of the library in the menu to your left.</p>"},{"location":"api/exemplar_selectors/","title":"Exemplar Selectors","text":"<p>Module for exemplar selectors.</p>"},{"location":"api/exemplar_selectors/#promptolution.exemplar_selectors.base_exemplar_selector","title":"<code>base_exemplar_selector</code>","text":"<p>Base class for exemplar selectors.</p>"},{"location":"api/exemplar_selectors/#promptolution.exemplar_selectors.base_exemplar_selector.BaseExemplarSelector","title":"<code>BaseExemplarSelector</code>","text":"<p>               Bases: <code>ABC</code></p> <p>An abstract base class for exemplar selectors.</p> <p>This class defines the basic interface and common functionality that all exemplar selectors should implement.</p> Source code in <code>promptolution/exemplar_selectors/base_exemplar_selector.py</code> <pre><code>class BaseExemplarSelector(ABC):\n    \"\"\"An abstract base class for exemplar selectors.\n\n    This class defines the basic interface and common functionality\n    that all exemplar selectors should implement.\n    \"\"\"\n\n    def __init__(self, task: \"BaseTask\", predictor: \"BasePredictor\", config: Optional[\"ExperimentConfig\"] = None):\n        \"\"\"Initialize the BaseExemplarSelector.\n\n        Args:\n            task (BaseTask): An object representing the task to be performed.\n            predictor (BasePredictor): An object capable of making predictions based on prompts.\n            config (ExperimentConfig, optional): \"ExperimentConfig\" overwriting the defaults\n        \"\"\"\n        self.task = task\n        self.predictor = predictor\n\n        if config is not None:\n            config.apply_to(self)\n\n    @abstractmethod\n    def select_exemplars(self, prompt: str, n_examples: int = 5) -&gt; str:\n        \"\"\"Select exemplars based on the given prompt.\n\n        Args:\n            prompt (str): The input prompt to base the exemplar selection on.\n            n_examples (int, optional): The number of exemplars to select. Defaults to 5.\n\n        Returns:\n            str: A new prompt that includes the original prompt and the selected exemplars.\n\n        Raises:\n            NotImplementedError: This method should be implemented by subclasses.\n        \"\"\"\n        raise NotImplementedError(\"This method should be implemented by subclasses.\")\n</code></pre>"},{"location":"api/exemplar_selectors/#promptolution.exemplar_selectors.base_exemplar_selector.BaseExemplarSelector.__init__","title":"<code>__init__(task, predictor, config=None)</code>","text":"<p>Initialize the BaseExemplarSelector.</p> <p>Parameters:</p> Name Type Description Default <code>task</code> <code>BaseTask</code> <p>An object representing the task to be performed.</p> required <code>predictor</code> <code>BasePredictor</code> <p>An object capable of making predictions based on prompts.</p> required <code>config</code> <code>ExperimentConfig</code> <p>\"ExperimentConfig\" overwriting the defaults</p> <code>None</code> Source code in <code>promptolution/exemplar_selectors/base_exemplar_selector.py</code> <pre><code>def __init__(self, task: \"BaseTask\", predictor: \"BasePredictor\", config: Optional[\"ExperimentConfig\"] = None):\n    \"\"\"Initialize the BaseExemplarSelector.\n\n    Args:\n        task (BaseTask): An object representing the task to be performed.\n        predictor (BasePredictor): An object capable of making predictions based on prompts.\n        config (ExperimentConfig, optional): \"ExperimentConfig\" overwriting the defaults\n    \"\"\"\n    self.task = task\n    self.predictor = predictor\n\n    if config is not None:\n        config.apply_to(self)\n</code></pre>"},{"location":"api/exemplar_selectors/#promptolution.exemplar_selectors.base_exemplar_selector.BaseExemplarSelector.select_exemplars","title":"<code>select_exemplars(prompt, n_examples=5)</code>  <code>abstractmethod</code>","text":"<p>Select exemplars based on the given prompt.</p> <p>Parameters:</p> Name Type Description Default <code>prompt</code> <code>str</code> <p>The input prompt to base the exemplar selection on.</p> required <code>n_examples</code> <code>int</code> <p>The number of exemplars to select. Defaults to 5.</p> <code>5</code> <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>A new prompt that includes the original prompt and the selected exemplars.</p> <p>Raises:</p> Type Description <code>NotImplementedError</code> <p>This method should be implemented by subclasses.</p> Source code in <code>promptolution/exemplar_selectors/base_exemplar_selector.py</code> <pre><code>@abstractmethod\ndef select_exemplars(self, prompt: str, n_examples: int = 5) -&gt; str:\n    \"\"\"Select exemplars based on the given prompt.\n\n    Args:\n        prompt (str): The input prompt to base the exemplar selection on.\n        n_examples (int, optional): The number of exemplars to select. Defaults to 5.\n\n    Returns:\n        str: A new prompt that includes the original prompt and the selected exemplars.\n\n    Raises:\n        NotImplementedError: This method should be implemented by subclasses.\n    \"\"\"\n    raise NotImplementedError(\"This method should be implemented by subclasses.\")\n</code></pre>"},{"location":"api/exemplar_selectors/#promptolution.exemplar_selectors.random_search_selector","title":"<code>random_search_selector</code>","text":"<p>Random search exemplar selector.</p>"},{"location":"api/exemplar_selectors/#promptolution.exemplar_selectors.random_search_selector.RandomSearchSelector","title":"<code>RandomSearchSelector</code>","text":"<p>               Bases: <code>BaseExemplarSelector</code></p> <p>A selector that uses random search to find the best set of exemplars.</p> <p>This class implements a strategy that generates multiple sets of random examples, evaluates their performance, and selects the best performing set.</p> Source code in <code>promptolution/exemplar_selectors/random_search_selector.py</code> <pre><code>class RandomSearchSelector(BaseExemplarSelector):\n    \"\"\"A selector that uses random search to find the best set of exemplars.\n\n    This class implements a strategy that generates multiple sets of random examples,\n    evaluates their performance, and selects the best performing set.\n    \"\"\"\n\n    def select_exemplars(self, prompt: str, n_trials: int = 5) -&gt; str:\n        \"\"\"Select exemplars using a random search strategy.\n\n        This method generates multiple sets of random examples, evaluates their performance\n        when combined with the original prompt, and returns the best performing set.\n\n        Args:\n            prompt (str): The input prompt to base the exemplar selection on.\n            n_trials (int, optional): The number of random trials to perform. Defaults to 5.\n\n        Returns:\n            str: The best performing prompt, which includes the original prompt and the selected exemplars.\n        \"\"\"\n        best_score = 0.0\n        best_prompt = prompt\n\n        for _ in range(n_trials):\n            _, seq = self.task.evaluate(\n                prompt, self.predictor, eval_strategy=\"subsample\", return_seq=True, return_agg_scores=False\n            )\n            prompt_with_examples = \"\\n\\n\".join([prompt] + [seq[0][0]]) + \"\\n\\n\"\n            # evaluate prompts as few shot prompt\n            score = self.task.evaluate(prompt_with_examples, self.predictor, eval_strategy=\"subsample\")[0]\n            if score &gt; best_score:\n                best_score = score\n                best_prompt = prompt_with_examples\n\n        return best_prompt\n</code></pre>"},{"location":"api/exemplar_selectors/#promptolution.exemplar_selectors.random_search_selector.RandomSearchSelector.select_exemplars","title":"<code>select_exemplars(prompt, n_trials=5)</code>","text":"<p>Select exemplars using a random search strategy.</p> <p>This method generates multiple sets of random examples, evaluates their performance when combined with the original prompt, and returns the best performing set.</p> <p>Parameters:</p> Name Type Description Default <code>prompt</code> <code>str</code> <p>The input prompt to base the exemplar selection on.</p> required <code>n_trials</code> <code>int</code> <p>The number of random trials to perform. Defaults to 5.</p> <code>5</code> <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>The best performing prompt, which includes the original prompt and the selected exemplars.</p> Source code in <code>promptolution/exemplar_selectors/random_search_selector.py</code> <pre><code>def select_exemplars(self, prompt: str, n_trials: int = 5) -&gt; str:\n    \"\"\"Select exemplars using a random search strategy.\n\n    This method generates multiple sets of random examples, evaluates their performance\n    when combined with the original prompt, and returns the best performing set.\n\n    Args:\n        prompt (str): The input prompt to base the exemplar selection on.\n        n_trials (int, optional): The number of random trials to perform. Defaults to 5.\n\n    Returns:\n        str: The best performing prompt, which includes the original prompt and the selected exemplars.\n    \"\"\"\n    best_score = 0.0\n    best_prompt = prompt\n\n    for _ in range(n_trials):\n        _, seq = self.task.evaluate(\n            prompt, self.predictor, eval_strategy=\"subsample\", return_seq=True, return_agg_scores=False\n        )\n        prompt_with_examples = \"\\n\\n\".join([prompt] + [seq[0][0]]) + \"\\n\\n\"\n        # evaluate prompts as few shot prompt\n        score = self.task.evaluate(prompt_with_examples, self.predictor, eval_strategy=\"subsample\")[0]\n        if score &gt; best_score:\n            best_score = score\n            best_prompt = prompt_with_examples\n\n    return best_prompt\n</code></pre>"},{"location":"api/exemplar_selectors/#promptolution.exemplar_selectors.random_selector","title":"<code>random_selector</code>","text":"<p>Random exemplar selector.</p>"},{"location":"api/exemplar_selectors/#promptolution.exemplar_selectors.random_selector.RandomSelector","title":"<code>RandomSelector</code>","text":"<p>               Bases: <code>BaseExemplarSelector</code></p> <p>A selector that randomly selects correct exemplars.</p> <p>This class implements a strategy that generates random examples and selects those that are evaluated as correct until the desired number of exemplars is reached.</p> Source code in <code>promptolution/exemplar_selectors/random_selector.py</code> <pre><code>class RandomSelector(BaseExemplarSelector):\n    \"\"\"A selector that randomly selects correct exemplars.\n\n    This class implements a strategy that generates random examples and selects\n    those that are evaluated as correct until the desired number of exemplars is reached.\n    \"\"\"\n\n    def __init__(\n        self,\n        task: \"BaseTask\",\n        predictor: \"BasePredictor\",\n        desired_score: int = 1,\n        config: Optional[\"ExperimentConfig\"] = None,\n    ) -&gt; None:\n        \"\"\"Initialize the RandomSelector.\n\n        Args:\n            task (BaseTask): An object representing the task to be performed.\n            predictor (BasePredictor): An object capable of making predictions based on prompts.\n            desired_score (int, optional): The desired score for the exemplars. Defaults to 1.\n            config (ExperimentConfig, optional): Configuration for the selector, overriding defaults.\n        \"\"\"\n        self.desired_score = desired_score\n        super().__init__(task, predictor, config)\n\n    def select_exemplars(self, prompt: str, n_examples: int = 5) -&gt; str:\n        \"\"\"Select exemplars using a random selection strategy.\n\n        This method generates random examples and selects those that are evaluated as correct\n        (score == self.desired_score) until the desired number of exemplars is reached.\n\n        Args:\n            prompt (str): The input prompt to base the exemplar selection on.\n            n_examples (int, optional): The number of exemplars to select. Defaults to 5.\n\n        Returns:\n            str: A new prompt that includes the original prompt and the selected exemplars.\n        \"\"\"\n        examples: List[str] = []\n        while len(examples) &lt; n_examples:\n            scores, seqs = self.task.evaluate(\n                prompt, self.predictor, eval_strategy=\"subsample\", return_seq=True, return_agg_scores=False\n            )\n            score = np.mean(scores)\n            seq = seqs[0][0]\n            if score == self.desired_score:\n                examples.append(seq)\n        return \"\\n\\n\".join([prompt] + examples) + \"\\n\\n\"\n</code></pre>"},{"location":"api/exemplar_selectors/#promptolution.exemplar_selectors.random_selector.RandomSelector.__init__","title":"<code>__init__(task, predictor, desired_score=1, config=None)</code>","text":"<p>Initialize the RandomSelector.</p> <p>Parameters:</p> Name Type Description Default <code>task</code> <code>BaseTask</code> <p>An object representing the task to be performed.</p> required <code>predictor</code> <code>BasePredictor</code> <p>An object capable of making predictions based on prompts.</p> required <code>desired_score</code> <code>int</code> <p>The desired score for the exemplars. Defaults to 1.</p> <code>1</code> <code>config</code> <code>ExperimentConfig</code> <p>Configuration for the selector, overriding defaults.</p> <code>None</code> Source code in <code>promptolution/exemplar_selectors/random_selector.py</code> <pre><code>def __init__(\n    self,\n    task: \"BaseTask\",\n    predictor: \"BasePredictor\",\n    desired_score: int = 1,\n    config: Optional[\"ExperimentConfig\"] = None,\n) -&gt; None:\n    \"\"\"Initialize the RandomSelector.\n\n    Args:\n        task (BaseTask): An object representing the task to be performed.\n        predictor (BasePredictor): An object capable of making predictions based on prompts.\n        desired_score (int, optional): The desired score for the exemplars. Defaults to 1.\n        config (ExperimentConfig, optional): Configuration for the selector, overriding defaults.\n    \"\"\"\n    self.desired_score = desired_score\n    super().__init__(task, predictor, config)\n</code></pre>"},{"location":"api/exemplar_selectors/#promptolution.exemplar_selectors.random_selector.RandomSelector.select_exemplars","title":"<code>select_exemplars(prompt, n_examples=5)</code>","text":"<p>Select exemplars using a random selection strategy.</p> <p>This method generates random examples and selects those that are evaluated as correct (score == self.desired_score) until the desired number of exemplars is reached.</p> <p>Parameters:</p> Name Type Description Default <code>prompt</code> <code>str</code> <p>The input prompt to base the exemplar selection on.</p> required <code>n_examples</code> <code>int</code> <p>The number of exemplars to select. Defaults to 5.</p> <code>5</code> <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>A new prompt that includes the original prompt and the selected exemplars.</p> Source code in <code>promptolution/exemplar_selectors/random_selector.py</code> <pre><code>def select_exemplars(self, prompt: str, n_examples: int = 5) -&gt; str:\n    \"\"\"Select exemplars using a random selection strategy.\n\n    This method generates random examples and selects those that are evaluated as correct\n    (score == self.desired_score) until the desired number of exemplars is reached.\n\n    Args:\n        prompt (str): The input prompt to base the exemplar selection on.\n        n_examples (int, optional): The number of exemplars to select. Defaults to 5.\n\n    Returns:\n        str: A new prompt that includes the original prompt and the selected exemplars.\n    \"\"\"\n    examples: List[str] = []\n    while len(examples) &lt; n_examples:\n        scores, seqs = self.task.evaluate(\n            prompt, self.predictor, eval_strategy=\"subsample\", return_seq=True, return_agg_scores=False\n        )\n        score = np.mean(scores)\n        seq = seqs[0][0]\n        if score == self.desired_score:\n            examples.append(seq)\n    return \"\\n\\n\".join([prompt] + examples) + \"\\n\\n\"\n</code></pre>"},{"location":"api/helpers/","title":"Helpers","text":"<p>Helper functions for the usage of the libary.</p>"},{"location":"api/helpers/#promptolution.helpers.get_exemplar_selector","title":"<code>get_exemplar_selector(name, task, predictor)</code>","text":"<p>Factory function to get an exemplar selector based on the given name.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>The name of the exemplar selector to instantiate.</p> required <code>task</code> <code>BaseTask</code> <p>The task object to be passed to the selector.</p> required <code>predictor</code> <code>BasePredictor</code> <p>The predictor object to be passed to the selector.</p> required <p>Returns:</p> Name Type Description <code>BaseExemplarSelector</code> <code>BaseExemplarSelector</code> <p>An instance of the requested exemplar selector.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the requested selector name is not found.</p> Source code in <code>promptolution/helpers.py</code> <pre><code>def get_exemplar_selector(\n    name: Literal[\"random\", \"random_search\"], task: \"BaseTask\", predictor: \"BasePredictor\"\n) -&gt; \"BaseExemplarSelector\":\n    \"\"\"Factory function to get an exemplar selector based on the given name.\n\n    Args:\n        name (str): The name of the exemplar selector to instantiate.\n        task (BaseTask): The task object to be passed to the selector.\n        predictor (BasePredictor): The predictor object to be passed to the selector.\n\n    Returns:\n        BaseExemplarSelector: An instance of the requested exemplar selector.\n\n    Raises:\n        ValueError: If the requested selector name is not found.\n    \"\"\"\n    if name == \"random_search\":\n        return RandomSearchSelector(task, predictor)\n    elif name == \"random\":\n        return RandomSelector(task, predictor)\n    else:\n        raise ValueError(f\"Unknown exemplar selector: {name}\")\n</code></pre>"},{"location":"api/helpers/#promptolution.helpers.get_llm","title":"<code>get_llm(model_id=None, config=None)</code>","text":"<p>Factory function to create and return a language model instance based on the provided model_id.</p> <p>This function supports three types of language models: 1. LocalLLM: For running models locally. 2. VLLM: For running models using the vLLM library. 3. APILLM: For API-based models (default if not matching other types).</p> <p>Parameters:</p> Name Type Description Default <code>model_id</code> <code>str</code> <p>Identifier for the model to use. Special cases:             - \"local-{model_name}\" for LocalLLM             - \"vllm-{model_name}\" for VLLM             - Any other string for APILLM</p> <code>None</code> <code>config</code> <code>ExperimentConfig</code> <p>\"ExperimentConfig\" overwriting defaults.</p> <code>None</code> <p>Returns:</p> Type Description <code>BaseLLM</code> <p>An instance of LocalLLM, or APILLM based on the model_id.</p> Source code in <code>promptolution/helpers.py</code> <pre><code>def get_llm(model_id: Optional[str] = None, config: Optional[\"ExperimentConfig\"] = None) -&gt; \"BaseLLM\":\n    \"\"\"Factory function to create and return a language model instance based on the provided model_id.\n\n    This function supports three types of language models:\n    1. LocalLLM: For running models locally.\n    2. VLLM: For running models using the vLLM library.\n    3. APILLM: For API-based models (default if not matching other types).\n\n    Args:\n        model_id (str): Identifier for the model to use. Special cases:\n                        - \"local-{model_name}\" for LocalLLM\n                        - \"vllm-{model_name}\" for VLLM\n                        - Any other string for APILLM\n        config (ExperimentConfig, optional): \"ExperimentConfig\" overwriting defaults.\n\n    Returns:\n        An instance of LocalLLM, or APILLM based on the model_id.\n    \"\"\"\n    final_model_id = model_id or (config.model_id if config else None)\n    if not final_model_id:\n        raise ValueError(\"model_id must be provided either directly or through config.\")\n\n    if \"local\" in final_model_id:\n        model_name = \"-\".join(final_model_id.split(\"-\")[1:])\n        return LocalLLM(model_name, config=config)\n    if \"vllm\" in final_model_id:\n        model_name = \"-\".join(final_model_id.split(\"-\")[1:])\n        return VLLM(model_name, config=config)\n\n    return APILLM(model_id=final_model_id, config=config)\n</code></pre>"},{"location":"api/helpers/#promptolution.helpers.get_optimizer","title":"<code>get_optimizer(predictor, meta_llm, task, optimizer=None, task_description=None, config=None)</code>","text":"<p>Creates and returns an optimizer instance based on provided parameters.</p> <p>Parameters:</p> Name Type Description Default <code>predictor</code> <code>BasePredictor</code> <p>The predictor used for prompt evaluation</p> required <code>meta_llm</code> <code>BaseLLM</code> <p>The language model used for generating meta-prompts</p> required <code>task</code> <code>BaseTask</code> <p>The task object used for evaluating prompts</p> required <code>optimizer</code> <code>Optional[OptimizerType]</code> <p>String identifying which optimizer to use</p> <code>None</code> <code>meta_prompt</code> <p>Meta prompt text for the optimizer</p> required <code>task_description</code> <code>Optional[str]</code> <p>Description of the task for the optimizer</p> <code>None</code> <code>config</code> <code>Optional[ExperimentConfig]</code> <p>Configuration object with default parameters</p> <code>None</code> <p>Returns:</p> Type Description <code>BaseOptimizer</code> <p>An optimizer instance</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If an unknown optimizer type is specified</p> Source code in <code>promptolution/helpers.py</code> <pre><code>def get_optimizer(\n    predictor: \"BasePredictor\",\n    meta_llm: \"BaseLLM\",\n    task: \"BaseTask\",\n    optimizer: Optional[\"OptimizerType\"] = None,\n    task_description: Optional[str] = None,\n    config: Optional[\"ExperimentConfig\"] = None,\n) -&gt; \"BaseOptimizer\":\n    \"\"\"Creates and returns an optimizer instance based on provided parameters.\n\n    Args:\n        predictor: The predictor used for prompt evaluation\n        meta_llm: The language model used for generating meta-prompts\n        task: The task object used for evaluating prompts\n        optimizer: String identifying which optimizer to use\n        meta_prompt: Meta prompt text for the optimizer\n        task_description: Description of the task for the optimizer\n        config: Configuration object with default parameters\n\n    Returns:\n        An optimizer instance\n\n    Raises:\n        ValueError: If an unknown optimizer type is specified\n    \"\"\"\n    final_optimizer = optimizer or (config.optimizer if config else None)\n    final_task_description = task_description or (config.task_description if config else None)\n\n    if final_optimizer == \"capo\":\n        crossover_template = (\n            CAPO_CROSSOVER_TEMPLATE.replace(\"&lt;task_desc&gt;\", final_task_description)\n            if final_task_description\n            else CAPO_CROSSOVER_TEMPLATE\n        )\n        mutation_template = (\n            CAPO_MUTATION_TEMPLATE.replace(\"&lt;task_desc&gt;\", final_task_description)\n            if final_task_description\n            else CAPO_MUTATION_TEMPLATE\n        )\n\n        return CAPO(\n            predictor=predictor,\n            meta_llm=meta_llm,\n            task=task,\n            crossover_template=crossover_template,\n            mutation_template=mutation_template,\n            config=config,\n        )\n\n    if final_optimizer == \"evopromptde\":\n        template = (\n            EVOPROMPT_DE_TEMPLATE_TD.replace(\"&lt;task_desc&gt;\", final_task_description)\n            if final_task_description\n            else EVOPROMPT_DE_TEMPLATE\n        )\n        return EvoPromptDE(predictor=predictor, meta_llm=meta_llm, task=task, prompt_template=template, config=config)\n\n    if final_optimizer == \"evopromptga\":\n        template = (\n            EVOPROMPT_GA_TEMPLATE_TD.replace(\"&lt;task_desc&gt;\", final_task_description)\n            if final_task_description\n            else EVOPROMPT_GA_TEMPLATE\n        )\n        return EvoPromptGA(predictor=predictor, meta_llm=meta_llm, task=task, prompt_template=template, config=config)\n\n    if final_optimizer == \"opro\":\n        template = (\n            OPRO_TEMPLATE_TD.replace(\"&lt;task_desc&gt;\", final_task_description) if final_task_description else OPRO_TEMPLATE\n        )\n        return OPRO(predictor=predictor, meta_llm=meta_llm, task=task, prompt_template=template, config=config)\n\n    raise ValueError(f\"Unknown optimizer: {final_optimizer}\")\n</code></pre>"},{"location":"api/helpers/#promptolution.helpers.get_predictor","title":"<code>get_predictor(downstream_llm=None, type='marker', *args, **kwargs)</code>","text":"<p>Factory function to create and return a predictor instance.</p> <p>This function supports three types of predictors: 1. FirstOccurrenceClassifier: A predictor that classifies based on first occurrence of the label. 2. MarkerBasedClassifier: A predictor that classifies based on a marker.</p> <p>Parameters:</p> Name Type Description Default <code>downstream_llm</code> <p>The language model to use for prediction.</p> <code>None</code> <code>type</code> <code>Literal['first_occurrence', 'marker']</code> <p>The type of predictor to create:         - \"first_occurrence\" for FirstOccurrenceClassifier         - \"marker\" (default) for MarkerBasedClassifier</p> <code>'marker'</code> <code>*args</code> <p>Variable length argument list passed to the predictor constructor.</p> <code>()</code> <code>**kwargs</code> <p>Arbitrary keyword arguments passed to the predictor constructor.</p> <code>{}</code> <p>Returns:</p> Type Description <code>BasePredictor</code> <p>An instance of FirstOccurrenceClassifier or MarkerBasedClassifier.</p> Source code in <code>promptolution/helpers.py</code> <pre><code>def get_predictor(downstream_llm=None, type: \"PredictorType\" = \"marker\", *args, **kwargs) -&gt; \"BasePredictor\":\n    \"\"\"Factory function to create and return a predictor instance.\n\n    This function supports three types of predictors:\n    1. FirstOccurrenceClassifier: A predictor that classifies based on first occurrence of the label.\n    2. MarkerBasedClassifier: A predictor that classifies based on a marker.\n\n    Args:\n        downstream_llm: The language model to use for prediction.\n        type (Literal[\"first_occurrence\", \"marker\"]): The type of predictor to create:\n                    - \"first_occurrence\" for FirstOccurrenceClassifier\n                    - \"marker\" (default) for MarkerBasedClassifier\n        *args: Variable length argument list passed to the predictor constructor.\n        **kwargs: Arbitrary keyword arguments passed to the predictor constructor.\n\n    Returns:\n        An instance of FirstOccurrenceClassifier or MarkerBasedClassifier.\n    \"\"\"\n    if type == \"first_occurrence\":\n        return FirstOccurrenceClassifier(downstream_llm, *args, **kwargs)\n    elif type == \"marker\":\n        return MarkerBasedClassifier(downstream_llm, *args, **kwargs)\n    else:\n        raise ValueError(f\"Invalid predictor type: '{type}'\")\n</code></pre>"},{"location":"api/helpers/#promptolution.helpers.get_task","title":"<code>get_task(df, config, task_type=None, judge_llm=None, reward_function=None)</code>","text":"<p>Get the task based on the provided DataFrame and configuration.</p> <p>So far only ClassificationTask is supported.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>Input DataFrame containing the data.</p> required <code>config</code> <code>ExperimentConfig</code> <p>Configuration for the experiment.</p> required <p>Returns:</p> Name Type Description <code>BaseTask</code> <code>BaseTask</code> <p>An instance of a task class based on the provided DataFrame and configuration.</p> Source code in <code>promptolution/helpers.py</code> <pre><code>def get_task(\n    df: pd.DataFrame,\n    config: \"ExperimentConfig\",\n    task_type: Optional[\"TaskType\"] = None,\n    judge_llm: Optional[\"BaseLLM\"] = None,\n    reward_function: Optional[Callable] = None,\n) -&gt; \"BaseTask\":\n    \"\"\"Get the task based on the provided DataFrame and configuration.\n\n    So far only ClassificationTask is supported.\n\n    Args:\n        df (pd.DataFrame): Input DataFrame containing the data.\n        config (ExperimentConfig): Configuration for the experiment.\n\n    Returns:\n        BaseTask: An instance of a task class based on the provided DataFrame and configuration.\n    \"\"\"\n    final_task_type = task_type or (config.task_type if config else None)\n\n    if final_task_type == \"reward\":\n        if reward_function is None:\n            reward_function = config.reward_function if config else None\n        assert reward_function is not None, \"Reward function must be provided for reward tasks.\"\n        return RewardTask(\n            df=df,\n            reward_function=reward_function,\n            config=config,\n        )\n    elif final_task_type == \"judge\":\n        assert judge_llm is not None, \"Judge LLM must be provided for judge tasks.\"\n        return JudgeTask(df, judge_llm=judge_llm, config=config)\n\n    return ClassificationTask(df, config=config)\n</code></pre>"},{"location":"api/helpers/#promptolution.helpers.run_evaluation","title":"<code>run_evaluation(df, config, prompts)</code>","text":"<p>Run the evaluation phase of the experiment.</p> <p>Configures all LLMs (downstream, meta, and judge) to use the same instance, that is defined in <code>config.llm</code>.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>Input DataFrame containing the data.</p> required <code>config</code> <code>Config</code> <p>Configuration object for the experiment.</p> required <code>prompts</code> <code>List[str]</code> <p>List of prompts to evaluate.</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>pd.DataFrame: A DataFrame containing the prompts and their scores.</p> Source code in <code>promptolution/helpers.py</code> <pre><code>def run_evaluation(df: pd.DataFrame, config: \"ExperimentConfig\", prompts: List[str]) -&gt; pd.DataFrame:\n    \"\"\"Run the evaluation phase of the experiment.\n\n    Configures all LLMs (downstream, meta, and judge) to use\n    the same instance, that is defined in `config.llm`.\n\n    Args:\n        df (pd.DataFrame): Input DataFrame containing the data.\n        config (Config): Configuration object for the experiment.\n        prompts (List[str]): List of prompts to evaluate.\n\n    Returns:\n        pd.DataFrame: A DataFrame containing the prompts and their scores.\n    \"\"\"\n    llm = get_llm(config=config)\n    task = get_task(df, config, judge_llm=llm)\n    predictor = get_predictor(llm, config=config)\n    logger.warning(\"\ud83d\udcca Starting evaluation...\")\n    scores = task.evaluate(prompts, predictor, eval_strategy=\"full\")\n    df = pd.DataFrame(dict(prompt=prompts, score=scores))\n    df = df.sort_values(\"score\", ascending=False, ignore_index=True)\n\n    return df\n</code></pre>"},{"location":"api/helpers/#promptolution.helpers.run_experiment","title":"<code>run_experiment(df, config)</code>","text":"<p>Run a full experiment based on the provided configuration.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>Input DataFrame containing the data.</p> required <code>config</code> <code>Config</code> <p>Configuration object for the experiment.</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>pd.DataFrame: A DataFrame containing the prompts and their scores.</p> Source code in <code>promptolution/helpers.py</code> <pre><code>def run_experiment(df: pd.DataFrame, config: \"ExperimentConfig\") -&gt; pd.DataFrame:\n    \"\"\"Run a full experiment based on the provided configuration.\n\n    Args:\n        df (pd.DataFrame): Input DataFrame containing the data.\n        config (Config): Configuration object for the experiment.\n\n    Returns:\n        pd.DataFrame: A DataFrame containing the prompts and their scores.\n    \"\"\"\n    # train test split\n    train_df = df.sample(frac=0.8, random_state=42)\n    test_df = df.drop(train_df.index)\n    prompts = run_optimization(train_df, config)\n    df_prompt_scores = run_evaluation(test_df, config, prompts)\n\n    return df_prompt_scores\n</code></pre>"},{"location":"api/helpers/#promptolution.helpers.run_optimization","title":"<code>run_optimization(df, config)</code>","text":"<p>Run the optimization phase of the experiment.</p> <p>Configures all LLMs (downstream, meta, and judge) to use the same instance, that is defined in <code>config.llm</code>.</p> <p>Parameters:</p> Name Type Description Default <code>config</code> <code>Config</code> <p>Configuration object for the experiment.</p> required <p>Returns:</p> Type Description <code>List[str]</code> <p>List[str]: The optimized list of prompts.</p> Source code in <code>promptolution/helpers.py</code> <pre><code>def run_optimization(df: pd.DataFrame, config: \"ExperimentConfig\") -&gt; List[str]:\n    \"\"\"Run the optimization phase of the experiment.\n\n    Configures all LLMs (downstream, meta, and judge) to use\n    the same instance, that is defined in `config.llm`.\n\n    Args:\n        config (Config): Configuration object for the experiment.\n\n    Returns:\n        List[str]: The optimized list of prompts.\n    \"\"\"\n    llm = get_llm(config=config)\n    predictor = get_predictor(llm, config=config)\n\n    config.task_description = (config.task_description or \"\") + \" \" + (predictor.extraction_description or \"\")\n    if config.optimizer == \"capo\" and (config.eval_strategy is None or \"block\" not in config.eval_strategy):\n        logger.warning(\"\ud83d\udccc CAPO requires block evaluation strategy. Setting it to 'sequential_block'.\")\n        config.eval_strategy = \"sequential_block\"\n\n    task = get_task(df, config, judge_llm=llm)\n    optimizer = get_optimizer(\n        predictor=predictor,\n        meta_llm=llm,\n        task=task,\n        config=config,\n    )\n    logger.warning(\"\ud83d\udd25 Starting optimization...\")\n    prompts = optimizer.optimize(n_steps=config.n_steps)\n\n    if hasattr(config, \"prepend_exemplars\") and config.prepend_exemplars:\n        selector = get_exemplar_selector(config.exemplar_selector, task, predictor)\n        prompts = [selector.select_exemplars(p, n_examples=config.n_exemplars) for p in prompts]\n\n    return prompts\n</code></pre>"},{"location":"api/llms/","title":"LLMs","text":"<p>Module for Large Language Models.</p>"},{"location":"api/llms/#promptolution.llms.api_llm","title":"<code>api_llm</code>","text":"<p>Module to interface with various language models through their respective APIs.</p>"},{"location":"api/llms/#promptolution.llms.api_llm.APILLM","title":"<code>APILLM</code>","text":"<p>               Bases: <code>BaseLLM</code></p> <p>A class to interface with language models through their respective APIs.</p> <p>This class provides a unified interface for making API calls to language models using the OpenAI client library. It handles rate limiting through semaphores and supports both synchronous and asynchronous operations.</p> <p>Attributes:</p> Name Type Description <code>model_id</code> <code>str</code> <p>Identifier for the model to use.</p> <code>client</code> <code>AsyncOpenAI</code> <p>The initialized API client.</p> <code>max_tokens</code> <code>int</code> <p>Maximum number of tokens in model responses.</p> <code>semaphore</code> <code>Semaphore</code> <p>Semaphore to limit concurrent API calls.</p> Source code in <code>promptolution/llms/api_llm.py</code> <pre><code>class APILLM(BaseLLM):\n    \"\"\"A class to interface with language models through their respective APIs.\n\n    This class provides a unified interface for making API calls to language models\n    using the OpenAI client library. It handles rate limiting through semaphores\n    and supports both synchronous and asynchronous operations.\n\n    Attributes:\n        model_id (str): Identifier for the model to use.\n        client (AsyncOpenAI): The initialized API client.\n        max_tokens (int): Maximum number of tokens in model responses.\n        semaphore (asyncio.Semaphore): Semaphore to limit concurrent API calls.\n    \"\"\"\n\n    def __init__(\n        self,\n        api_url: Optional[str] = None,\n        model_id: Optional[str] = None,\n        api_key: Optional[str] = None,\n        max_concurrent_calls: int = 50,\n        max_tokens: int = 512,\n        config: Optional[\"ExperimentConfig\"] = None,\n    ) -&gt; None:\n        \"\"\"Initialize the APILLM with a specific model and API configuration.\n\n        Args:\n            api_url (str): The base URL for the API endpoint.\n            model_id (str): Identifier for the model to use.\n            api_key (str, optional): API key for authentication. Defaults to None.\n            max_concurrent_calls (int, optional): Maximum number of concurrent API calls. Defaults to 50.\n            max_tokens (int, optional): Maximum number of tokens in model responses. Defaults to 512.\n            config (ExperimentConfig, optional): Configuration for the LLM, overriding defaults.\n\n        Raises:\n            ImportError: If required libraries are not installed.\n        \"\"\"\n        if not import_successful:\n            raise ImportError(\n                \"Could not import at least one of the required libraries: openai, asyncio. \"\n                \"Please ensure they are installed in your environment.\"\n            )\n\n        self.api_url = api_url\n        self.model_id = model_id\n        self.api_key = api_key\n        self.max_concurrent_calls = max_concurrent_calls\n        self.max_tokens = max_tokens\n\n        super().__init__(config=config)\n        self.client = AsyncOpenAI(base_url=self.api_url, api_key=self.api_key)\n        self.semaphore = asyncio.Semaphore(self.max_concurrent_calls)\n\n    def _get_response(self, prompts: List[str], system_prompts: List[str]) -&gt; List[str]:\n        # Setup for async execution in sync context\n        try:\n            loop = asyncio.get_running_loop()\n        except RuntimeError:  # 'get_running_loop' raises a RuntimeError if there is no running loop\n            loop = asyncio.new_event_loop()\n            asyncio.set_event_loop(loop)\n\n        responses = loop.run_until_complete(self._get_response_async(prompts, system_prompts))\n        return responses\n\n    async def _get_response_async(self, prompts: List[str], system_prompts: List[str]) -&gt; List[str]:\n        assert self.model_id is not None, \"model_id must be set\"\n        tasks = [\n            _invoke_model(prompt, system_prompt, self.max_tokens, self.model_id, self.client, self.semaphore)\n            for prompt, system_prompt in zip(prompts, system_prompts)\n        ]\n        messages = await asyncio.gather(*tasks)\n        responses = []\n        for message in messages:\n            response = message.choices[0].message.content\n            if response is None:\n                raise ValueError(\"Received None response from the API.\")\n            responses.append(response)\n        return responses\n</code></pre>"},{"location":"api/llms/#promptolution.llms.api_llm.APILLM.__init__","title":"<code>__init__(api_url=None, model_id=None, api_key=None, max_concurrent_calls=50, max_tokens=512, config=None)</code>","text":"<p>Initialize the APILLM with a specific model and API configuration.</p> <p>Parameters:</p> Name Type Description Default <code>api_url</code> <code>str</code> <p>The base URL for the API endpoint.</p> <code>None</code> <code>model_id</code> <code>str</code> <p>Identifier for the model to use.</p> <code>None</code> <code>api_key</code> <code>str</code> <p>API key for authentication. Defaults to None.</p> <code>None</code> <code>max_concurrent_calls</code> <code>int</code> <p>Maximum number of concurrent API calls. Defaults to 50.</p> <code>50</code> <code>max_tokens</code> <code>int</code> <p>Maximum number of tokens in model responses. Defaults to 512.</p> <code>512</code> <code>config</code> <code>ExperimentConfig</code> <p>Configuration for the LLM, overriding defaults.</p> <code>None</code> <p>Raises:</p> Type Description <code>ImportError</code> <p>If required libraries are not installed.</p> Source code in <code>promptolution/llms/api_llm.py</code> <pre><code>def __init__(\n    self,\n    api_url: Optional[str] = None,\n    model_id: Optional[str] = None,\n    api_key: Optional[str] = None,\n    max_concurrent_calls: int = 50,\n    max_tokens: int = 512,\n    config: Optional[\"ExperimentConfig\"] = None,\n) -&gt; None:\n    \"\"\"Initialize the APILLM with a specific model and API configuration.\n\n    Args:\n        api_url (str): The base URL for the API endpoint.\n        model_id (str): Identifier for the model to use.\n        api_key (str, optional): API key for authentication. Defaults to None.\n        max_concurrent_calls (int, optional): Maximum number of concurrent API calls. Defaults to 50.\n        max_tokens (int, optional): Maximum number of tokens in model responses. Defaults to 512.\n        config (ExperimentConfig, optional): Configuration for the LLM, overriding defaults.\n\n    Raises:\n        ImportError: If required libraries are not installed.\n    \"\"\"\n    if not import_successful:\n        raise ImportError(\n            \"Could not import at least one of the required libraries: openai, asyncio. \"\n            \"Please ensure they are installed in your environment.\"\n        )\n\n    self.api_url = api_url\n    self.model_id = model_id\n    self.api_key = api_key\n    self.max_concurrent_calls = max_concurrent_calls\n    self.max_tokens = max_tokens\n\n    super().__init__(config=config)\n    self.client = AsyncOpenAI(base_url=self.api_url, api_key=self.api_key)\n    self.semaphore = asyncio.Semaphore(self.max_concurrent_calls)\n</code></pre>"},{"location":"api/llms/#promptolution.llms.base_llm","title":"<code>base_llm</code>","text":"<p>Base module for LLMs in the promptolution library.</p>"},{"location":"api/llms/#promptolution.llms.base_llm.BaseLLM","title":"<code>BaseLLM</code>","text":"<p>               Bases: <code>ABC</code></p> <p>Abstract base class for Language Models in the promptolution library.</p> <p>This class defines the interface that all concrete LLM implementations should follow. It's designed to track which configuration parameters are actually used.</p> <p>Attributes:</p> Name Type Description <code>config</code> <code>LLMModelConfig</code> <p>Configuration for the language model.</p> <code>input_token_count</code> <code>int</code> <p>Count of input tokens processed.</p> <code>output_token_count</code> <code>int</code> <p>Count of output tokens generated.</p> <code>tokenizer</code> <code>Optional[PreTrainedTokenizer]</code> <p>The tokenizer for the model.</p> Source code in <code>promptolution/llms/base_llm.py</code> <pre><code>class BaseLLM(ABC):\n    \"\"\"Abstract base class for Language Models in the promptolution library.\n\n    This class defines the interface that all concrete LLM implementations should follow.\n    It's designed to track which configuration parameters are actually used.\n\n    Attributes:\n        config (LLMModelConfig): Configuration for the language model.\n        input_token_count (int): Count of input tokens processed.\n        output_token_count (int): Count of output tokens generated.\n        tokenizer (Optional[PreTrainedTokenizer]): The tokenizer for the model.\n    \"\"\"\n\n    def __init__(self, config: Optional[\"ExperimentConfig\"] = None):\n        \"\"\"Initialize the LLM with a configuration or direct parameters.\n\n        This constructor supports both config-based and direct parameter initialization\n        for backward compatibility.\n\n        Args:\n            config (ExperimentConfig, optional): Configuration for the LLM, overriding defaults.\n        \"\"\"\n        if config is not None:\n            config.apply_to(self)\n        # Initialize token counters\n        self.input_token_count = 0\n        self.output_token_count = 0\n        self.tokenizer: Optional[PreTrainedTokenizer] = None\n\n    def get_token_count(self) -&gt; Dict[str, int]:\n        \"\"\"Get the current count of input and output tokens.\n\n        Returns:\n            dict: A dictionary containing the input and output token counts.\n        \"\"\"\n        return {\n            \"input_tokens\": self.input_token_count,\n            \"output_tokens\": self.output_token_count,\n            \"total_tokens\": self.input_token_count + self.output_token_count,\n        }\n\n    def reset_token_count(self) -&gt; None:\n        \"\"\"Reset the token counters to zero.\"\"\"\n        self.input_token_count = 0\n        self.output_token_count = 0\n\n    def update_token_count(self, inputs: List[str], outputs: List[str]) -&gt; None:\n        \"\"\"Update the token count based on the given inputs and outputs.\n\n        It uses a simple tokenization method (splitting by whitespace) to count tokens in the base class.\n\n        Args:\n            inputs (List[str]): A list of input prompts.\n            outputs (List[str]): A list of generated responses.\n        \"\"\"\n        input_tokens = sum([len(i.split()) for i in inputs])\n        output_tokens = sum([len(o.split()) for o in outputs])\n        self.input_token_count += input_tokens\n        self.output_token_count += output_tokens\n\n    def get_response(\n        self, prompts: Union[str, List[str]], system_prompts: Optional[Union[str, List[str]]] = None\n    ) -&gt; List[str]:\n        \"\"\"Generate responses for the given prompts.\n\n        This method calls the _get_response method to generate responses\n        for the given prompts. It also updates the token count for the\n        input and output tokens.\n\n        Args:\n            prompts (str or List[str]): Input prompt(s). If a single string is provided,\n                                        it's converted to a list containing that string.\n            system_prompts (Optional, str or List[str]): System prompt(s) to provide context to the model.\n\n        Returns:\n            List[str]: A list of generated responses, one for each input prompt.\n        \"\"\"\n        if system_prompts is None:\n            system_prompts = DEFAULT_SYS_PROMPT\n        if isinstance(prompts, str):\n            prompts = [prompts]\n        if isinstance(system_prompts, str):\n            system_prompts = [system_prompts] * len(prompts)\n        responses = self._get_response(prompts, system_prompts)\n        self.update_token_count(prompts + system_prompts, responses)\n\n        return responses\n\n    def set_generation_seed(self, seed: int) -&gt; None:\n        \"\"\"Set the random seed for reproducibility per request.\n\n        Args:\n            seed (int): Random seed value.\n        \"\"\"\n        pass\n\n    @abstractmethod\n    def _get_response(self, prompts: List[str], system_prompts: List[str]) -&gt; List[str]:\n        \"\"\"Generate responses for the given prompts.\n\n        This method should be implemented by subclasses to define how\n        the LLM generates responses.\n\n        Args:\n            prompts (List[str]): A list of input prompts.\n            system_prompts (List[str]): A list of system prompts to provide context to the model.\n\n        Returns:\n            List[str]: A list of generated responses corresponding to the input prompts.\n        \"\"\"\n        raise NotImplementedError\n</code></pre>"},{"location":"api/llms/#promptolution.llms.base_llm.BaseLLM.__init__","title":"<code>__init__(config=None)</code>","text":"<p>Initialize the LLM with a configuration or direct parameters.</p> <p>This constructor supports both config-based and direct parameter initialization for backward compatibility.</p> <p>Parameters:</p> Name Type Description Default <code>config</code> <code>ExperimentConfig</code> <p>Configuration for the LLM, overriding defaults.</p> <code>None</code> Source code in <code>promptolution/llms/base_llm.py</code> <pre><code>def __init__(self, config: Optional[\"ExperimentConfig\"] = None):\n    \"\"\"Initialize the LLM with a configuration or direct parameters.\n\n    This constructor supports both config-based and direct parameter initialization\n    for backward compatibility.\n\n    Args:\n        config (ExperimentConfig, optional): Configuration for the LLM, overriding defaults.\n    \"\"\"\n    if config is not None:\n        config.apply_to(self)\n    # Initialize token counters\n    self.input_token_count = 0\n    self.output_token_count = 0\n    self.tokenizer: Optional[PreTrainedTokenizer] = None\n</code></pre>"},{"location":"api/llms/#promptolution.llms.base_llm.BaseLLM.get_response","title":"<code>get_response(prompts, system_prompts=None)</code>","text":"<p>Generate responses for the given prompts.</p> <p>This method calls the _get_response method to generate responses for the given prompts. It also updates the token count for the input and output tokens.</p> <p>Parameters:</p> Name Type Description Default <code>prompts</code> <code>str or List[str]</code> <p>Input prompt(s). If a single string is provided,                         it's converted to a list containing that string.</p> required <code>system_prompts</code> <code>(Optional, str or List[str])</code> <p>System prompt(s) to provide context to the model.</p> <code>None</code> <p>Returns:</p> Type Description <code>List[str]</code> <p>List[str]: A list of generated responses, one for each input prompt.</p> Source code in <code>promptolution/llms/base_llm.py</code> <pre><code>def get_response(\n    self, prompts: Union[str, List[str]], system_prompts: Optional[Union[str, List[str]]] = None\n) -&gt; List[str]:\n    \"\"\"Generate responses for the given prompts.\n\n    This method calls the _get_response method to generate responses\n    for the given prompts. It also updates the token count for the\n    input and output tokens.\n\n    Args:\n        prompts (str or List[str]): Input prompt(s). If a single string is provided,\n                                    it's converted to a list containing that string.\n        system_prompts (Optional, str or List[str]): System prompt(s) to provide context to the model.\n\n    Returns:\n        List[str]: A list of generated responses, one for each input prompt.\n    \"\"\"\n    if system_prompts is None:\n        system_prompts = DEFAULT_SYS_PROMPT\n    if isinstance(prompts, str):\n        prompts = [prompts]\n    if isinstance(system_prompts, str):\n        system_prompts = [system_prompts] * len(prompts)\n    responses = self._get_response(prompts, system_prompts)\n    self.update_token_count(prompts + system_prompts, responses)\n\n    return responses\n</code></pre>"},{"location":"api/llms/#promptolution.llms.base_llm.BaseLLM.get_token_count","title":"<code>get_token_count()</code>","text":"<p>Get the current count of input and output tokens.</p> <p>Returns:</p> Name Type Description <code>dict</code> <code>Dict[str, int]</code> <p>A dictionary containing the input and output token counts.</p> Source code in <code>promptolution/llms/base_llm.py</code> <pre><code>def get_token_count(self) -&gt; Dict[str, int]:\n    \"\"\"Get the current count of input and output tokens.\n\n    Returns:\n        dict: A dictionary containing the input and output token counts.\n    \"\"\"\n    return {\n        \"input_tokens\": self.input_token_count,\n        \"output_tokens\": self.output_token_count,\n        \"total_tokens\": self.input_token_count + self.output_token_count,\n    }\n</code></pre>"},{"location":"api/llms/#promptolution.llms.base_llm.BaseLLM.reset_token_count","title":"<code>reset_token_count()</code>","text":"<p>Reset the token counters to zero.</p> Source code in <code>promptolution/llms/base_llm.py</code> <pre><code>def reset_token_count(self) -&gt; None:\n    \"\"\"Reset the token counters to zero.\"\"\"\n    self.input_token_count = 0\n    self.output_token_count = 0\n</code></pre>"},{"location":"api/llms/#promptolution.llms.base_llm.BaseLLM.set_generation_seed","title":"<code>set_generation_seed(seed)</code>","text":"<p>Set the random seed for reproducibility per request.</p> <p>Parameters:</p> Name Type Description Default <code>seed</code> <code>int</code> <p>Random seed value.</p> required Source code in <code>promptolution/llms/base_llm.py</code> <pre><code>def set_generation_seed(self, seed: int) -&gt; None:\n    \"\"\"Set the random seed for reproducibility per request.\n\n    Args:\n        seed (int): Random seed value.\n    \"\"\"\n    pass\n</code></pre>"},{"location":"api/llms/#promptolution.llms.base_llm.BaseLLM.update_token_count","title":"<code>update_token_count(inputs, outputs)</code>","text":"<p>Update the token count based on the given inputs and outputs.</p> <p>It uses a simple tokenization method (splitting by whitespace) to count tokens in the base class.</p> <p>Parameters:</p> Name Type Description Default <code>inputs</code> <code>List[str]</code> <p>A list of input prompts.</p> required <code>outputs</code> <code>List[str]</code> <p>A list of generated responses.</p> required Source code in <code>promptolution/llms/base_llm.py</code> <pre><code>def update_token_count(self, inputs: List[str], outputs: List[str]) -&gt; None:\n    \"\"\"Update the token count based on the given inputs and outputs.\n\n    It uses a simple tokenization method (splitting by whitespace) to count tokens in the base class.\n\n    Args:\n        inputs (List[str]): A list of input prompts.\n        outputs (List[str]): A list of generated responses.\n    \"\"\"\n    input_tokens = sum([len(i.split()) for i in inputs])\n    output_tokens = sum([len(o.split()) for o in outputs])\n    self.input_token_count += input_tokens\n    self.output_token_count += output_tokens\n</code></pre>"},{"location":"api/llms/#promptolution.llms.local_llm","title":"<code>local_llm</code>","text":"<p>Module for running LLMs locally using the Hugging Face Transformers library.</p>"},{"location":"api/llms/#promptolution.llms.local_llm.LocalLLM","title":"<code>LocalLLM</code>","text":"<p>               Bases: <code>BaseLLM</code></p> <p>A class for running language models locally using the Hugging Face Transformers library.</p> <p>This class sets up a text generation pipeline with specified model parameters and provides a method to generate responses for given prompts.</p> <p>Attributes:</p> Name Type Description <code>pipeline</code> <code>Pipeline</code> <p>The text generation pipeline.</p> <p>Methods:</p> Name Description <code>get_response</code> <p>Generate responses for a list of prompts.</p> Source code in <code>promptolution/llms/local_llm.py</code> <pre><code>class LocalLLM(BaseLLM):\n    \"\"\"A class for running language models locally using the Hugging Face Transformers library.\n\n    This class sets up a text generation pipeline with specified model parameters\n    and provides a method to generate responses for given prompts.\n\n    Attributes:\n        pipeline (transformers.Pipeline): The text generation pipeline.\n\n    Methods:\n        get_response: Generate responses for a list of prompts.\n    \"\"\"\n\n    def __init__(self, model_id: str, batch_size: int = 8, config: Optional[\"ExperimentConfig\"] = None) -&gt; None:\n        \"\"\"Initialize the LocalLLM with a specific model.\n\n        Args:\n            model_id (str): The identifier of the model to use (e.g., \"gpt2\", \"facebook/opt-1.3b\").\n            batch_size (int, optional): The batch size for text generation. Defaults to 8.\n            config (ExperimentConfig, optional): \"ExperimentConfig\" overwriting defaults.\n\n        Note:\n            This method sets up a text generation pipeline with bfloat16 precision,\n            automatic device mapping, and specific generation parameters.\n        \"\"\"\n        if not imports_successful:\n            raise ImportError(\n                \"Could not import at least one of the required libraries: torch, transformers. \"\n                \"Please ensure they are installed in your environment.\"\n            )\n        self.pipeline: Pipeline = pipeline(\n            \"text-generation\",\n            model=model_id,\n            model_kwargs={\"torch_dtype\": torch.bfloat16},\n            device_map=\"auto\",\n            max_new_tokens=256,\n            batch_size=batch_size,\n            num_return_sequences=1,\n            return_full_text=False,\n        )\n        super().__init__(config)\n        self.tokenizer = self.pipeline.tokenizer\n        assert self.tokenizer is not None, \"Tokenizer must be initialized.\"\n        self.eos_token_id = self.tokenizer.eos_token_id\n        self.tokenizer.pad_token_id = self.eos_token_id\n        self.tokenizer.padding_side = \"left\"\n\n    def _get_response(self, prompts: List[str], system_prompts: List[str]) -&gt; List[str]:\n        \"\"\"Generate responses for a list of prompts using the local language model.\n\n        Args:\n            prompts (list[str]): A list of input prompts.\n            system_prompts (list[str]): A list of system prompts to guide the model's behavior.\n\n        Returns:\n            list[str]: A list of generated responses corresponding to the input prompts.\n\n        Note:\n            This method uses torch.no_grad() for inference to reduce memory usage.\n            It handles both single and batch inputs, ensuring consistent output format.\n        \"\"\"\n        inputs: List[List[Dict[str, str]]] = []\n        for prompt, sys_prompt in zip(prompts, system_prompts):\n            inputs.append([{\"role\": \"system\", \"prompt\": sys_prompt}, {\"role\": \"user\", \"prompt\": prompt}])\n\n        with torch.no_grad():\n            response = self.pipeline(inputs, pad_token_id=self.eos_token_id)\n\n        if len(response) != 1:\n            response = [r[0] if isinstance(r, list) else r for r in response]\n\n        response = [r[\"generated_text\"] for r in response]\n        return response\n\n    def __del__(self) -&gt; None:\n        \"\"\"Cleanup method to delete the pipeline and free up GPU memory.\"\"\"\n        if hasattr(self, \"pipeline\"):\n            del self.pipeline\n        if \"torch\" in globals() and hasattr(torch, \"cuda\") and torch.cuda.is_available():\n            torch.cuda.empty_cache()\n</code></pre>"},{"location":"api/llms/#promptolution.llms.local_llm.LocalLLM.__del__","title":"<code>__del__()</code>","text":"<p>Cleanup method to delete the pipeline and free up GPU memory.</p> Source code in <code>promptolution/llms/local_llm.py</code> <pre><code>def __del__(self) -&gt; None:\n    \"\"\"Cleanup method to delete the pipeline and free up GPU memory.\"\"\"\n    if hasattr(self, \"pipeline\"):\n        del self.pipeline\n    if \"torch\" in globals() and hasattr(torch, \"cuda\") and torch.cuda.is_available():\n        torch.cuda.empty_cache()\n</code></pre>"},{"location":"api/llms/#promptolution.llms.local_llm.LocalLLM.__init__","title":"<code>__init__(model_id, batch_size=8, config=None)</code>","text":"<p>Initialize the LocalLLM with a specific model.</p> <p>Parameters:</p> Name Type Description Default <code>model_id</code> <code>str</code> <p>The identifier of the model to use (e.g., \"gpt2\", \"facebook/opt-1.3b\").</p> required <code>batch_size</code> <code>int</code> <p>The batch size for text generation. Defaults to 8.</p> <code>8</code> <code>config</code> <code>ExperimentConfig</code> <p>\"ExperimentConfig\" overwriting defaults.</p> <code>None</code> Note <p>This method sets up a text generation pipeline with bfloat16 precision, automatic device mapping, and specific generation parameters.</p> Source code in <code>promptolution/llms/local_llm.py</code> <pre><code>def __init__(self, model_id: str, batch_size: int = 8, config: Optional[\"ExperimentConfig\"] = None) -&gt; None:\n    \"\"\"Initialize the LocalLLM with a specific model.\n\n    Args:\n        model_id (str): The identifier of the model to use (e.g., \"gpt2\", \"facebook/opt-1.3b\").\n        batch_size (int, optional): The batch size for text generation. Defaults to 8.\n        config (ExperimentConfig, optional): \"ExperimentConfig\" overwriting defaults.\n\n    Note:\n        This method sets up a text generation pipeline with bfloat16 precision,\n        automatic device mapping, and specific generation parameters.\n    \"\"\"\n    if not imports_successful:\n        raise ImportError(\n            \"Could not import at least one of the required libraries: torch, transformers. \"\n            \"Please ensure they are installed in your environment.\"\n        )\n    self.pipeline: Pipeline = pipeline(\n        \"text-generation\",\n        model=model_id,\n        model_kwargs={\"torch_dtype\": torch.bfloat16},\n        device_map=\"auto\",\n        max_new_tokens=256,\n        batch_size=batch_size,\n        num_return_sequences=1,\n        return_full_text=False,\n    )\n    super().__init__(config)\n    self.tokenizer = self.pipeline.tokenizer\n    assert self.tokenizer is not None, \"Tokenizer must be initialized.\"\n    self.eos_token_id = self.tokenizer.eos_token_id\n    self.tokenizer.pad_token_id = self.eos_token_id\n    self.tokenizer.padding_side = \"left\"\n</code></pre>"},{"location":"api/llms/#promptolution.llms.vllm","title":"<code>vllm</code>","text":"<p>Module for running language models locally using the vLLM library.</p>"},{"location":"api/llms/#promptolution.llms.vllm.VLLM","title":"<code>VLLM</code>","text":"<p>               Bases: <code>BaseLLM</code></p> <p>A class for running language models using the vLLM library.</p> <p>This class sets up a vLLM inference engine with specified model parameters and provides a method to generate responses for given prompts.</p> <p>Attributes:</p> Name Type Description <code>llm</code> <code>LLM</code> <p>The vLLM inference engine.</p> <code>tokenizer</code> <code>PreTrainedTokenizer</code> <p>The tokenizer for the model.</p> <code>sampling_params</code> <code>SamplingParams</code> <p>Parameters for text generation.</p> <p>Methods:</p> Name Description <code>get_response</code> <p>Generate responses for a list of prompts.</p> <code>update_token_count</code> <p>Update the token count based on the given inputs and outputs.</p> Source code in <code>promptolution/llms/vllm.py</code> <pre><code>class VLLM(BaseLLM):\n    \"\"\"A class for running language models using the vLLM library.\n\n    This class sets up a vLLM inference engine with specified model parameters\n    and provides a method to generate responses for given prompts.\n\n    Attributes:\n        llm (vllm.LLM): The vLLM inference engine.\n        tokenizer (PreTrainedTokenizer): The tokenizer for the model.\n        sampling_params (vllm.SamplingParams): Parameters for text generation.\n\n    Methods:\n        get_response: Generate responses for a list of prompts.\n        update_token_count: Update the token count based on the given inputs and outputs.\n    \"\"\"\n\n    tokenizer: PreTrainedTokenizer\n\n    def __init__(\n        self,\n        model_id: str,\n        batch_size: Optional[int] = None,\n        max_generated_tokens: int = 256,\n        temperature: float = 0.1,\n        top_p: float = 0.9,\n        model_storage_path: Optional[str] = None,\n        dtype: str = \"auto\",\n        tensor_parallel_size: int = 1,\n        gpu_memory_utilization: float = 0.95,\n        max_model_len: int = 2048,\n        trust_remote_code: bool = False,\n        seed: int = 42,\n        llm_kwargs: Optional[Dict[str, Any]] = None,\n        config: Optional[\"ExperimentConfig\"] = None,\n    ) -&gt; None:\n        \"\"\"Initialize the VLLM with a specific model.\n\n        Args:\n            model_id (str): The identifier of the model to use.\n            batch_size (int, optional): The batch size for text generation. Defaults to 8.\n            max_generated_tokens (int, optional): Maximum number of tokens to generate. Defaults to 256.\n            temperature (float, optional): Sampling temperature. Defaults to 0.1.\n            top_p (float, optional): Top-p sampling parameter. Defaults to 0.9.\n            model_storage_path (str, optional): Directory to store the model. Defaults to None.\n            dtype (str, optional): Data type for model weights. Defaults to \"float16\".\n            tensor_parallel_size (int, optional): Number of GPUs for tensor parallelism. Defaults to 1.\n            gpu_memory_utilization (float, optional): Fraction of GPU memory to use. Defaults to 0.95.\n            max_model_len (int, optional): Maximum sequence length for the model. Defaults to 2048.\n            trust_remote_code (bool, optional): Whether to trust remote code. Defaults to False.\n            seed (int, optional): Random seed for the model. Defaults to 42.\n            llm_kwargs (dict, optional): Additional keyword arguments for the LLM. Defaults to None.\n            config (ExperimentConfig, optional): Configuration for the LLM, overriding defaults.\n\n        Note:\n            This method sets up a vLLM engine with specified parameters for efficient inference.\n        \"\"\"\n        if not imports_successful:\n            raise ImportError(\n                \"Could not import at least one of the required libraries: transformers, vllm. \"\n                \"Please ensure they are installed in your environment.\"\n            )\n\n        self.dtype = dtype\n        self.tensor_parallel_size = tensor_parallel_size\n        self.gpu_memory_utilization = gpu_memory_utilization\n        self.max_model_len = max_model_len\n        self.trust_remote_code = trust_remote_code\n\n        super().__init__(config)\n\n        # Configure sampling parameters\n        self.sampling_params = SamplingParams(\n            temperature=temperature, top_p=top_p, max_tokens=max_generated_tokens, seed=seed\n        )\n\n        llm_kwargs = llm_kwargs or {}\n        # Initialize the vLLM engine with both explicit parameters and any additional kwargs\n        llm_params: Dict[str, Any] = {\n            \"model\": model_id,\n            \"tokenizer\": model_id,\n            \"dtype\": self.dtype,\n            \"tensor_parallel_size\": self.tensor_parallel_size,\n            \"gpu_memory_utilization\": self.gpu_memory_utilization,\n            \"max_model_len\": self.max_model_len,\n            \"download_dir\": model_storage_path,\n            \"trust_remote_code\": self.trust_remote_code,\n            \"seed\": seed,\n            **llm_kwargs,\n        }\n\n        self.llm = LLM(**llm_params)\n\n        # Initialize tokenizer separately for potential pre-processing\n        self.tokenizer = AutoTokenizer.from_pretrained(model_id)\n\n        if batch_size is None:\n            cache_config = self.llm.llm_engine.model_executor.cache_config\n            if (\n                cache_config.num_gpu_blocks is not None\n                and cache_config.block_size is not None\n                and self.max_model_len is not None\n            ):\n                self.batch_size = int(\n                    (cache_config.num_gpu_blocks * cache_config.block_size / self.max_model_len) * 0.95\n                )\n                logger.info(f\"\ud83d\ude80 Batch size set to {self.batch_size} based on GPU memory.\")\n            else:\n                self.batch_size = 1\n                logger.warning(\"\u26a0\ufe0f Could not determine batch size from GPU memory. Using batch size of 1.\")\n        else:\n            self.batch_size = batch_size\n\n    def _get_response(self, prompts: List[str], system_prompts: List[str]) -&gt; List[str]:\n        \"\"\"Generate responses for a list of prompts using the vLLM engine.\n\n        Args:\n            prompts (list[str]): A list of input prompts.\n            system_prompts (list[str]): A list of system prompts to guide the model's behavior.\n\n        Returns:\n            list[str]: A list of generated responses corresponding to the input prompts.\n\n        Note:\n            This method uses vLLM's batched generation capabilities for efficient inference.\n            It also counts input and output tokens.\n        \"\"\"\n        prompts = [\n            str(\n                self.tokenizer.apply_chat_template(\n                    [\n                        {\n                            \"role\": \"system\",\n                            \"content\": sys_prompt,\n                        },\n                        {\"role\": \"user\", \"content\": prompt},\n                    ],\n                    tokenize=False,\n                    add_generation_prompt=True,\n                )\n            )\n            for prompt, sys_prompt in zip(prompts, system_prompts)\n        ]\n\n        # generate responses for self.batch_size prompts at the same time\n        all_responses = []\n        for i in range(0, len(prompts), self.batch_size):\n            batch = prompts[i : i + self.batch_size]\n            outputs = self.llm.generate(batch, self.sampling_params)\n            responses = [output.outputs[0].text for output in outputs]\n\n            all_responses.extend(responses)\n\n        return all_responses\n\n    def update_token_count(self, inputs: List[str], outputs: List[str]) -&gt; None:\n        \"\"\"Update the token count based on the given inputs and outputs.\n\n            Uses the tokenizer to count the tokens.\n\n        Args:\n            inputs (List[str]): A list of input prompts.\n            outputs (List[str]): A list of generated responses.\n        \"\"\"\n        for input in inputs:\n            self.input_token_count += len(self.tokenizer.encode(input))\n\n        for output in outputs:\n            self.output_token_count += len(self.tokenizer.encode(output))\n\n    def set_generation_seed(self, seed: int) -&gt; None:\n        \"\"\"Set the random seed for text generation.\n\n        Args:\n            seed (int): Random seed for text generation.\n        \"\"\"\n        self.sampling_params.seed = seed\n</code></pre>"},{"location":"api/llms/#promptolution.llms.vllm.VLLM.__init__","title":"<code>__init__(model_id, batch_size=None, max_generated_tokens=256, temperature=0.1, top_p=0.9, model_storage_path=None, dtype='auto', tensor_parallel_size=1, gpu_memory_utilization=0.95, max_model_len=2048, trust_remote_code=False, seed=42, llm_kwargs=None, config=None)</code>","text":"<p>Initialize the VLLM with a specific model.</p> <p>Parameters:</p> Name Type Description Default <code>model_id</code> <code>str</code> <p>The identifier of the model to use.</p> required <code>batch_size</code> <code>int</code> <p>The batch size for text generation. Defaults to 8.</p> <code>None</code> <code>max_generated_tokens</code> <code>int</code> <p>Maximum number of tokens to generate. Defaults to 256.</p> <code>256</code> <code>temperature</code> <code>float</code> <p>Sampling temperature. Defaults to 0.1.</p> <code>0.1</code> <code>top_p</code> <code>float</code> <p>Top-p sampling parameter. Defaults to 0.9.</p> <code>0.9</code> <code>model_storage_path</code> <code>str</code> <p>Directory to store the model. Defaults to None.</p> <code>None</code> <code>dtype</code> <code>str</code> <p>Data type for model weights. Defaults to \"float16\".</p> <code>'auto'</code> <code>tensor_parallel_size</code> <code>int</code> <p>Number of GPUs for tensor parallelism. Defaults to 1.</p> <code>1</code> <code>gpu_memory_utilization</code> <code>float</code> <p>Fraction of GPU memory to use. Defaults to 0.95.</p> <code>0.95</code> <code>max_model_len</code> <code>int</code> <p>Maximum sequence length for the model. Defaults to 2048.</p> <code>2048</code> <code>trust_remote_code</code> <code>bool</code> <p>Whether to trust remote code. Defaults to False.</p> <code>False</code> <code>seed</code> <code>int</code> <p>Random seed for the model. Defaults to 42.</p> <code>42</code> <code>llm_kwargs</code> <code>dict</code> <p>Additional keyword arguments for the LLM. Defaults to None.</p> <code>None</code> <code>config</code> <code>ExperimentConfig</code> <p>Configuration for the LLM, overriding defaults.</p> <code>None</code> Note <p>This method sets up a vLLM engine with specified parameters for efficient inference.</p> Source code in <code>promptolution/llms/vllm.py</code> <pre><code>def __init__(\n    self,\n    model_id: str,\n    batch_size: Optional[int] = None,\n    max_generated_tokens: int = 256,\n    temperature: float = 0.1,\n    top_p: float = 0.9,\n    model_storage_path: Optional[str] = None,\n    dtype: str = \"auto\",\n    tensor_parallel_size: int = 1,\n    gpu_memory_utilization: float = 0.95,\n    max_model_len: int = 2048,\n    trust_remote_code: bool = False,\n    seed: int = 42,\n    llm_kwargs: Optional[Dict[str, Any]] = None,\n    config: Optional[\"ExperimentConfig\"] = None,\n) -&gt; None:\n    \"\"\"Initialize the VLLM with a specific model.\n\n    Args:\n        model_id (str): The identifier of the model to use.\n        batch_size (int, optional): The batch size for text generation. Defaults to 8.\n        max_generated_tokens (int, optional): Maximum number of tokens to generate. Defaults to 256.\n        temperature (float, optional): Sampling temperature. Defaults to 0.1.\n        top_p (float, optional): Top-p sampling parameter. Defaults to 0.9.\n        model_storage_path (str, optional): Directory to store the model. Defaults to None.\n        dtype (str, optional): Data type for model weights. Defaults to \"float16\".\n        tensor_parallel_size (int, optional): Number of GPUs for tensor parallelism. Defaults to 1.\n        gpu_memory_utilization (float, optional): Fraction of GPU memory to use. Defaults to 0.95.\n        max_model_len (int, optional): Maximum sequence length for the model. Defaults to 2048.\n        trust_remote_code (bool, optional): Whether to trust remote code. Defaults to False.\n        seed (int, optional): Random seed for the model. Defaults to 42.\n        llm_kwargs (dict, optional): Additional keyword arguments for the LLM. Defaults to None.\n        config (ExperimentConfig, optional): Configuration for the LLM, overriding defaults.\n\n    Note:\n        This method sets up a vLLM engine with specified parameters for efficient inference.\n    \"\"\"\n    if not imports_successful:\n        raise ImportError(\n            \"Could not import at least one of the required libraries: transformers, vllm. \"\n            \"Please ensure they are installed in your environment.\"\n        )\n\n    self.dtype = dtype\n    self.tensor_parallel_size = tensor_parallel_size\n    self.gpu_memory_utilization = gpu_memory_utilization\n    self.max_model_len = max_model_len\n    self.trust_remote_code = trust_remote_code\n\n    super().__init__(config)\n\n    # Configure sampling parameters\n    self.sampling_params = SamplingParams(\n        temperature=temperature, top_p=top_p, max_tokens=max_generated_tokens, seed=seed\n    )\n\n    llm_kwargs = llm_kwargs or {}\n    # Initialize the vLLM engine with both explicit parameters and any additional kwargs\n    llm_params: Dict[str, Any] = {\n        \"model\": model_id,\n        \"tokenizer\": model_id,\n        \"dtype\": self.dtype,\n        \"tensor_parallel_size\": self.tensor_parallel_size,\n        \"gpu_memory_utilization\": self.gpu_memory_utilization,\n        \"max_model_len\": self.max_model_len,\n        \"download_dir\": model_storage_path,\n        \"trust_remote_code\": self.trust_remote_code,\n        \"seed\": seed,\n        **llm_kwargs,\n    }\n\n    self.llm = LLM(**llm_params)\n\n    # Initialize tokenizer separately for potential pre-processing\n    self.tokenizer = AutoTokenizer.from_pretrained(model_id)\n\n    if batch_size is None:\n        cache_config = self.llm.llm_engine.model_executor.cache_config\n        if (\n            cache_config.num_gpu_blocks is not None\n            and cache_config.block_size is not None\n            and self.max_model_len is not None\n        ):\n            self.batch_size = int(\n                (cache_config.num_gpu_blocks * cache_config.block_size / self.max_model_len) * 0.95\n            )\n            logger.info(f\"\ud83d\ude80 Batch size set to {self.batch_size} based on GPU memory.\")\n        else:\n            self.batch_size = 1\n            logger.warning(\"\u26a0\ufe0f Could not determine batch size from GPU memory. Using batch size of 1.\")\n    else:\n        self.batch_size = batch_size\n</code></pre>"},{"location":"api/llms/#promptolution.llms.vllm.VLLM.set_generation_seed","title":"<code>set_generation_seed(seed)</code>","text":"<p>Set the random seed for text generation.</p> <p>Parameters:</p> Name Type Description Default <code>seed</code> <code>int</code> <p>Random seed for text generation.</p> required Source code in <code>promptolution/llms/vllm.py</code> <pre><code>def set_generation_seed(self, seed: int) -&gt; None:\n    \"\"\"Set the random seed for text generation.\n\n    Args:\n        seed (int): Random seed for text generation.\n    \"\"\"\n    self.sampling_params.seed = seed\n</code></pre>"},{"location":"api/llms/#promptolution.llms.vllm.VLLM.update_token_count","title":"<code>update_token_count(inputs, outputs)</code>","text":"<p>Update the token count based on the given inputs and outputs.</p> <pre><code>Uses the tokenizer to count the tokens.\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>inputs</code> <code>List[str]</code> <p>A list of input prompts.</p> required <code>outputs</code> <code>List[str]</code> <p>A list of generated responses.</p> required Source code in <code>promptolution/llms/vllm.py</code> <pre><code>def update_token_count(self, inputs: List[str], outputs: List[str]) -&gt; None:\n    \"\"\"Update the token count based on the given inputs and outputs.\n\n        Uses the tokenizer to count the tokens.\n\n    Args:\n        inputs (List[str]): A list of input prompts.\n        outputs (List[str]): A list of generated responses.\n    \"\"\"\n    for input in inputs:\n        self.input_token_count += len(self.tokenizer.encode(input))\n\n    for output in outputs:\n        self.output_token_count += len(self.tokenizer.encode(output))\n</code></pre>"},{"location":"api/optimizers/","title":"Optimizers","text":"<p>Module for prompt optimizers.</p>"},{"location":"api/optimizers/#promptolution.optimizers.base_optimizer","title":"<code>base_optimizer</code>","text":"<p>Base module for optimizers in the promptolution library.</p>"},{"location":"api/optimizers/#promptolution.optimizers.base_optimizer.BaseOptimizer","title":"<code>BaseOptimizer</code>","text":"<p>               Bases: <code>ABC</code></p> <p>Abstract base class for prompt optimizers.</p> <p>This class defines the basic structure and interface for prompt optimization algorithms.</p> <p>Attributes:</p> Name Type Description <code>config</code> <code>ExperimentConfig</code> <p>Configuration for the optimizer, overriding defaults.</p> <code>prompts</code> <code>List[str]</code> <p>List of current prompts being optimized.</p> <code>task</code> <code>BaseTask</code> <p>The task object for evaluating prompts.</p> <code>callbacks</code> <code>List[Callable]</code> <p>List of callback functions to be called during optimization.</p> <code>predictor</code> <p>The predictor used for prompt evaluation (if applicable).</p> Source code in <code>promptolution/optimizers/base_optimizer.py</code> <pre><code>class BaseOptimizer(ABC):\n    \"\"\"Abstract base class for prompt optimizers.\n\n    This class defines the basic structure and interface for prompt optimization algorithms.\n\n    Attributes:\n        config (ExperimentConfig, optional): Configuration for the optimizer, overriding defaults.\n        prompts (List[str]): List of current prompts being optimized.\n        task (BaseTask): The task object for evaluating prompts.\n        callbacks (List[Callable]): List of callback functions to be called during optimization.\n        predictor: The predictor used for prompt evaluation (if applicable).\n    \"\"\"\n\n    def __init__(\n        self,\n        predictor: \"BasePredictor\",\n        task: \"BaseTask\",\n        initial_prompts: Optional[List[str]] = None,\n        callbacks: Optional[List[\"BaseCallback\"]] = None,\n        config: Optional[\"ExperimentConfig\"] = None,\n    ) -&gt; None:\n        \"\"\"Initialize the optimizer with a configuration and/or direct parameters.\n\n        Args:\n            task: Task object for prompt evaluation.\n            predictor: Predictor for prompt evaluation.\n            initial_prompts: Initial set of prompts to start optimization with.\n            callbacks: List of callback functions.\n            config (ExperimentConfig, optional): Configuration for the optimizer, overriding defaults.\n        \"\"\"\n        # Set up optimizer state\n        self.prompts: List[str] = initial_prompts or []\n        self.task = task\n        self.callbacks: List[\"BaseCallback\"] = callbacks or []\n        self.predictor = predictor\n        self.scores: List[float] = []\n\n        if config is not None:\n            config.apply_to(self)\n\n        self.config = config\n\n    def optimize(self, n_steps: int) -&gt; List[str]:\n        \"\"\"Perform the optimization process.\n\n        This method should be implemented by concrete optimizer classes to define\n        the specific optimization algorithm.\n\n        Args:\n            n_steps (int): Number of optimization steps to perform.\n\n        Returns:\n            The optimized list of prompts after all steps.\n        \"\"\"\n        # validate config\n        if self.config is not None:\n            self.config.validate()\n        self._pre_optimization_loop()\n\n        for _ in range(n_steps):\n            try:\n                self.prompts = self._step()\n            except Exception as e:\n                # exit training loop and gracefully fail\n                logger.error(f\"\u26d4 Error during optimization step: {e}\")\n                logger.error(\"\u26a0\ufe0f Exiting optimization loop.\")\n                break\n\n            # Callbacks at the end of each step\n            continue_optimization = self._on_step_end()\n            if not continue_optimization:\n                break\n\n        self._on_train_end()\n\n        return self.prompts\n\n    @abstractmethod\n    def _pre_optimization_loop(self) -&gt; None:\n        \"\"\"Prepare for the optimization loop.\n\n        This method should be implemented by concrete optimizer classes to define\n        any setup required before the optimization loop starts.\n        \"\"\"\n        pass\n\n    @abstractmethod\n    def _step(self) -&gt; List[str]:\n        \"\"\"Perform a single optimization step.\n\n        This method should be implemented by concrete optimizer classes to define\n        the specific optimization step.\n\n        Returns:\n            The optimized list of prompts after the step.\n        \"\"\"\n        pass\n\n    def _on_step_end(self) -&gt; bool:\n        \"\"\"Call all registered callbacks at the end of each optimization step.\"\"\"\n        continue_optimization = True\n        for callback in self.callbacks:\n            if not callback.on_step_end(self):\n                continue_optimization = False\n\n        return continue_optimization\n\n    def _on_train_end(self) -&gt; None:\n        \"\"\"Call all registered callbacks at the end of the entire optimization process.\"\"\"\n        for callback in self.callbacks:\n            callback.on_train_end(self)\n</code></pre>"},{"location":"api/optimizers/#promptolution.optimizers.base_optimizer.BaseOptimizer.__init__","title":"<code>__init__(predictor, task, initial_prompts=None, callbacks=None, config=None)</code>","text":"<p>Initialize the optimizer with a configuration and/or direct parameters.</p> <p>Parameters:</p> Name Type Description Default <code>task</code> <code>BaseTask</code> <p>Task object for prompt evaluation.</p> required <code>predictor</code> <code>BasePredictor</code> <p>Predictor for prompt evaluation.</p> required <code>initial_prompts</code> <code>Optional[List[str]]</code> <p>Initial set of prompts to start optimization with.</p> <code>None</code> <code>callbacks</code> <code>Optional[List[BaseCallback]]</code> <p>List of callback functions.</p> <code>None</code> <code>config</code> <code>ExperimentConfig</code> <p>Configuration for the optimizer, overriding defaults.</p> <code>None</code> Source code in <code>promptolution/optimizers/base_optimizer.py</code> <pre><code>def __init__(\n    self,\n    predictor: \"BasePredictor\",\n    task: \"BaseTask\",\n    initial_prompts: Optional[List[str]] = None,\n    callbacks: Optional[List[\"BaseCallback\"]] = None,\n    config: Optional[\"ExperimentConfig\"] = None,\n) -&gt; None:\n    \"\"\"Initialize the optimizer with a configuration and/or direct parameters.\n\n    Args:\n        task: Task object for prompt evaluation.\n        predictor: Predictor for prompt evaluation.\n        initial_prompts: Initial set of prompts to start optimization with.\n        callbacks: List of callback functions.\n        config (ExperimentConfig, optional): Configuration for the optimizer, overriding defaults.\n    \"\"\"\n    # Set up optimizer state\n    self.prompts: List[str] = initial_prompts or []\n    self.task = task\n    self.callbacks: List[\"BaseCallback\"] = callbacks or []\n    self.predictor = predictor\n    self.scores: List[float] = []\n\n    if config is not None:\n        config.apply_to(self)\n\n    self.config = config\n</code></pre>"},{"location":"api/optimizers/#promptolution.optimizers.base_optimizer.BaseOptimizer.optimize","title":"<code>optimize(n_steps)</code>","text":"<p>Perform the optimization process.</p> <p>This method should be implemented by concrete optimizer classes to define the specific optimization algorithm.</p> <p>Parameters:</p> Name Type Description Default <code>n_steps</code> <code>int</code> <p>Number of optimization steps to perform.</p> required <p>Returns:</p> Type Description <code>List[str]</code> <p>The optimized list of prompts after all steps.</p> Source code in <code>promptolution/optimizers/base_optimizer.py</code> <pre><code>def optimize(self, n_steps: int) -&gt; List[str]:\n    \"\"\"Perform the optimization process.\n\n    This method should be implemented by concrete optimizer classes to define\n    the specific optimization algorithm.\n\n    Args:\n        n_steps (int): Number of optimization steps to perform.\n\n    Returns:\n        The optimized list of prompts after all steps.\n    \"\"\"\n    # validate config\n    if self.config is not None:\n        self.config.validate()\n    self._pre_optimization_loop()\n\n    for _ in range(n_steps):\n        try:\n            self.prompts = self._step()\n        except Exception as e:\n            # exit training loop and gracefully fail\n            logger.error(f\"\u26d4 Error during optimization step: {e}\")\n            logger.error(\"\u26a0\ufe0f Exiting optimization loop.\")\n            break\n\n        # Callbacks at the end of each step\n        continue_optimization = self._on_step_end()\n        if not continue_optimization:\n            break\n\n    self._on_train_end()\n\n    return self.prompts\n</code></pre>"},{"location":"api/optimizers/#promptolution.optimizers.capo","title":"<code>capo</code>","text":"<p>Implementation of the CAPO (Cost-Aware Prompt Optimization) algorithm.</p>"},{"location":"api/optimizers/#promptolution.optimizers.capo.CAPO","title":"<code>CAPO</code>","text":"<p>               Bases: <code>BaseOptimizer</code></p> <p>CAPO: Cost-Aware Prompt Optimization.</p> <p>This class implements an evolutionary algorithm for optimizing prompts in large language models by incorporating racing techniques and multi-objective optimization. It uses crossover, mutation, and racing based on evaluation scores and statistical tests to improve efficiency while balancing performance with prompt length. It is adapted from the paper \"CAPO: Cost-Aware Prompt Optimization\" by Zehle et al., 2025.</p> Source code in <code>promptolution/optimizers/capo.py</code> <pre><code>class CAPO(BaseOptimizer):\n    \"\"\"CAPO: Cost-Aware Prompt Optimization.\n\n    This class implements an evolutionary algorithm for optimizing prompts in large language models\n    by incorporating racing techniques and multi-objective optimization. It uses crossover, mutation,\n    and racing based on evaluation scores and statistical tests to improve efficiency while balancing\n    performance with prompt length. It is adapted from the paper \"CAPO: Cost-Aware Prompt Optimization\" by Zehle et al., 2025.\n    \"\"\"\n\n    def __init__(\n        self,\n        predictor: \"BasePredictor\",\n        task: \"BaseTask\",\n        meta_llm: \"BaseLLM\",\n        initial_prompts: Optional[List[str]] = None,\n        crossovers_per_iter: int = 4,\n        upper_shots: int = 5,\n        max_n_blocks_eval: int = 10,\n        test_statistic: \"TestStatistics\" = \"paired_t_test\",\n        alpha: float = 0.2,\n        length_penalty: float = 0.05,\n        check_fs_accuracy: bool = True,\n        create_fs_reasoning: bool = True,\n        df_few_shots: Optional[pd.DataFrame] = None,\n        crossover_template: Optional[str] = None,\n        mutation_template: Optional[str] = None,\n        callbacks: Optional[List[\"BaseCallback\"]] = None,\n        config: Optional[\"ExperimentConfig\"] = None,\n    ) -&gt; None:\n        \"\"\"Initializes the CAPOptimizer with various parameters for prompt evolution.\n\n        Args:\n            predictor (BasePredictor): The predictor for evaluating prompt performance.\n            task (BaseTask): The task instance containing dataset and description.\n            meta_llm (BaseLLM): The meta language model for crossover/mutation.\n            initial_prompts (List[str]): Initial prompt instructions.\n            crossovers_per_iter (int): Number of crossover operations per iteration.\n            upper_shots (int): Maximum number of few-shot examples per prompt.\n            p_few_shot_reasoning (float): Probability of generating llm-reasoning for few-shot examples, instead of simply using input-output pairs.\n            max_n_blocks_eval (int): Maximum number of evaluation blocks.\n            test_statistic (TestStatistics): Statistical test to compare prompt performance. Default is \"paired_t_test\".\n            alpha (float): Significance level for the statistical test.\n            length_penalty (float): Penalty factor for prompt length.\n            check_fs_accuracy (bool): Whether to check the accuracy of few-shot examples before appending them to the prompt.\n                In cases such as reward tasks, this can be set to False, as no ground truth is available. Default is True.\n            create_fs_reasoning (bool): Whether to create reasoning for few-shot examples using the downstream model,\n                instead of simply using input-output pairs from the few shots DataFrame. Default is True.\n            df_few_shots (pd.DataFrame): DataFrame containing few-shot examples. If None, will pop 10% of datapoints from task.\n            crossover_template (str, optional): Template for crossover instructions.\n            mutation_template (str, optional): Template for mutation instructions.\n            callbacks (List[Callable], optional): Callbacks for optimizer events.\n            config (ExperimentConfig, optional): Configuration for the optimizer.\n        \"\"\"\n        self.meta_llm = meta_llm\n        self.downstream_llm = predictor.llm\n\n        self.crossover_template = crossover_template or CAPO_CROSSOVER_TEMPLATE\n        self.mutation_template = mutation_template or CAPO_MUTATION_TEMPLATE\n\n        self.crossovers_per_iter = crossovers_per_iter\n        self.upper_shots = upper_shots\n        self.max_n_blocks_eval = max_n_blocks_eval\n        self.test_statistic = get_test_statistic_func(test_statistic)\n        self.alpha = alpha\n\n        self.length_penalty = length_penalty\n        self.token_counter = get_token_counter(self.downstream_llm)\n\n        self.check_fs_accuracy = check_fs_accuracy\n        self.create_fs_reasoning = create_fs_reasoning\n\n        self.scores: List[float] = []\n        super().__init__(predictor, task, initial_prompts, callbacks, config)\n        self.df_few_shots = df_few_shots if df_few_shots is not None else task.pop_datapoints(frac=0.1)\n        if self.max_n_blocks_eval &gt; self.task.n_blocks:\n            logger.warning(\n                f\"\u2139\ufe0f max_n_blocks_eval ({self.max_n_blocks_eval}) is larger than the number of blocks ({self.task.n_blocks}).\"\n                f\" Setting max_n_blocks_eval to {self.task.n_blocks}.\"\n            )\n            self.max_n_blocks_eval = self.task.n_blocks\n        self.population_size = len(self.prompts)\n\n        if hasattr(self.predictor, \"begin_marker\") and hasattr(self.predictor, \"end_marker\"):\n            self.target_begin_marker = self.predictor.begin_marker\n            self.target_end_marker = self.predictor.end_marker\n        else:\n            self.target_begin_marker = \"\"\n            self.target_end_marker = \"\"\n\n    def _initialize_population(self, initial_prompts: List[str]) -&gt; List[CAPOPrompt]:\n        \"\"\"Initializes the population of Prompt objects from initial instructions.\n\n        Args:\n            initial_prompts (List[str]): List of initial prompt instructions.\n\n        Returns:\n            List[Prompt]: Initialized population of prompts with few-shot examples.\n        \"\"\"\n        population = []\n        for instruction_text in initial_prompts:\n            num_examples = random.randint(0, self.upper_shots)\n            few_shots = self._create_few_shot_examples(instruction_text, num_examples)\n            population.append(CAPOPrompt(instruction_text, few_shots))\n\n        return population\n\n    def _create_few_shot_examples(self, instruction: str, num_examples: int) -&gt; List[str]:\n        if num_examples == 0:\n            return []\n\n        few_shot_samples = self.df_few_shots.sample(num_examples, replace=False)\n        sample_inputs = few_shot_samples[self.task.x_column].values.astype(str)\n        sample_targets = few_shot_samples[self.task.y_column].values\n        few_shots = [\n            CAPO_FEWSHOT_TEMPLATE.replace(\"&lt;input&gt;\", i).replace(\n                \"&lt;output&gt;\", f\"{self.target_begin_marker}{t}{self.target_end_marker}\"\n            )\n            for i, t in zip(sample_inputs, sample_targets)\n        ]\n\n        if not self.create_fs_reasoning:\n            # If we do not create reasoning, return the few-shot examples directly\n            return few_shots\n\n        preds, seqs = self.predictor.predict(\n            [instruction] * num_examples,\n            list(sample_inputs),\n            return_seq=True,\n        )\n        if isinstance(seqs, str):\n            seqs = [seqs]\n        if isinstance(preds, str):\n            preds = [preds]\n\n        # Check which predictions are correct and get a single one per example\n        for j in range(num_examples):\n            # Process and clean up the generated sequences\n            seqs[j] = seqs[j].replace(sample_inputs[j], \"\").strip()\n            # Check if the prediction is correct and add reasoning if so\n            if preds[j] == sample_targets[j] or not self.check_fs_accuracy:\n                few_shots[j] = CAPO_FEWSHOT_TEMPLATE.replace(\"&lt;input&gt;\", sample_inputs[j]).replace(\"&lt;output&gt;\", seqs[j])\n\n        return few_shots\n\n    def _crossover(self, parents: List[CAPOPrompt]) -&gt; List[CAPOPrompt]:\n        \"\"\"Performs crossover among parent prompts to generate offsprings.\n\n        Args:\n            parents (List[CAPOPrompt]): List of parent prompts.\n\n        Returns:\n            List[Prompt]: List of new offsprings after crossover.\n        \"\"\"\n        crossover_prompts = []\n        offspring_few_shots = []\n        for _ in range(self.crossovers_per_iter):\n            mother, father = random.sample(parents, 2)\n            crossover_prompt = (\n                self.crossover_template.replace(\"&lt;mother&gt;\", mother.instruction_text)\n                .replace(\"&lt;father&gt;\", father.instruction_text)\n                .strip()\n            )\n            # collect all crossover prompts then pass them bundled to the meta llm (speedup)\n            crossover_prompts.append(crossover_prompt)\n            combined_few_shots = mother.few_shots + father.few_shots\n            num_few_shots = (len(mother.few_shots) + len(father.few_shots)) // 2\n            offspring_few_shot = random.sample(combined_few_shots, num_few_shots) if combined_few_shots else []\n            offspring_few_shots.append(offspring_few_shot)\n\n        child_instructions = self.meta_llm.get_response(crossover_prompts)\n\n        offsprings = []\n        for instruction, examples in zip(child_instructions, offspring_few_shots):\n            instruction = extract_from_tag(instruction, \"&lt;prompt&gt;\", \"&lt;/prompt&gt;\")\n            offsprings.append(CAPOPrompt(instruction, examples))\n\n        return offsprings\n\n    def _mutate(self, offsprings: List[CAPOPrompt]) -&gt; List[CAPOPrompt]:\n        \"\"\"Apply mutation to offsprings to generate new candidate prompts.\n\n        Args:\n            offsprings (List[CAPOPrompt]): List of offsprings to mutate.\n\n        Returns:\n            List[Prompt]: List of mutated prompts.\n        \"\"\"\n        # collect all mutation prompts then pass them bundled to the meta llm (speedup)\n        mutation_prompts = [\n            self.mutation_template.replace(\"&lt;instruction&gt;\", prompt.instruction_text) for prompt in offsprings\n        ]\n        new_instructions = self.meta_llm.get_response(mutation_prompts)\n\n        mutated = []\n        for new_instruction, prompt in zip(new_instructions, offsprings):\n            new_instruction = extract_from_tag(new_instruction, \"&lt;prompt&gt;\", \"&lt;/prompt&gt;\")\n            p = random.random()\n\n            new_few_shots: List[str]\n            if p &lt; 1 / 3 and len(prompt.few_shots) &lt; self.upper_shots:  # add a random few shot\n                new_few_shot = self._create_few_shot_examples(new_instruction, 1)\n                new_few_shots = prompt.few_shots + new_few_shot\n            elif 1 / 3 &lt;= p &lt; 2 / 3 and len(prompt.few_shots) &gt; 0:  # remove a random few shot\n                new_few_shots = random.sample(prompt.few_shots, len(prompt.few_shots) - 1)\n            else:  # do not change few shots, but shuffle\n                new_few_shots = prompt.few_shots\n\n            random.shuffle(new_few_shots)\n            mutated.append(CAPOPrompt(new_instruction, new_few_shots))\n\n        return mutated\n\n    def _do_racing(self, candidates: List[CAPOPrompt], k: int) -&gt; List[CAPOPrompt]:\n        \"\"\"Perform the racing (selection) phase by comparing candidates based on their evaluation scores using the provided test statistic.\n\n        Args:\n            candidates (List[CAPOPrompt]): List of candidate prompts.\n            k (int): Number of survivors to retain.\n\n        Returns:\n            List[Prompt]: List of surviving prompts after racing.\n        \"\"\"\n        self.task.reset_block_idx()\n        block_scores: List[List[float]] = []\n        i = 0\n        while len(candidates) &gt; k and i &lt; self.max_n_blocks_eval:\n            # new_scores shape: (n_candidates, n_samples)\n            new_scores: List[float] = self.task.evaluate(\n                [c.construct_prompt() for c in candidates], self.predictor, return_agg_scores=False\n            )\n\n            # subtract length penalty\n            prompt_lengths = np.array([self.token_counter(c.construct_prompt()) for c in candidates])\n            rel_prompt_lengths = prompt_lengths / self.max_prompt_length\n\n            penalized_new_scores = np.array(new_scores) - self.length_penalty * rel_prompt_lengths[:, None]\n\n            new_scores = penalized_new_scores.tolist()\n\n            block_scores.append(new_scores)\n            scores = np.concatenate(block_scores, axis=1)\n\n            # boolean matrix C_ij indicating if candidate j is better than candidate i\n            comparison_matrix = np.array(\n                [[self.test_statistic(other_score, score, self.alpha) for other_score in scores] for score in scores]\n            )\n\n            # Sum along rows to get number of better scores for each candidate\n            n_better = np.sum(comparison_matrix, axis=1)\n\n            # Create mask for survivors and filter candidates\n            survivor_mask = n_better &lt; k\n            candidates = list(compress(candidates, survivor_mask))\n            block_scores = list(compress(block_scores, survivor_mask))\n\n            i += 1\n            self.task.increment_block_idx()\n\n        avg_scores = self.task.evaluate(\n            [c.construct_prompt() for c in candidates], self.predictor, eval_strategy=\"evaluated\"\n        )\n        order = np.argsort(-np.array(avg_scores))[:k]\n        candidates = [candidates[i] for i in order]\n        self.scores = [avg_scores[i] for i in order]\n\n        return candidates\n\n    def _pre_optimization_loop(self) -&gt; None:\n        self.prompt_objects = self._initialize_population(self.prompts)\n        self.prompts = [p.construct_prompt() for p in self.prompt_objects]\n        self.max_prompt_length = max(self.token_counter(p) for p in self.prompts) if self.prompts else 1\n        self.task.reset_block_idx()\n\n    def _step(self) -&gt; List[str]:\n        \"\"\"Perform a single optimization step.\n\n        Returns:\n            List[str]: The optimized list of prompts after the step.\n        \"\"\"\n        offsprings = self._crossover(self.prompt_objects)\n        mutated = self._mutate(offsprings)\n        combined = self.prompt_objects + mutated\n\n        self.prompt_objects = self._do_racing(combined, self.population_size)\n        self.prompts = [p.construct_prompt() for p in self.prompt_objects]\n\n        return self.prompts\n</code></pre>"},{"location":"api/optimizers/#promptolution.optimizers.capo.CAPO.__init__","title":"<code>__init__(predictor, task, meta_llm, initial_prompts=None, crossovers_per_iter=4, upper_shots=5, max_n_blocks_eval=10, test_statistic='paired_t_test', alpha=0.2, length_penalty=0.05, check_fs_accuracy=True, create_fs_reasoning=True, df_few_shots=None, crossover_template=None, mutation_template=None, callbacks=None, config=None)</code>","text":"<p>Initializes the CAPOptimizer with various parameters for prompt evolution.</p> <p>Parameters:</p> Name Type Description Default <code>predictor</code> <code>BasePredictor</code> <p>The predictor for evaluating prompt performance.</p> required <code>task</code> <code>BaseTask</code> <p>The task instance containing dataset and description.</p> required <code>meta_llm</code> <code>BaseLLM</code> <p>The meta language model for crossover/mutation.</p> required <code>initial_prompts</code> <code>List[str]</code> <p>Initial prompt instructions.</p> <code>None</code> <code>crossovers_per_iter</code> <code>int</code> <p>Number of crossover operations per iteration.</p> <code>4</code> <code>upper_shots</code> <code>int</code> <p>Maximum number of few-shot examples per prompt.</p> <code>5</code> <code>p_few_shot_reasoning</code> <code>float</code> <p>Probability of generating llm-reasoning for few-shot examples, instead of simply using input-output pairs.</p> required <code>max_n_blocks_eval</code> <code>int</code> <p>Maximum number of evaluation blocks.</p> <code>10</code> <code>test_statistic</code> <code>TestStatistics</code> <p>Statistical test to compare prompt performance. Default is \"paired_t_test\".</p> <code>'paired_t_test'</code> <code>alpha</code> <code>float</code> <p>Significance level for the statistical test.</p> <code>0.2</code> <code>length_penalty</code> <code>float</code> <p>Penalty factor for prompt length.</p> <code>0.05</code> <code>check_fs_accuracy</code> <code>bool</code> <p>Whether to check the accuracy of few-shot examples before appending them to the prompt. In cases such as reward tasks, this can be set to False, as no ground truth is available. Default is True.</p> <code>True</code> <code>create_fs_reasoning</code> <code>bool</code> <p>Whether to create reasoning for few-shot examples using the downstream model, instead of simply using input-output pairs from the few shots DataFrame. Default is True.</p> <code>True</code> <code>df_few_shots</code> <code>DataFrame</code> <p>DataFrame containing few-shot examples. If None, will pop 10% of datapoints from task.</p> <code>None</code> <code>crossover_template</code> <code>str</code> <p>Template for crossover instructions.</p> <code>None</code> <code>mutation_template</code> <code>str</code> <p>Template for mutation instructions.</p> <code>None</code> <code>callbacks</code> <code>List[Callable]</code> <p>Callbacks for optimizer events.</p> <code>None</code> <code>config</code> <code>ExperimentConfig</code> <p>Configuration for the optimizer.</p> <code>None</code> Source code in <code>promptolution/optimizers/capo.py</code> <pre><code>def __init__(\n    self,\n    predictor: \"BasePredictor\",\n    task: \"BaseTask\",\n    meta_llm: \"BaseLLM\",\n    initial_prompts: Optional[List[str]] = None,\n    crossovers_per_iter: int = 4,\n    upper_shots: int = 5,\n    max_n_blocks_eval: int = 10,\n    test_statistic: \"TestStatistics\" = \"paired_t_test\",\n    alpha: float = 0.2,\n    length_penalty: float = 0.05,\n    check_fs_accuracy: bool = True,\n    create_fs_reasoning: bool = True,\n    df_few_shots: Optional[pd.DataFrame] = None,\n    crossover_template: Optional[str] = None,\n    mutation_template: Optional[str] = None,\n    callbacks: Optional[List[\"BaseCallback\"]] = None,\n    config: Optional[\"ExperimentConfig\"] = None,\n) -&gt; None:\n    \"\"\"Initializes the CAPOptimizer with various parameters for prompt evolution.\n\n    Args:\n        predictor (BasePredictor): The predictor for evaluating prompt performance.\n        task (BaseTask): The task instance containing dataset and description.\n        meta_llm (BaseLLM): The meta language model for crossover/mutation.\n        initial_prompts (List[str]): Initial prompt instructions.\n        crossovers_per_iter (int): Number of crossover operations per iteration.\n        upper_shots (int): Maximum number of few-shot examples per prompt.\n        p_few_shot_reasoning (float): Probability of generating llm-reasoning for few-shot examples, instead of simply using input-output pairs.\n        max_n_blocks_eval (int): Maximum number of evaluation blocks.\n        test_statistic (TestStatistics): Statistical test to compare prompt performance. Default is \"paired_t_test\".\n        alpha (float): Significance level for the statistical test.\n        length_penalty (float): Penalty factor for prompt length.\n        check_fs_accuracy (bool): Whether to check the accuracy of few-shot examples before appending them to the prompt.\n            In cases such as reward tasks, this can be set to False, as no ground truth is available. Default is True.\n        create_fs_reasoning (bool): Whether to create reasoning for few-shot examples using the downstream model,\n            instead of simply using input-output pairs from the few shots DataFrame. Default is True.\n        df_few_shots (pd.DataFrame): DataFrame containing few-shot examples. If None, will pop 10% of datapoints from task.\n        crossover_template (str, optional): Template for crossover instructions.\n        mutation_template (str, optional): Template for mutation instructions.\n        callbacks (List[Callable], optional): Callbacks for optimizer events.\n        config (ExperimentConfig, optional): Configuration for the optimizer.\n    \"\"\"\n    self.meta_llm = meta_llm\n    self.downstream_llm = predictor.llm\n\n    self.crossover_template = crossover_template or CAPO_CROSSOVER_TEMPLATE\n    self.mutation_template = mutation_template or CAPO_MUTATION_TEMPLATE\n\n    self.crossovers_per_iter = crossovers_per_iter\n    self.upper_shots = upper_shots\n    self.max_n_blocks_eval = max_n_blocks_eval\n    self.test_statistic = get_test_statistic_func(test_statistic)\n    self.alpha = alpha\n\n    self.length_penalty = length_penalty\n    self.token_counter = get_token_counter(self.downstream_llm)\n\n    self.check_fs_accuracy = check_fs_accuracy\n    self.create_fs_reasoning = create_fs_reasoning\n\n    self.scores: List[float] = []\n    super().__init__(predictor, task, initial_prompts, callbacks, config)\n    self.df_few_shots = df_few_shots if df_few_shots is not None else task.pop_datapoints(frac=0.1)\n    if self.max_n_blocks_eval &gt; self.task.n_blocks:\n        logger.warning(\n            f\"\u2139\ufe0f max_n_blocks_eval ({self.max_n_blocks_eval}) is larger than the number of blocks ({self.task.n_blocks}).\"\n            f\" Setting max_n_blocks_eval to {self.task.n_blocks}.\"\n        )\n        self.max_n_blocks_eval = self.task.n_blocks\n    self.population_size = len(self.prompts)\n\n    if hasattr(self.predictor, \"begin_marker\") and hasattr(self.predictor, \"end_marker\"):\n        self.target_begin_marker = self.predictor.begin_marker\n        self.target_end_marker = self.predictor.end_marker\n    else:\n        self.target_begin_marker = \"\"\n        self.target_end_marker = \"\"\n</code></pre>"},{"location":"api/optimizers/#promptolution.optimizers.capo.CAPOPrompt","title":"<code>CAPOPrompt</code>","text":"<p>Represents a prompt consisting of an instruction and few-shot examples.</p> Source code in <code>promptolution/optimizers/capo.py</code> <pre><code>class CAPOPrompt:\n    \"\"\"Represents a prompt consisting of an instruction and few-shot examples.\"\"\"\n\n    def __init__(self, instruction_text: str, few_shots: List[str]) -&gt; None:\n        \"\"\"Initializes the Prompt with an instruction and associated examples.\n\n        Args:\n            instruction_text (str): The instruction or prompt text.\n            few_shots (List[str]): List of examples as string.\n        \"\"\"\n        self.instruction_text = instruction_text.strip()\n        self.few_shots = few_shots\n\n    def construct_prompt(self) -&gt; str:\n        \"\"\"Constructs the full prompt string by replacing placeholders in the template with the instruction and formatted examples.\n\n        Returns:\n            str: The constructed prompt string.\n        \"\"\"\n        few_shot_str = \"\\n\\n\".join(self.few_shots).strip()\n        prompt = (\n            CAPO_DOWNSTREAM_TEMPLATE.replace(\"&lt;instruction&gt;\", self.instruction_text)\n            .replace(\"&lt;few_shots&gt;\", few_shot_str)\n            .replace(\"\\n\\n\\n\\n\", \"\\n\\n\")  # replace extra newlines if no few shots are provided\n            .strip()\n        )\n        return prompt\n\n    def __str__(self) -&gt; str:\n        \"\"\"Returns the string representation of the prompt.\"\"\"\n        return self.construct_prompt()\n</code></pre>"},{"location":"api/optimizers/#promptolution.optimizers.capo.CAPOPrompt.__init__","title":"<code>__init__(instruction_text, few_shots)</code>","text":"<p>Initializes the Prompt with an instruction and associated examples.</p> <p>Parameters:</p> Name Type Description Default <code>instruction_text</code> <code>str</code> <p>The instruction or prompt text.</p> required <code>few_shots</code> <code>List[str]</code> <p>List of examples as string.</p> required Source code in <code>promptolution/optimizers/capo.py</code> <pre><code>def __init__(self, instruction_text: str, few_shots: List[str]) -&gt; None:\n    \"\"\"Initializes the Prompt with an instruction and associated examples.\n\n    Args:\n        instruction_text (str): The instruction or prompt text.\n        few_shots (List[str]): List of examples as string.\n    \"\"\"\n    self.instruction_text = instruction_text.strip()\n    self.few_shots = few_shots\n</code></pre>"},{"location":"api/optimizers/#promptolution.optimizers.capo.CAPOPrompt.__str__","title":"<code>__str__()</code>","text":"<p>Returns the string representation of the prompt.</p> Source code in <code>promptolution/optimizers/capo.py</code> <pre><code>def __str__(self) -&gt; str:\n    \"\"\"Returns the string representation of the prompt.\"\"\"\n    return self.construct_prompt()\n</code></pre>"},{"location":"api/optimizers/#promptolution.optimizers.capo.CAPOPrompt.construct_prompt","title":"<code>construct_prompt()</code>","text":"<p>Constructs the full prompt string by replacing placeholders in the template with the instruction and formatted examples.</p> <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>The constructed prompt string.</p> Source code in <code>promptolution/optimizers/capo.py</code> <pre><code>def construct_prompt(self) -&gt; str:\n    \"\"\"Constructs the full prompt string by replacing placeholders in the template with the instruction and formatted examples.\n\n    Returns:\n        str: The constructed prompt string.\n    \"\"\"\n    few_shot_str = \"\\n\\n\".join(self.few_shots).strip()\n    prompt = (\n        CAPO_DOWNSTREAM_TEMPLATE.replace(\"&lt;instruction&gt;\", self.instruction_text)\n        .replace(\"&lt;few_shots&gt;\", few_shot_str)\n        .replace(\"\\n\\n\\n\\n\", \"\\n\\n\")  # replace extra newlines if no few shots are provided\n        .strip()\n    )\n    return prompt\n</code></pre>"},{"location":"api/optimizers/#promptolution.optimizers.evoprompt_de","title":"<code>evoprompt_de</code>","text":"<p>Module for EvoPromptDE optimizer.</p>"},{"location":"api/optimizers/#promptolution.optimizers.evoprompt_de.EvoPromptDE","title":"<code>EvoPromptDE</code>","text":"<p>               Bases: <code>BaseOptimizer</code></p> <p>EvoPromptDE: Differential Evolution-based Prompt Optimizer.</p> <p>This class implements a differential evolution algorithm for optimizing prompts in large language models. It is adapted from the paper \"Connecting Large Language Models with Evolutionary Algorithms Yields Powerful Prompt Optimizers\" by Guo et al., 2023.</p> <p>The optimizer uses a differential evolution strategy to generate new prompts from existing ones, with an option to use the current best prompt as a donor.</p> <p>Attributes:</p> Name Type Description <code>prompt_template</code> <code>str</code> <p>Template for generating meta-prompts during evolution.</p> <code>donor_random</code> <code>bool</code> <p>If False, uses the current best prompt as a donor; if True, uses a random prompt.</p> <code>meta_llm</code> <p>Language model used for generating child prompts from meta-prompts.</p> <p>Parameters:</p> Name Type Description Default <code>prompt_template</code> <code>str</code> <p>Template for meta-prompts.</p> required <code>meta_llm</code> <code>BaseLLM</code> <p>Language model for child prompt generation.</p> required <code>donor_random</code> <code>bool</code> <p>Whether to use a random donor. Defaults to False.</p> <code>False</code> <code>config</code> <code>ExperimentConfig</code> <p>Configuration for the optimizer, overriding defaults.</p> <code>None</code> Source code in <code>promptolution/optimizers/evoprompt_de.py</code> <pre><code>class EvoPromptDE(BaseOptimizer):\n    \"\"\"EvoPromptDE: Differential Evolution-based Prompt Optimizer.\n\n    This class implements a differential evolution algorithm for optimizing prompts in large language models.\n    It is adapted from the paper \"Connecting Large Language Models with Evolutionary Algorithms\n    Yields Powerful Prompt Optimizers\" by Guo et al., 2023.\n\n    The optimizer uses a differential evolution strategy to generate new prompts from existing ones,\n    with an option to use the current best prompt as a donor.\n\n    Attributes:\n        prompt_template (str): Template for generating meta-prompts during evolution.\n        donor_random (bool): If False, uses the current best prompt as a donor; if True, uses a random prompt.\n        meta_llm: Language model used for generating child prompts from meta-prompts.\n\n    Args:\n        prompt_template (str): Template for meta-prompts.\n        meta_llm: Language model for child prompt generation.\n        donor_random (bool, optional): Whether to use a random donor. Defaults to False.\n        config (ExperimentConfig, optional): Configuration for the optimizer, overriding defaults.\n    \"\"\"\n\n    def __init__(\n        self,\n        predictor: \"BasePredictor\",\n        task: \"BaseTask\",\n        prompt_template: str,\n        meta_llm: \"BaseLLM\",\n        initial_prompts: Optional[List[str]] = None,\n        donor_random: bool = False,\n        callbacks: Optional[List[\"BaseCallback\"]] = None,\n        config: Optional[\"ExperimentConfig\"] = None,\n    ) -&gt; None:\n        \"\"\"Initialize the EvoPromptDE optimizer.\"\"\"\n        self.prompt_template = prompt_template\n        self.donor_random = donor_random\n        self.meta_llm = meta_llm\n        super().__init__(\n            predictor=predictor, task=task, initial_prompts=initial_prompts, callbacks=callbacks, config=config\n        )\n\n    def _pre_optimization_loop(self) -&gt; None:\n        self.scores = self.task.evaluate(self.prompts, self.predictor, return_agg_scores=True)\n        self.prompts = [prompt for _, prompt in sorted(zip(self.scores, self.prompts), reverse=True)]\n        self.scores = sorted(self.scores, reverse=True)\n\n    def _step(self) -&gt; List[str]:\n        \"\"\"Perform the optimization process for a specified number of steps.\n\n        This method iteratively improves the prompts using a differential evolution strategy.\n        It evaluates prompts, generates new prompts using the DE algorithm, and replaces\n        prompts if the new ones perform better.\n\n\n        Returns:\n            List[str]: The optimized list of prompts after all steps.\n        \"\"\"\n        cur_best = self.prompts[0]\n        meta_prompts = []\n        for i in range(len(self.prompts)):\n            # create meta prompts\n            old_prompt = self.prompts[i]\n\n            candidates = [prompt for prompt in self.prompts if prompt != old_prompt]\n            a, b, c = np.random.choice(candidates, size=3, replace=False)\n\n            if not self.donor_random:\n                c = cur_best\n\n            meta_prompt = (\n                self.prompt_template.replace(\"&lt;prompt0&gt;\", old_prompt)\n                .replace(\"&lt;prompt1&gt;\", a)\n                .replace(\"&lt;prompt2&gt;\", b)\n                .replace(\"&lt;prompt3&gt;\", c)\n            )\n\n            meta_prompts.append(meta_prompt)\n\n        child_prompts = self.meta_llm.get_response(meta_prompts)\n        child_prompts = extract_from_tag(child_prompts, \"&lt;prompt&gt;\", \"&lt;/prompt&gt;\")\n\n        child_scores = self.task.evaluate(child_prompts, self.predictor, return_agg_scores=True)\n\n        for i in range(len(self.prompts)):\n            if child_scores[i] &gt; self.scores[i]:\n                self.prompts[i] = child_prompts[i]\n                self.scores[i] = child_scores[i]\n\n        self.prompts = [prompt for _, prompt in sorted(zip(self.scores, self.prompts), reverse=True)]\n        self.scores = sorted(self.scores, reverse=True)\n\n        return self.prompts\n</code></pre>"},{"location":"api/optimizers/#promptolution.optimizers.evoprompt_de.EvoPromptDE.__init__","title":"<code>__init__(predictor, task, prompt_template, meta_llm, initial_prompts=None, donor_random=False, callbacks=None, config=None)</code>","text":"<p>Initialize the EvoPromptDE optimizer.</p> Source code in <code>promptolution/optimizers/evoprompt_de.py</code> <pre><code>def __init__(\n    self,\n    predictor: \"BasePredictor\",\n    task: \"BaseTask\",\n    prompt_template: str,\n    meta_llm: \"BaseLLM\",\n    initial_prompts: Optional[List[str]] = None,\n    donor_random: bool = False,\n    callbacks: Optional[List[\"BaseCallback\"]] = None,\n    config: Optional[\"ExperimentConfig\"] = None,\n) -&gt; None:\n    \"\"\"Initialize the EvoPromptDE optimizer.\"\"\"\n    self.prompt_template = prompt_template\n    self.donor_random = donor_random\n    self.meta_llm = meta_llm\n    super().__init__(\n        predictor=predictor, task=task, initial_prompts=initial_prompts, callbacks=callbacks, config=config\n    )\n</code></pre>"},{"location":"api/optimizers/#promptolution.optimizers.evoprompt_ga","title":"<code>evoprompt_ga</code>","text":"<p>Module for EvoPromptGA optimizer.</p>"},{"location":"api/optimizers/#promptolution.optimizers.evoprompt_ga.EvoPromptGA","title":"<code>EvoPromptGA</code>","text":"<p>               Bases: <code>BaseOptimizer</code></p> <p>EvoPromptGA: Genetic Algorithm-based Prompt Optimizer.</p> <p>This class implements a genetic algorithm for optimizing prompts in large language models. It is adapted from the paper \"Connecting Large Language Models with Evolutionary Algorithms Yields Powerful Prompt Optimizers\" by Guo et al., 2023.</p> <p>The optimizer uses crossover operations to generate new prompts from existing ones, with different selection methods available for choosing parent prompts.</p> <p>Attributes:</p> Name Type Description <code>prompt_template</code> <code>str</code> <p>Template for generating meta-prompts during crossover.</p> <code>meta_llm</code> <p>Language model used for generating child prompts from meta-prompts.</p> <code>selection_mode</code> <code>str</code> <p>Method for selecting parent prompts ('random', 'wheel', or 'tour').</p> <p>Parameters:</p> Name Type Description Default <code>prompt_template</code> <code>str</code> <p>Template for meta-prompts.</p> required <code>meta_llm</code> <code>BaseLLM</code> <p>Language model for child prompt generation.</p> required <code>selection_mode</code> <code>str</code> <p>Parent selection method. Defaults to \"wheel\".</p> <code>'wheel'</code> <p>Raises:</p> Type Description <code>AssertionError</code> <p>If an invalid selection mode is provided.</p> Source code in <code>promptolution/optimizers/evoprompt_ga.py</code> <pre><code>class EvoPromptGA(BaseOptimizer):\n    \"\"\"EvoPromptGA: Genetic Algorithm-based Prompt Optimizer.\n\n    This class implements a genetic algorithm for optimizing prompts in large language models.\n    It is adapted from the paper \"Connecting Large Language Models with Evolutionary Algorithms\n    Yields Powerful Prompt Optimizers\" by Guo et al., 2023.\n\n    The optimizer uses crossover operations to generate new prompts from existing ones,\n    with different selection methods available for choosing parent prompts.\n\n    Attributes:\n        prompt_template (str): Template for generating meta-prompts during crossover.\n        meta_llm: Language model used for generating child prompts from meta-prompts.\n        selection_mode (str): Method for selecting parent prompts ('random', 'wheel', or 'tour').\n\n    Args:\n        prompt_template (str): Template for meta-prompts.\n        meta_llm: Language model for child prompt generation.\n        selection_mode (str, optional): Parent selection method. Defaults to \"wheel\".\n\n    Raises:\n        AssertionError: If an invalid selection mode is provided.\n    \"\"\"\n\n    def __init__(\n        self,\n        predictor: \"BasePredictor\",\n        task: \"BaseTask\",\n        prompt_template: str,\n        meta_llm: \"BaseLLM\",\n        initial_prompts: Optional[List[str]] = None,\n        selection_mode: str = \"wheel\",\n        callbacks: Optional[List[\"BaseCallback\"]] = None,\n        config: Optional[\"ExperimentConfig\"] = None,\n    ) -&gt; None:\n        \"\"\"Initialize the EvoPromptGA optimizer.\"\"\"\n        self.prompt_template = prompt_template\n        self.meta_llm = meta_llm\n        self.selection_mode = selection_mode\n        super().__init__(\n            predictor=predictor, initial_prompts=initial_prompts, task=task, callbacks=callbacks, config=config\n        )\n        assert self.selection_mode in [\"random\", \"wheel\", \"tour\"], \"Invalid selection mode.\"\n\n    def _pre_optimization_loop(self) -&gt; None:\n        self.scores = self.task.evaluate(self.prompts, self.predictor, return_agg_scores=True)\n        # sort prompts by score\n        self.prompts = [prompt for _, prompt in sorted(zip(self.scores, self.prompts), reverse=True)]\n        self.scores = sorted(self.scores, reverse=True)\n\n    def _step(self) -&gt; List[str]:\n        new_prompts = self._crossover(self.prompts, self.scores)\n        prompts = self.prompts + new_prompts\n\n        new_scores = self.task.evaluate(new_prompts, self.predictor, return_agg_scores=True)\n\n        scores = self.scores + new_scores\n\n        # sort scores and prompts\n        self.prompts = [prompt for _, prompt in sorted(zip(scores, prompts), reverse=True)][: len(self.prompts)]\n        self.scores = sorted(scores, reverse=True)[: len(self.prompts)]\n\n        return self.prompts\n\n    def _crossover(self, prompts: List[str], scores: List[float]) -&gt; List[str]:\n        \"\"\"Perform crossover operation to generate new child prompts.\n\n        This method selects parent prompts based on the chosen selection mode,\n        creates meta-prompts using the prompt template, and generates new child\n        prompts using the meta language model.\n\n        Args:\n            prompts (List[str]): List of current prompts.\n            scores (List[float]): Corresponding scores for the prompts.\n\n        Returns:\n            List[str]: Newly generated child prompts.\n        \"\"\"\n        # parent selection\n        if self.selection_mode == \"wheel\":\n            wheel_idx = np.random.choice(\n                np.arange(0, len(prompts)),\n                size=len(prompts),\n                replace=True,\n                p=np.array(scores) / np.sum(scores) if np.sum(scores) &gt; 0 else np.ones(len(scores)) / len(scores),\n            ).tolist()\n            parent_pop = [self.prompts[idx] for idx in wheel_idx]\n\n        elif self.selection_mode in [\"random\", \"tour\"]:\n            parent_pop = self.prompts\n\n        # crossover\n        meta_prompts = []\n        for _ in self.prompts:\n            if self.selection_mode in [\"random\", \"wheel\"]:\n                parent_1, parent_2 = np.random.choice(parent_pop, size=2, replace=False)\n            elif self.selection_mode == \"tour\":\n                group_1 = np.random.choice(parent_pop, size=2, replace=False)\n                group_2 = np.random.choice(parent_pop, size=2, replace=False)\n                # use the best of each group based on scores\n                parent_1 = group_1[np.argmax([self.scores[self.prompts.index(p)] for p in group_1])]\n                parent_2 = group_2[np.argmax([self.scores[self.prompts.index(p)] for p in group_2])]\n\n            meta_prompt = self.prompt_template.replace(\"&lt;prompt1&gt;\", parent_1).replace(\"&lt;prompt2&gt;\", parent_2)\n            meta_prompts.append(meta_prompt)\n\n        child_prompts = self.meta_llm.get_response(meta_prompts)\n        child_prompts = extract_from_tag(child_prompts, \"&lt;prompt&gt;\", \"&lt;/prompt&gt;\")\n\n        return child_prompts\n</code></pre>"},{"location":"api/optimizers/#promptolution.optimizers.evoprompt_ga.EvoPromptGA.__init__","title":"<code>__init__(predictor, task, prompt_template, meta_llm, initial_prompts=None, selection_mode='wheel', callbacks=None, config=None)</code>","text":"<p>Initialize the EvoPromptGA optimizer.</p> Source code in <code>promptolution/optimizers/evoprompt_ga.py</code> <pre><code>def __init__(\n    self,\n    predictor: \"BasePredictor\",\n    task: \"BaseTask\",\n    prompt_template: str,\n    meta_llm: \"BaseLLM\",\n    initial_prompts: Optional[List[str]] = None,\n    selection_mode: str = \"wheel\",\n    callbacks: Optional[List[\"BaseCallback\"]] = None,\n    config: Optional[\"ExperimentConfig\"] = None,\n) -&gt; None:\n    \"\"\"Initialize the EvoPromptGA optimizer.\"\"\"\n    self.prompt_template = prompt_template\n    self.meta_llm = meta_llm\n    self.selection_mode = selection_mode\n    super().__init__(\n        predictor=predictor, initial_prompts=initial_prompts, task=task, callbacks=callbacks, config=config\n    )\n    assert self.selection_mode in [\"random\", \"wheel\", \"tour\"], \"Invalid selection mode.\"\n</code></pre>"},{"location":"api/optimizers/#promptolution.optimizers.opro","title":"<code>opro</code>","text":"<p>Module implementing the OPRO (Optimization by PROmpting) algorithm.</p>"},{"location":"api/optimizers/#promptolution.optimizers.opro.OPRO","title":"<code>OPRO</code>","text":"<p>               Bases: <code>BaseOptimizer</code></p> <p>OPRO: Optimization by PROmpting.</p> <p>Implementation of the technique proposed in \"Large Language Models as Optimizers\" (Yang et al., 2023: https://arxiv.org/abs/2309.03409).</p> <p>OPRO works by providing a meta-LLM with task descriptions and previous prompt-score pairs to generate improved prompts for a downstream LLM.</p> Source code in <code>promptolution/optimizers/opro.py</code> <pre><code>class OPRO(BaseOptimizer):\n    \"\"\"OPRO: Optimization by PROmpting.\n\n    Implementation of the technique proposed in \"Large Language Models as Optimizers\"\n    (Yang et al., 2023: https://arxiv.org/abs/2309.03409).\n\n    OPRO works by providing a meta-LLM with task descriptions and previous\n    prompt-score pairs to generate improved prompts for a downstream LLM.\n    \"\"\"\n\n    def __init__(\n        self,\n        predictor: \"BasePredictor\",\n        task: \"BaseTask\",\n        meta_llm: \"BaseLLM\",\n        initial_prompts: Optional[List[str]] = None,\n        prompt_template: Optional[str] = None,\n        max_num_instructions: int = 20,\n        num_instructions_per_step: int = 8,\n        num_few_shots: int = 3,\n        callbacks: Optional[List[\"BaseCallback\"]] = None,\n        config: Optional[\"ExperimentConfig\"] = None,\n    ) -&gt; None:\n        \"\"\"Initialize the OPRO optimizer.\n\n        Args:\n            predictor: Predictor for prompt evaluation\n            task: Task object for prompt evaluation\n            meta_llm: LLM that generates improved prompts\n            initial_prompts: Initial set of prompts to start optimization with\n            prompt_template: Custom meta prompt template (uses OPRO_TEMPLATE if None)\n            max_num_instructions: Maximum previous instructions to include in meta prompt\n            num_instructions_per_step: Number of prompts to generate in each step\n            num_few_shots: Number of few-shot examples to include (0 for none)\n            callbacks: List of callback functions\n            config: \"ExperimentConfig\" overwriting default parameters\n        \"\"\"\n        self.meta_llm = meta_llm\n        self.meta_prompt_template = prompt_template or OPRO_TEMPLATE\n        self.max_num_instructions = max_num_instructions\n        self.num_instructions_per_step = num_instructions_per_step\n        self.num_few_shots = num_few_shots\n        super().__init__(\n            predictor=predictor, task=task, initial_prompts=initial_prompts, callbacks=callbacks, config=config\n        )\n\n    def _sample_examples(self) -&gt; str:\n        \"\"\"Sample few-shot examples from the dataset.\n\n        Returns:\n            Formatted string of few-shot examples with inputs and expected outputs\n        \"\"\"\n        idx = np.random.choice(len(self.task.xs), self.num_few_shots)\n        sample_x = [self.task.xs[i] for i in idx]\n        sample_y = [self.task.ys[i] for i in idx]\n\n        return \"\\n\".join([f\"Input: {x}\\nOutput: {y}\" for x, y in zip(sample_x, sample_y)])\n\n    def _format_instructions(self) -&gt; str:\n        \"\"\"Format previous prompts and their scores for the meta prompt.\n\n        Returns:\n            Formatted string of previous prompts and their scores,\n            sorted by ascending score (worse to better)\n        \"\"\"\n        prompt_score_pairs = list(zip(self.prompts, self.scores))\n        sorted_pairs = sorted(prompt_score_pairs, key=lambda x: x[1])\n\n        return \"\".join([f\"text:\\n{prompt}\\nscore: {int(100 * round(score, 2))}\\n\\n\" for prompt, score in sorted_pairs])\n\n    def _add_prompt_and_score(self, prompt: str, score: float) -&gt; None:\n        \"\"\"Add a prompt and its score to the lists, maintaining max length.\n\n        Args:\n            prompt: The prompt to add\n            score: The corresponding score for the prompt\n        \"\"\"\n        if prompt in self.prompts:\n            return\n\n        self.prompts.append(prompt)\n        self.scores.append(score)\n\n        # Keep only the top-performing prompts if we exceed the maximum number of instructions\n        keep_indices = np.argsort(self.scores)[-self.max_num_instructions :]\n        self.prompts = [self.prompts[i] for i in keep_indices]\n        self.scores = [self.scores[i] for i in keep_indices]\n\n    def _pre_optimization_loop(self):\n        self.scores = list(self.task.evaluate(self.prompts, self.predictor))\n        self.meta_prompt = self.meta_prompt_template.replace(\"&lt;instructions&gt;\", self._format_instructions()).replace(\n            \"&lt;examples&gt;\", self._sample_examples()\n        )\n\n    def _step(self) -&gt; List[str]:\n        duplicate_prompts = 0\n        for _ in range(self.num_instructions_per_step):\n            generation_seed = np.random.randint(0, int(1e9))\n            self.meta_llm.set_generation_seed(generation_seed)\n\n            response = self.meta_llm.get_response([self.meta_prompt])[0]\n\n            prompt = extract_from_tag(response, \"&lt;prompt&gt;\", \"&lt;/prompt&gt;\")\n\n            if prompt in self.prompts:\n                duplicate_prompts += 1\n                continue\n\n            score = self.task.evaluate(prompt, self.predictor)[0]\n\n            self._add_prompt_and_score(prompt, score)\n\n        # Update meta prompt\n        self.meta_prompt = self.meta_prompt_template.replace(\"&lt;instructions&gt;\", self._format_instructions()).replace(\n            \"&lt;examples&gt;\", self._sample_examples()\n        )\n\n        return self.prompts\n</code></pre>"},{"location":"api/optimizers/#promptolution.optimizers.opro.OPRO.__init__","title":"<code>__init__(predictor, task, meta_llm, initial_prompts=None, prompt_template=None, max_num_instructions=20, num_instructions_per_step=8, num_few_shots=3, callbacks=None, config=None)</code>","text":"<p>Initialize the OPRO optimizer.</p> <p>Parameters:</p> Name Type Description Default <code>predictor</code> <code>BasePredictor</code> <p>Predictor for prompt evaluation</p> required <code>task</code> <code>BaseTask</code> <p>Task object for prompt evaluation</p> required <code>meta_llm</code> <code>BaseLLM</code> <p>LLM that generates improved prompts</p> required <code>initial_prompts</code> <code>Optional[List[str]]</code> <p>Initial set of prompts to start optimization with</p> <code>None</code> <code>prompt_template</code> <code>Optional[str]</code> <p>Custom meta prompt template (uses OPRO_TEMPLATE if None)</p> <code>None</code> <code>max_num_instructions</code> <code>int</code> <p>Maximum previous instructions to include in meta prompt</p> <code>20</code> <code>num_instructions_per_step</code> <code>int</code> <p>Number of prompts to generate in each step</p> <code>8</code> <code>num_few_shots</code> <code>int</code> <p>Number of few-shot examples to include (0 for none)</p> <code>3</code> <code>callbacks</code> <code>Optional[List[BaseCallback]]</code> <p>List of callback functions</p> <code>None</code> <code>config</code> <code>Optional[ExperimentConfig]</code> <p>\"ExperimentConfig\" overwriting default parameters</p> <code>None</code> Source code in <code>promptolution/optimizers/opro.py</code> <pre><code>def __init__(\n    self,\n    predictor: \"BasePredictor\",\n    task: \"BaseTask\",\n    meta_llm: \"BaseLLM\",\n    initial_prompts: Optional[List[str]] = None,\n    prompt_template: Optional[str] = None,\n    max_num_instructions: int = 20,\n    num_instructions_per_step: int = 8,\n    num_few_shots: int = 3,\n    callbacks: Optional[List[\"BaseCallback\"]] = None,\n    config: Optional[\"ExperimentConfig\"] = None,\n) -&gt; None:\n    \"\"\"Initialize the OPRO optimizer.\n\n    Args:\n        predictor: Predictor for prompt evaluation\n        task: Task object for prompt evaluation\n        meta_llm: LLM that generates improved prompts\n        initial_prompts: Initial set of prompts to start optimization with\n        prompt_template: Custom meta prompt template (uses OPRO_TEMPLATE if None)\n        max_num_instructions: Maximum previous instructions to include in meta prompt\n        num_instructions_per_step: Number of prompts to generate in each step\n        num_few_shots: Number of few-shot examples to include (0 for none)\n        callbacks: List of callback functions\n        config: \"ExperimentConfig\" overwriting default parameters\n    \"\"\"\n    self.meta_llm = meta_llm\n    self.meta_prompt_template = prompt_template or OPRO_TEMPLATE\n    self.max_num_instructions = max_num_instructions\n    self.num_instructions_per_step = num_instructions_per_step\n    self.num_few_shots = num_few_shots\n    super().__init__(\n        predictor=predictor, task=task, initial_prompts=initial_prompts, callbacks=callbacks, config=config\n    )\n</code></pre>"},{"location":"api/optimizers/#promptolution.optimizers.templates","title":"<code>templates</code>","text":"<p>Meta-prompt templates for different prompt optimization methods.</p>"},{"location":"api/predictors/","title":"Predictors","text":"<p>Module for LLM predictors.</p>"},{"location":"api/predictors/#promptolution.predictors.base_predictor","title":"<code>base_predictor</code>","text":"<p>Base module for predictors in the promptolution library.</p>"},{"location":"api/predictors/#promptolution.predictors.base_predictor.BasePredictor","title":"<code>BasePredictor</code>","text":"<p>               Bases: <code>ABC</code></p> <p>Abstract base class for predictors in the promptolution library.</p> <p>This class defines the interface that all concrete predictor implementations should follow.</p> Source code in <code>promptolution/predictors/base_predictor.py</code> <pre><code>class BasePredictor(ABC):\n    \"\"\"Abstract base class for predictors in the promptolution library.\n\n    This class defines the interface that all concrete predictor implementations should follow.\n    \"\"\"\n\n    def __init__(self, llm: \"BaseLLM\", config: Optional[\"ExperimentConfig\"] = None) -&gt; None:\n        \"\"\"Initialize the predictor with a language model and configuration.\n\n        Args:\n            llm: Language model to use for prediction.\n            config: Configuration for the predictor.\n        \"\"\"\n        self.llm = llm\n        self.extraction_description = \"\"\n        if config is not None:\n            config.apply_to(self)\n\n    def predict(\n        self,\n        prompts: Union[str, List[str]],\n        xs: List[str],\n        system_prompts: Optional[Union[str, List[str]]] = None,\n        return_seq: bool = False,\n    ) -&gt; Union[List[str], Tuple[List[str], List[str]]]:\n        \"\"\"Abstract method to make predictions based on prompts and input data.\n\n        Args:\n            prompts: Prompt or list of prompts to use for prediction.\n            xs: Array of input data.\n            system_prompts: List of system prompts to use for the language model.\n            return_seq: Whether to return the generating sequence.\n\n        Returns:\n            Array of predictions, optionally with sequences.\n        \"\"\"\n        if isinstance(prompts, str):\n            prompts = [prompts]\n\n        inputs = [prompt + \"\\n\" + x for prompt, x in zip(prompts, xs)]\n        outputs = self.llm.get_response(inputs, system_prompts=system_prompts)\n        preds = self._extract_preds(outputs)\n\n        if return_seq:\n            seqs = [f\"{x}\\n{out}\" for x, out in zip(xs, outputs)]\n            return preds, seqs\n\n        return preds\n\n    @abstractmethod\n    def _extract_preds(self, preds: List[str]) -&gt; List[str]:\n        \"\"\"Extract class labels from the predictions, based on the list of valid class labels.\n\n        Args:\n            preds: The raw predictions from the language model.\n\n        Returns:\n            List[str]: Extracted class labels from the predictions.\n        \"\"\"\n        raise NotImplementedError\n</code></pre>"},{"location":"api/predictors/#promptolution.predictors.base_predictor.BasePredictor.__init__","title":"<code>__init__(llm, config=None)</code>","text":"<p>Initialize the predictor with a language model and configuration.</p> <p>Parameters:</p> Name Type Description Default <code>llm</code> <code>BaseLLM</code> <p>Language model to use for prediction.</p> required <code>config</code> <code>Optional[ExperimentConfig]</code> <p>Configuration for the predictor.</p> <code>None</code> Source code in <code>promptolution/predictors/base_predictor.py</code> <pre><code>def __init__(self, llm: \"BaseLLM\", config: Optional[\"ExperimentConfig\"] = None) -&gt; None:\n    \"\"\"Initialize the predictor with a language model and configuration.\n\n    Args:\n        llm: Language model to use for prediction.\n        config: Configuration for the predictor.\n    \"\"\"\n    self.llm = llm\n    self.extraction_description = \"\"\n    if config is not None:\n        config.apply_to(self)\n</code></pre>"},{"location":"api/predictors/#promptolution.predictors.base_predictor.BasePredictor.predict","title":"<code>predict(prompts, xs, system_prompts=None, return_seq=False)</code>","text":"<p>Abstract method to make predictions based on prompts and input data.</p> <p>Parameters:</p> Name Type Description Default <code>prompts</code> <code>Union[str, List[str]]</code> <p>Prompt or list of prompts to use for prediction.</p> required <code>xs</code> <code>List[str]</code> <p>Array of input data.</p> required <code>system_prompts</code> <code>Optional[Union[str, List[str]]]</code> <p>List of system prompts to use for the language model.</p> <code>None</code> <code>return_seq</code> <code>bool</code> <p>Whether to return the generating sequence.</p> <code>False</code> <p>Returns:</p> Type Description <code>Union[List[str], Tuple[List[str], List[str]]]</code> <p>Array of predictions, optionally with sequences.</p> Source code in <code>promptolution/predictors/base_predictor.py</code> <pre><code>def predict(\n    self,\n    prompts: Union[str, List[str]],\n    xs: List[str],\n    system_prompts: Optional[Union[str, List[str]]] = None,\n    return_seq: bool = False,\n) -&gt; Union[List[str], Tuple[List[str], List[str]]]:\n    \"\"\"Abstract method to make predictions based on prompts and input data.\n\n    Args:\n        prompts: Prompt or list of prompts to use for prediction.\n        xs: Array of input data.\n        system_prompts: List of system prompts to use for the language model.\n        return_seq: Whether to return the generating sequence.\n\n    Returns:\n        Array of predictions, optionally with sequences.\n    \"\"\"\n    if isinstance(prompts, str):\n        prompts = [prompts]\n\n    inputs = [prompt + \"\\n\" + x for prompt, x in zip(prompts, xs)]\n    outputs = self.llm.get_response(inputs, system_prompts=system_prompts)\n    preds = self._extract_preds(outputs)\n\n    if return_seq:\n        seqs = [f\"{x}\\n{out}\" for x, out in zip(xs, outputs)]\n        return preds, seqs\n\n    return preds\n</code></pre>"},{"location":"api/predictors/#promptolution.predictors.classifier","title":"<code>classifier</code>","text":"<p>Module for classification predictors.</p>"},{"location":"api/predictors/#promptolution.predictors.classifier.FirstOccurrenceClassifier","title":"<code>FirstOccurrenceClassifier</code>","text":"<p>               Bases: <code>BasePredictor</code></p> <p>A predictor class for classification tasks using language models.</p> <p>This class takes a language model and a list of classes, and provides a method to predict classes for given prompts and input data. The class labels are extracted by matching the words in the prediction with the list of valid class labels. The first occurrence of a valid class label in the prediction is used as the predicted class. If no valid class label is found, the first class label in the list is used as the default prediction.</p> <p>Attributes:</p> Name Type Description <code>llm</code> <p>The language model used for generating predictions.</p> <code>classes</code> <code>List[str]</code> <p>The list of valid class labels.</p> <code>config</code> <code>ExperimentConfig</code> <p>Configuration for the classifier, overriding defaults.</p> Inherits from <p>BasePredictor: The base class for predictors in the promptolution library.</p> Source code in <code>promptolution/predictors/classifier.py</code> <pre><code>class FirstOccurrenceClassifier(BasePredictor):\n    \"\"\"A predictor class for classification tasks using language models.\n\n    This class takes a language model and a list of classes, and provides a method\n    to predict classes for given prompts and input data. The class labels are extracted\n    by matching the words in the prediction with the list of valid class labels.\n    The first occurrence of a valid class label in the prediction is used as the predicted class.\n    If no valid class label is found, the first class label in the list is used as the default prediction.\n\n    Attributes:\n        llm: The language model used for generating predictions.\n        classes (List[str]): The list of valid class labels.\n        config (ExperimentConfig, optional): Configuration for the classifier, overriding defaults.\n\n    Inherits from:\n        BasePredictor: The base class for predictors in the promptolution library.\n    \"\"\"\n\n    def __init__(self, llm: \"BaseLLM\", classes: List[str], config: Optional[\"ExperimentConfig\"] = None) -&gt; None:\n        \"\"\"Initialize the FirstOccurrenceClassifier.\n\n        Args:\n            llm: The language model to use for predictions.\n            classes (List[str]): The list of valid class labels.\n            config (ExperimentConfig, optional): Configuration for the classifier, overriding defaults.\n        \"\"\"\n        assert all([c.islower() for c in classes]), \"Class labels should be lowercase.\"\n        self.classes = classes\n\n        self.extraction_description = (\n            f\"The task is to classify the texts into one of those classes: {', '.join(classes)}.\"\n            \"The first occurrence of a valid class label in the prediction is used as the predicted class.\"\n        )\n\n        super().__init__(llm, config)\n\n    def _extract_preds(self, preds: List[str]) -&gt; List[str]:\n        \"\"\"Extract class labels from the predictions, based on the list of valid class labels.\n\n        Args:\n            preds: The raw predictions from the language model.\n        \"\"\"\n        result = []\n        for pred in preds:\n            predicted_class = self.classes[0]  # use first class as default pred\n            for word in pred.split():\n                word = \"\".join([c for c in word if c.isalnum()]).lower()\n                if word in self.classes:\n                    predicted_class = word\n                    break\n\n            result.append(predicted_class)\n\n        return result\n</code></pre>"},{"location":"api/predictors/#promptolution.predictors.classifier.FirstOccurrenceClassifier.__init__","title":"<code>__init__(llm, classes, config=None)</code>","text":"<p>Initialize the FirstOccurrenceClassifier.</p> <p>Parameters:</p> Name Type Description Default <code>llm</code> <code>BaseLLM</code> <p>The language model to use for predictions.</p> required <code>classes</code> <code>List[str]</code> <p>The list of valid class labels.</p> required <code>config</code> <code>ExperimentConfig</code> <p>Configuration for the classifier, overriding defaults.</p> <code>None</code> Source code in <code>promptolution/predictors/classifier.py</code> <pre><code>def __init__(self, llm: \"BaseLLM\", classes: List[str], config: Optional[\"ExperimentConfig\"] = None) -&gt; None:\n    \"\"\"Initialize the FirstOccurrenceClassifier.\n\n    Args:\n        llm: The language model to use for predictions.\n        classes (List[str]): The list of valid class labels.\n        config (ExperimentConfig, optional): Configuration for the classifier, overriding defaults.\n    \"\"\"\n    assert all([c.islower() for c in classes]), \"Class labels should be lowercase.\"\n    self.classes = classes\n\n    self.extraction_description = (\n        f\"The task is to classify the texts into one of those classes: {', '.join(classes)}.\"\n        \"The first occurrence of a valid class label in the prediction is used as the predicted class.\"\n    )\n\n    super().__init__(llm, config)\n</code></pre>"},{"location":"api/predictors/#promptolution.predictors.classifier.MarkerBasedClassifier","title":"<code>MarkerBasedClassifier</code>","text":"<p>               Bases: <code>BasePredictor</code></p> <p>A predictor class for classification tasks using language models.</p> <p>This class takes a language model and a list of classes, and provides a method to predict classes for given prompts and input data. The class labels are extracted.</p> <p>Attributes:</p> Name Type Description <code>llm</code> <p>The language model used for generating predictions.</p> <code>classes</code> <code>List[str]</code> <p>The list of valid class labels.</p> <code>marker</code> <code>str</code> <p>The marker to use for extracting the class label.</p> Inherits from <p>BasePredictor: The base class for predictors in the promptolution library.</p> Source code in <code>promptolution/predictors/classifier.py</code> <pre><code>class MarkerBasedClassifier(BasePredictor):\n    \"\"\"A predictor class for classification tasks using language models.\n\n    This class takes a language model and a list of classes, and provides a method\n    to predict classes for given prompts and input data. The class labels are extracted.\n\n    Attributes:\n        llm: The language model used for generating predictions.\n        classes (List[str]): The list of valid class labels.\n        marker (str): The marker to use for extracting the class label.\n\n    Inherits from:\n        BasePredictor: The base class for predictors in the promptolution library.\n    \"\"\"\n\n    def __init__(\n        self,\n        llm: \"BaseLLM\",\n        classes: Optional[List[str]] = None,\n        begin_marker: str = \"&lt;final_answer&gt;\",\n        end_marker: str = \"&lt;/final_answer&gt;\",\n        config: Optional[\"ExperimentConfig\"] = None,\n    ) -&gt; None:\n        \"\"\"Initialize the MarkerBasedClassifier.\n\n        Args:\n            llm: The language model to use for predictions.\n            classes (List[str]): The list of valid class labels. If None, does not force any class.\n            begin_marker (str): The marker to use for extracting the class label.\n            end_marker (str): The marker to use for extracting the class label.\n            config (ExperimentConfig, optional): Configuration for the classifier, overriding defaults.\n        \"\"\"\n        self.classes = classes\n        self.begin_marker = begin_marker\n        self.end_marker = end_marker\n\n        if classes is not None:\n            assert all([c.islower() for c in classes]), \"Class labels should be lowercase.\"\n\n            self.extraction_description = (\n                f\"The task is to classify the texts into one of those classes: {', '.join(classes)}.\"\n                f\"The class label is extracted from the text that are between these markers: {begin_marker} and {end_marker}.\"\n            )\n        else:\n            self.extraction_description = f\"The class label is extracted from the text that are between these markers: {begin_marker} and {end_marker}.\"\n\n        super().__init__(llm, config)\n\n    def _extract_preds(self, preds: List[str]) -&gt; List[str]:\n        \"\"\"Extract class labels from the predictions, by extracting the text following the marker.\n\n        Args:\n            preds: The raw predictions from the language model.\n        \"\"\"\n        result = []\n        for pred in preds:\n            pred = extract_from_tag(pred, self.begin_marker, self.end_marker).lower()\n            if self.classes is not None and pred not in self.classes:\n                pred = self.classes[0]\n\n            result.append(pred)\n\n        return result\n</code></pre>"},{"location":"api/predictors/#promptolution.predictors.classifier.MarkerBasedClassifier.__init__","title":"<code>__init__(llm, classes=None, begin_marker='&lt;final_answer&gt;', end_marker='&lt;/final_answer&gt;', config=None)</code>","text":"<p>Initialize the MarkerBasedClassifier.</p> <p>Parameters:</p> Name Type Description Default <code>llm</code> <code>BaseLLM</code> <p>The language model to use for predictions.</p> required <code>classes</code> <code>List[str]</code> <p>The list of valid class labels. If None, does not force any class.</p> <code>None</code> <code>begin_marker</code> <code>str</code> <p>The marker to use for extracting the class label.</p> <code>'&lt;final_answer&gt;'</code> <code>end_marker</code> <code>str</code> <p>The marker to use for extracting the class label.</p> <code>'&lt;/final_answer&gt;'</code> <code>config</code> <code>ExperimentConfig</code> <p>Configuration for the classifier, overriding defaults.</p> <code>None</code> Source code in <code>promptolution/predictors/classifier.py</code> <pre><code>def __init__(\n    self,\n    llm: \"BaseLLM\",\n    classes: Optional[List[str]] = None,\n    begin_marker: str = \"&lt;final_answer&gt;\",\n    end_marker: str = \"&lt;/final_answer&gt;\",\n    config: Optional[\"ExperimentConfig\"] = None,\n) -&gt; None:\n    \"\"\"Initialize the MarkerBasedClassifier.\n\n    Args:\n        llm: The language model to use for predictions.\n        classes (List[str]): The list of valid class labels. If None, does not force any class.\n        begin_marker (str): The marker to use for extracting the class label.\n        end_marker (str): The marker to use for extracting the class label.\n        config (ExperimentConfig, optional): Configuration for the classifier, overriding defaults.\n    \"\"\"\n    self.classes = classes\n    self.begin_marker = begin_marker\n    self.end_marker = end_marker\n\n    if classes is not None:\n        assert all([c.islower() for c in classes]), \"Class labels should be lowercase.\"\n\n        self.extraction_description = (\n            f\"The task is to classify the texts into one of those classes: {', '.join(classes)}.\"\n            f\"The class label is extracted from the text that are between these markers: {begin_marker} and {end_marker}.\"\n        )\n    else:\n        self.extraction_description = f\"The class label is extracted from the text that are between these markers: {begin_marker} and {end_marker}.\"\n\n    super().__init__(llm, config)\n</code></pre>"},{"location":"api/tasks/","title":"Tasks","text":"<p>Module for task-related functions and classes.</p>"},{"location":"api/tasks/#promptolution.tasks.base_task","title":"<code>base_task</code>","text":"<p>Base module for tasks.</p>"},{"location":"api/tasks/#promptolution.tasks.base_task.BaseTask","title":"<code>BaseTask</code>","text":"<p>               Bases: <code>ABC</code></p> <p>Abstract base class for tasks in the promptolution library.</p> Source code in <code>promptolution/tasks/base_task.py</code> <pre><code>class BaseTask(ABC):\n    \"\"\"Abstract base class for tasks in the promptolution library.\"\"\"\n\n    def __init__(\n        self,\n        df: pd.DataFrame,\n        x_column: str,\n        y_column: Optional[str] = None,\n        task_description: Optional[str] = None,\n        n_subsamples: int = 30,\n        eval_strategy: \"EvalStrategy\" = \"full\",\n        seed: int = 42,\n        config: Optional[\"ExperimentConfig\"] = None,\n    ) -&gt; None:\n        \"\"\"Initialize the BaseTask.\n\n        Args:\n            df (pd.DataFrame): The input DataFrame containing the data.\n            x_column (str): Name of the column containing input texts.\n            y_column (Optional[str]): Name of the column containing labels/ground truth (if applicable).\n            task_description (str): Description of the task.\n            n_subsamples (int): Number of subsamples to use for evaluation.\n            eval_strategy (Literal): Subsampling strategy (\"full\", \"subsample\", \"sequential_block\", \"random_block\", \"evaluated\").\n            seed (int): Random seed for reproducibility.\n            config (ExperimentConfig, optional): Configuration for the task, overriding defaults.\n        \"\"\"\n        self.df = df\n        self.x_column = x_column\n        self.y_column = y_column\n        self.task_description = task_description\n        self.n_subsamples = n_subsamples\n        self.eval_strategy = eval_strategy\n        self.seed = seed\n\n        super().__init__()\n        if config is not None:\n            config.apply_to(self)\n\n        self.xs: List[str] = df[self.x_column].values.astype(str).tolist()\n        self.has_y = y_column is not None\n        if self.has_y and y_column is not None:\n            self.ys: List[str] = df[y_column].values.astype(str).tolist()\n        else:\n            # If no y_column is provided, create a dummy y array\n            self.ys = [\"\"] * len(self.xs)\n\n        self.block_idx = 0\n        self.n_blocks = len(self.xs) // self.n_subsamples if self.n_subsamples &gt; 0 else 1\n        self.rng = np.random.default_rng(seed)\n\n        self.eval_cache: Dict[Tuple[str, str, str], float] = {}  # (prompt, x, y): scores per datapoint\n        self.seq_cache: Dict[Tuple[str, str, str], str] = {}  # (prompt, x, y): generating sequence per datapoint\n\n    def subsample(self, eval_strategy: \"EvalStrategy\" = None) -&gt; Tuple[List[str], List[str]]:\n        \"\"\"Subsample the dataset based on the specified parameters.\n\n        Args:\n            eval_strategy (EvalStrategy, optional): Subsampling strategy to use instead of self.eval_strategy. Defaults to None.\n\n        Returns:\n            Tuple[List[str], List[str]]: Subsampled input data and labels.\n        \"\"\"\n        if eval_strategy is None:\n            eval_strategy = self.eval_strategy\n\n        if eval_strategy in [\"full\", \"evaluated\"]:\n            return self.xs, self.ys\n        elif eval_strategy == \"subsample\":\n            indices = self.rng.choice(len(self.xs), min(self.n_subsamples, len(self.xs)), replace=False)\n            return [self.xs[i] for i in indices], [self.ys[i] for i in indices]\n        elif eval_strategy == \"random_block\":\n            block_id = self.rng.integers(0, self.n_blocks)\n            start_idx = block_id * self.n_subsamples\n            end_idx = min((block_id + 1) * self.n_subsamples, len(self.xs))\n            indices = np.arange(start_idx, end_idx)\n            return [self.xs[i] for i in indices], [self.ys[i] for i in indices]\n        elif eval_strategy == \"sequential_block\":\n            start_idx = self.block_idx * self.n_subsamples\n            end_idx = min((self.block_idx + 1) * self.n_subsamples, len(self.xs))\n            indices = np.arange(start_idx, end_idx)\n            return [self.xs[i] for i in indices], [self.ys[i] for i in indices]\n        else:\n            raise ValueError(f\"Unknown subsampling strategy: '{eval_strategy}'\")\n\n    def _prepare_batch(\n        self,\n        prompts: List[str],\n        xs: List[str],\n        ys: List[str],\n        eval_strategy: Literal[\"full\", \"subsample\", \"sequential_block\", \"random_block\", \"evaluated\"] = \"full\",\n    ) -&gt; List[Tuple[str, str, str]]:\n        \"\"\"Generates (prompt, x, y) keys that require prediction.\n\n        Returns keys not found in eval_cache.\n        \"\"\"\n        if eval_strategy == \"evaluated\":\n            return []\n        keys_to_predict = []\n        for prompt in prompts:\n            for x, y in zip(xs, ys):\n                cache_key = (prompt, x, str(y))\n                if cache_key not in self.eval_cache:\n                    keys_to_predict.append(cache_key)\n        return keys_to_predict\n\n    def _collect_results_from_cache(\n        self,\n        prompts: List[str],\n        xs: List[str],\n        ys: List[str],\n        return_agg_scores: bool,\n        return_seq: bool,\n    ) -&gt; Union[List[float], List[List[float]], Tuple[List[List[float]], List[List[str]]]]:\n        \"\"\"Collects all results for the current batch from the cache and formats them.\"\"\"\n        assert not (return_agg_scores and return_seq), \"Cannot return both aggregated scores and sequences\"\n\n        scores = []\n        seqs = []\n\n        for prompt in prompts:\n            datapoint_scores = []\n            datapoint_seqs = []\n            for x, y in zip(xs, ys):\n                cache_key = (prompt, x, y)\n                datapoint_scores.append(self.eval_cache[cache_key])\n                if return_seq:\n                    datapoint_seqs.append(self.seq_cache.get(cache_key, \"\"))\n            scores.append(datapoint_scores)\n            if return_seq:\n                seqs.append(datapoint_seqs)\n\n        if return_agg_scores:\n            agg_scores = [np.nanmean(s).item() for s in scores]\n            return agg_scores\n\n        return scores if not return_seq else (scores, seqs)\n\n    @abstractmethod\n    def _evaluate(self, xs: List[str], ys: List[str], preds: List[str]) -&gt; List[float]:\n        \"\"\"Abstract method to calculate the score for a predictions.\n\n        This method should be implemented by subclasses based on their specific evaluation logic.\n        \"\"\"\n        raise NotImplementedError\n\n    @overload\n    def evaluate(\n        self,\n        prompts: List[str],\n        predictor: \"BasePredictor\",\n        system_prompts: Optional[Union[str, List[str]]] = None,\n        return_agg_scores: Literal[True] = True,\n        return_seq: Literal[False] = False,\n        eval_strategy: Optional[\"EvalStrategy\"] = None,\n    ) -&gt; List[float]:\n        ...\n\n    @overload\n    def evaluate(\n        self,\n        prompts: List[str],\n        predictor: \"BasePredictor\",\n        system_prompts: Optional[Union[str, List[str]]] = None,\n        return_agg_scores: Literal[False] = False,\n        return_seq: Literal[False] = False,\n        eval_strategy: Optional[\"EvalStrategy\"] = None,\n    ) -&gt; List[List[float]]:\n        ...\n\n    @overload\n    def evaluate(\n        self,\n        prompts: List[str],\n        predictor: \"BasePredictor\",\n        system_prompts: Optional[Union[str, List[str]]] = None,\n        return_agg_scores: Literal[False] = False,\n        return_seq: Literal[True] = True,\n        eval_strategy: Optional[\"EvalStrategy\"] = None,\n    ) -&gt; Tuple[List[List[float]], List[List[str]]]:\n        ...\n\n    @overload\n    def evaluate(\n        self,\n        prompts: str,\n        predictor: \"BasePredictor\",\n        system_prompts: Optional[Union[str, List[str]]] = None,\n        return_agg_scores: Literal[True] = True,\n        return_seq: Literal[False] = False,\n        eval_strategy: Optional[\"EvalStrategy\"] = None,\n    ) -&gt; List[float]:\n        ...\n\n    @overload\n    def evaluate(\n        self,\n        prompts: str,\n        predictor: \"BasePredictor\",\n        system_prompts: Optional[Union[str, List[str]]] = None,\n        return_agg_scores: Literal[False] = False,\n        return_seq: Literal[False] = False,\n        eval_strategy: Optional[\"EvalStrategy\"] = None,\n    ) -&gt; List[List[float]]:\n        ...\n\n    @overload\n    def evaluate(\n        self,\n        prompts: str,\n        predictor: \"BasePredictor\",\n        system_prompts: Optional[Union[str, List[str]]] = None,\n        return_agg_scores: Literal[False] = False,\n        return_seq: Literal[True] = True,\n        eval_strategy: Optional[\"EvalStrategy\"] = None,\n    ) -&gt; Tuple[List[List[float]], List[List[str]]]:\n        ...\n\n    def evaluate(\n        self,\n        prompts: Union[str, List[str]],\n        predictor: \"BasePredictor\",\n        system_prompts: Optional[Union[str, List[str]]] = None,\n        return_agg_scores: bool = True,\n        return_seq: bool = False,\n        eval_strategy: Optional[\"EvalStrategy\"] = None,\n    ) -&gt; Union[List[float], List[List[float]], Tuple[List[List[float]], List[List[str]]]]:\n        \"\"\"Evaluate a set of prompts using a given predictor.\n\n        This method orchestrates subsampling, prediction, caching, and result collection.\n\n        Note: Cannot return both aggregated scores and sequences (assertion will fail).\n        \"\"\"\n        assert not (return_agg_scores and return_seq), \"Cannot return both aggregated scores and sequences\"\n\n        seqs: List[str] = []\n\n        prompts = [prompts] if isinstance(prompts, str) else prompts\n        eval_strategy = eval_strategy or self.eval_strategy\n        xs, ys = self.subsample(eval_strategy=eval_strategy)\n        batches = self._prepare_batch(prompts, xs, ys, eval_strategy=eval_strategy)\n        (prompts_to_evaluate, xs_to_evaluate, ys_to_evaluate) = ([], [], []) if not batches else zip(*batches)\n\n        if prompts_to_evaluate:\n            preds_seqs = predictor.predict(\n                prompts=list(prompts_to_evaluate),\n                xs=list(xs_to_evaluate),\n                system_prompts=system_prompts,\n                return_seq=return_seq,\n            )\n        else:\n            preds_seqs = ([], []) if return_seq else []\n\n        if return_seq:\n            preds, seqs = preds_seqs if isinstance(preds_seqs, tuple) else (preds_seqs, [])\n        else:\n            preds = preds_seqs\n\n        scores: List[float] = self._evaluate(list(xs_to_evaluate), list(ys_to_evaluate), preds)\n        for i, cache_key in enumerate(batches):\n            self.eval_cache[cache_key] = scores[i]\n            if return_seq:\n                self.seq_cache[cache_key] = seqs[i]\n\n        return self._collect_results_from_cache(\n            prompts,\n            xs,\n            ys,\n            return_agg_scores,\n            return_seq,\n        )\n\n    def pop_datapoints(self, n: Optional[int] = None, frac: Optional[float] = None) -&gt; pd.DataFrame:\n        \"\"\"Pop a number of datapoints from the dataset.\n\n        Args:\n            n (int, optional): Number of datapoints to pop. Defaults to None.\n            frac (float, optional): Fraction of datapoints to pop. Defaults to None.\n\n        Returns:\n            pd.DataFrame: DataFrame containing the popped datapoints.\n        \"\"\"\n        assert n is None or frac is None, \"Only one of n or frac can be specified.\"\n        if n is not None:\n            indices = self.rng.choice(len(self.xs), n, replace=False)\n        elif frac is not None:\n            indices = self.rng.choice(len(self.xs), int(len(self.xs) * frac), replace=False)\n        else:\n            raise ValueError(\"Either n or frac must be specified.\")\n\n        popped_xs = [self.xs[i] for i in indices]\n        popped_ys = [self.ys[i] for i in indices]\n        df_popped = pd.DataFrame({self.x_column: popped_xs, self.y_column: popped_ys})\n\n        self.xs = [x for i, x in enumerate(self.xs) if i not in indices]\n        self.ys = [y for i, y in enumerate(self.ys) if i not in indices]\n\n        # Update n_blocks and block_idx based on the new dataset size\n        self.n_blocks = len(self.xs) // self.n_subsamples if self.n_subsamples &gt; 0 else 1\n        self.block_idx = min(self.block_idx, self.n_blocks - 1) if self.n_blocks &gt; 0 else 0\n\n        # Clear cache for popped items (optional, but good practice if memory is a concern)\n        keys_to_remove = []\n        for key in self.eval_cache:\n            if key[1] in popped_xs and key[2] in popped_ys:  # Check if the x and y correspond to popped data\n                keys_to_remove.append(key)\n        for key in keys_to_remove:\n            self.eval_cache.pop(key, None)\n            self.seq_cache.pop(key, None)\n\n        return df_popped\n\n    def increment_block_idx(self) -&gt; None:\n        \"\"\"Increment the block index for subsampling.\n\n        Raises:\n            ValueError: If the eval_strategy does not contain \"block\".\n        \"\"\"\n        if \"block\" not in self.eval_strategy:\n            raise ValueError(\"Block increment is only valid for block subsampling.\")\n        self.block_idx += 1\n        if self.n_blocks &gt; 0:  # Ensure n_blocks is not zero to avoid division by zero\n            self.block_idx %= self.n_blocks\n        else:\n            self.block_idx = 0  # If no blocks, reset to 0\n\n    def reset_block_idx(self) -&gt; None:\n        \"\"\"Reset the block index for subsampling.\n\n        Raises:\n            ValueError: If the eval_strategy does not contain \"block\".\n        \"\"\"\n        if \"block\" not in self.eval_strategy:\n            raise ValueError(\"Block reset is only valid for block subsampling.\")\n        self.block_idx = 0\n</code></pre>"},{"location":"api/tasks/#promptolution.tasks.base_task.BaseTask.__init__","title":"<code>__init__(df, x_column, y_column=None, task_description=None, n_subsamples=30, eval_strategy='full', seed=42, config=None)</code>","text":"<p>Initialize the BaseTask.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>The input DataFrame containing the data.</p> required <code>x_column</code> <code>str</code> <p>Name of the column containing input texts.</p> required <code>y_column</code> <code>Optional[str]</code> <p>Name of the column containing labels/ground truth (if applicable).</p> <code>None</code> <code>task_description</code> <code>str</code> <p>Description of the task.</p> <code>None</code> <code>n_subsamples</code> <code>int</code> <p>Number of subsamples to use for evaluation.</p> <code>30</code> <code>eval_strategy</code> <code>Literal</code> <p>Subsampling strategy (\"full\", \"subsample\", \"sequential_block\", \"random_block\", \"evaluated\").</p> <code>'full'</code> <code>seed</code> <code>int</code> <p>Random seed for reproducibility.</p> <code>42</code> <code>config</code> <code>ExperimentConfig</code> <p>Configuration for the task, overriding defaults.</p> <code>None</code> Source code in <code>promptolution/tasks/base_task.py</code> <pre><code>def __init__(\n    self,\n    df: pd.DataFrame,\n    x_column: str,\n    y_column: Optional[str] = None,\n    task_description: Optional[str] = None,\n    n_subsamples: int = 30,\n    eval_strategy: \"EvalStrategy\" = \"full\",\n    seed: int = 42,\n    config: Optional[\"ExperimentConfig\"] = None,\n) -&gt; None:\n    \"\"\"Initialize the BaseTask.\n\n    Args:\n        df (pd.DataFrame): The input DataFrame containing the data.\n        x_column (str): Name of the column containing input texts.\n        y_column (Optional[str]): Name of the column containing labels/ground truth (if applicable).\n        task_description (str): Description of the task.\n        n_subsamples (int): Number of subsamples to use for evaluation.\n        eval_strategy (Literal): Subsampling strategy (\"full\", \"subsample\", \"sequential_block\", \"random_block\", \"evaluated\").\n        seed (int): Random seed for reproducibility.\n        config (ExperimentConfig, optional): Configuration for the task, overriding defaults.\n    \"\"\"\n    self.df = df\n    self.x_column = x_column\n    self.y_column = y_column\n    self.task_description = task_description\n    self.n_subsamples = n_subsamples\n    self.eval_strategy = eval_strategy\n    self.seed = seed\n\n    super().__init__()\n    if config is not None:\n        config.apply_to(self)\n\n    self.xs: List[str] = df[self.x_column].values.astype(str).tolist()\n    self.has_y = y_column is not None\n    if self.has_y and y_column is not None:\n        self.ys: List[str] = df[y_column].values.astype(str).tolist()\n    else:\n        # If no y_column is provided, create a dummy y array\n        self.ys = [\"\"] * len(self.xs)\n\n    self.block_idx = 0\n    self.n_blocks = len(self.xs) // self.n_subsamples if self.n_subsamples &gt; 0 else 1\n    self.rng = np.random.default_rng(seed)\n\n    self.eval_cache: Dict[Tuple[str, str, str], float] = {}  # (prompt, x, y): scores per datapoint\n    self.seq_cache: Dict[Tuple[str, str, str], str] = {}  # (prompt, x, y): generating sequence per datapoint\n</code></pre>"},{"location":"api/tasks/#promptolution.tasks.base_task.BaseTask.evaluate","title":"<code>evaluate(prompts, predictor, system_prompts=None, return_agg_scores=True, return_seq=False, eval_strategy=None)</code>","text":"<pre><code>evaluate(prompts: List[str], predictor: BasePredictor, system_prompts: Optional[Union[str, List[str]]] = None, return_agg_scores: Literal[True] = True, return_seq: Literal[False] = False, eval_strategy: Optional[EvalStrategy] = None) -&gt; List[float]\n</code></pre><pre><code>evaluate(prompts: List[str], predictor: BasePredictor, system_prompts: Optional[Union[str, List[str]]] = None, return_agg_scores: Literal[False] = False, return_seq: Literal[False] = False, eval_strategy: Optional[EvalStrategy] = None) -&gt; List[List[float]]\n</code></pre><pre><code>evaluate(prompts: List[str], predictor: BasePredictor, system_prompts: Optional[Union[str, List[str]]] = None, return_agg_scores: Literal[False] = False, return_seq: Literal[True] = True, eval_strategy: Optional[EvalStrategy] = None) -&gt; Tuple[List[List[float]], List[List[str]]]\n</code></pre><pre><code>evaluate(prompts: str, predictor: BasePredictor, system_prompts: Optional[Union[str, List[str]]] = None, return_agg_scores: Literal[True] = True, return_seq: Literal[False] = False, eval_strategy: Optional[EvalStrategy] = None) -&gt; List[float]\n</code></pre><pre><code>evaluate(prompts: str, predictor: BasePredictor, system_prompts: Optional[Union[str, List[str]]] = None, return_agg_scores: Literal[False] = False, return_seq: Literal[False] = False, eval_strategy: Optional[EvalStrategy] = None) -&gt; List[List[float]]\n</code></pre><pre><code>evaluate(prompts: str, predictor: BasePredictor, system_prompts: Optional[Union[str, List[str]]] = None, return_agg_scores: Literal[False] = False, return_seq: Literal[True] = True, eval_strategy: Optional[EvalStrategy] = None) -&gt; Tuple[List[List[float]], List[List[str]]]\n</code></pre> <p>Evaluate a set of prompts using a given predictor.</p> <p>This method orchestrates subsampling, prediction, caching, and result collection.</p> <p>Note: Cannot return both aggregated scores and sequences (assertion will fail).</p> Source code in <code>promptolution/tasks/base_task.py</code> <pre><code>def evaluate(\n    self,\n    prompts: Union[str, List[str]],\n    predictor: \"BasePredictor\",\n    system_prompts: Optional[Union[str, List[str]]] = None,\n    return_agg_scores: bool = True,\n    return_seq: bool = False,\n    eval_strategy: Optional[\"EvalStrategy\"] = None,\n) -&gt; Union[List[float], List[List[float]], Tuple[List[List[float]], List[List[str]]]]:\n    \"\"\"Evaluate a set of prompts using a given predictor.\n\n    This method orchestrates subsampling, prediction, caching, and result collection.\n\n    Note: Cannot return both aggregated scores and sequences (assertion will fail).\n    \"\"\"\n    assert not (return_agg_scores and return_seq), \"Cannot return both aggregated scores and sequences\"\n\n    seqs: List[str] = []\n\n    prompts = [prompts] if isinstance(prompts, str) else prompts\n    eval_strategy = eval_strategy or self.eval_strategy\n    xs, ys = self.subsample(eval_strategy=eval_strategy)\n    batches = self._prepare_batch(prompts, xs, ys, eval_strategy=eval_strategy)\n    (prompts_to_evaluate, xs_to_evaluate, ys_to_evaluate) = ([], [], []) if not batches else zip(*batches)\n\n    if prompts_to_evaluate:\n        preds_seqs = predictor.predict(\n            prompts=list(prompts_to_evaluate),\n            xs=list(xs_to_evaluate),\n            system_prompts=system_prompts,\n            return_seq=return_seq,\n        )\n    else:\n        preds_seqs = ([], []) if return_seq else []\n\n    if return_seq:\n        preds, seqs = preds_seqs if isinstance(preds_seqs, tuple) else (preds_seqs, [])\n    else:\n        preds = preds_seqs\n\n    scores: List[float] = self._evaluate(list(xs_to_evaluate), list(ys_to_evaluate), preds)\n    for i, cache_key in enumerate(batches):\n        self.eval_cache[cache_key] = scores[i]\n        if return_seq:\n            self.seq_cache[cache_key] = seqs[i]\n\n    return self._collect_results_from_cache(\n        prompts,\n        xs,\n        ys,\n        return_agg_scores,\n        return_seq,\n    )\n</code></pre>"},{"location":"api/tasks/#promptolution.tasks.base_task.BaseTask.increment_block_idx","title":"<code>increment_block_idx()</code>","text":"<p>Increment the block index for subsampling.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the eval_strategy does not contain \"block\".</p> Source code in <code>promptolution/tasks/base_task.py</code> <pre><code>def increment_block_idx(self) -&gt; None:\n    \"\"\"Increment the block index for subsampling.\n\n    Raises:\n        ValueError: If the eval_strategy does not contain \"block\".\n    \"\"\"\n    if \"block\" not in self.eval_strategy:\n        raise ValueError(\"Block increment is only valid for block subsampling.\")\n    self.block_idx += 1\n    if self.n_blocks &gt; 0:  # Ensure n_blocks is not zero to avoid division by zero\n        self.block_idx %= self.n_blocks\n    else:\n        self.block_idx = 0  # If no blocks, reset to 0\n</code></pre>"},{"location":"api/tasks/#promptolution.tasks.base_task.BaseTask.pop_datapoints","title":"<code>pop_datapoints(n=None, frac=None)</code>","text":"<p>Pop a number of datapoints from the dataset.</p> <p>Parameters:</p> Name Type Description Default <code>n</code> <code>int</code> <p>Number of datapoints to pop. Defaults to None.</p> <code>None</code> <code>frac</code> <code>float</code> <p>Fraction of datapoints to pop. Defaults to None.</p> <code>None</code> <p>Returns:</p> Type Description <code>DataFrame</code> <p>pd.DataFrame: DataFrame containing the popped datapoints.</p> Source code in <code>promptolution/tasks/base_task.py</code> <pre><code>def pop_datapoints(self, n: Optional[int] = None, frac: Optional[float] = None) -&gt; pd.DataFrame:\n    \"\"\"Pop a number of datapoints from the dataset.\n\n    Args:\n        n (int, optional): Number of datapoints to pop. Defaults to None.\n        frac (float, optional): Fraction of datapoints to pop. Defaults to None.\n\n    Returns:\n        pd.DataFrame: DataFrame containing the popped datapoints.\n    \"\"\"\n    assert n is None or frac is None, \"Only one of n or frac can be specified.\"\n    if n is not None:\n        indices = self.rng.choice(len(self.xs), n, replace=False)\n    elif frac is not None:\n        indices = self.rng.choice(len(self.xs), int(len(self.xs) * frac), replace=False)\n    else:\n        raise ValueError(\"Either n or frac must be specified.\")\n\n    popped_xs = [self.xs[i] for i in indices]\n    popped_ys = [self.ys[i] for i in indices]\n    df_popped = pd.DataFrame({self.x_column: popped_xs, self.y_column: popped_ys})\n\n    self.xs = [x for i, x in enumerate(self.xs) if i not in indices]\n    self.ys = [y for i, y in enumerate(self.ys) if i not in indices]\n\n    # Update n_blocks and block_idx based on the new dataset size\n    self.n_blocks = len(self.xs) // self.n_subsamples if self.n_subsamples &gt; 0 else 1\n    self.block_idx = min(self.block_idx, self.n_blocks - 1) if self.n_blocks &gt; 0 else 0\n\n    # Clear cache for popped items (optional, but good practice if memory is a concern)\n    keys_to_remove = []\n    for key in self.eval_cache:\n        if key[1] in popped_xs and key[2] in popped_ys:  # Check if the x and y correspond to popped data\n            keys_to_remove.append(key)\n    for key in keys_to_remove:\n        self.eval_cache.pop(key, None)\n        self.seq_cache.pop(key, None)\n\n    return df_popped\n</code></pre>"},{"location":"api/tasks/#promptolution.tasks.base_task.BaseTask.reset_block_idx","title":"<code>reset_block_idx()</code>","text":"<p>Reset the block index for subsampling.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the eval_strategy does not contain \"block\".</p> Source code in <code>promptolution/tasks/base_task.py</code> <pre><code>def reset_block_idx(self) -&gt; None:\n    \"\"\"Reset the block index for subsampling.\n\n    Raises:\n        ValueError: If the eval_strategy does not contain \"block\".\n    \"\"\"\n    if \"block\" not in self.eval_strategy:\n        raise ValueError(\"Block reset is only valid for block subsampling.\")\n    self.block_idx = 0\n</code></pre>"},{"location":"api/tasks/#promptolution.tasks.base_task.BaseTask.subsample","title":"<code>subsample(eval_strategy=None)</code>","text":"<p>Subsample the dataset based on the specified parameters.</p> <p>Parameters:</p> Name Type Description Default <code>eval_strategy</code> <code>EvalStrategy</code> <p>Subsampling strategy to use instead of self.eval_strategy. Defaults to None.</p> <code>None</code> <p>Returns:</p> Type Description <code>Tuple[List[str], List[str]]</code> <p>Tuple[List[str], List[str]]: Subsampled input data and labels.</p> Source code in <code>promptolution/tasks/base_task.py</code> <pre><code>def subsample(self, eval_strategy: \"EvalStrategy\" = None) -&gt; Tuple[List[str], List[str]]:\n    \"\"\"Subsample the dataset based on the specified parameters.\n\n    Args:\n        eval_strategy (EvalStrategy, optional): Subsampling strategy to use instead of self.eval_strategy. Defaults to None.\n\n    Returns:\n        Tuple[List[str], List[str]]: Subsampled input data and labels.\n    \"\"\"\n    if eval_strategy is None:\n        eval_strategy = self.eval_strategy\n\n    if eval_strategy in [\"full\", \"evaluated\"]:\n        return self.xs, self.ys\n    elif eval_strategy == \"subsample\":\n        indices = self.rng.choice(len(self.xs), min(self.n_subsamples, len(self.xs)), replace=False)\n        return [self.xs[i] for i in indices], [self.ys[i] for i in indices]\n    elif eval_strategy == \"random_block\":\n        block_id = self.rng.integers(0, self.n_blocks)\n        start_idx = block_id * self.n_subsamples\n        end_idx = min((block_id + 1) * self.n_subsamples, len(self.xs))\n        indices = np.arange(start_idx, end_idx)\n        return [self.xs[i] for i in indices], [self.ys[i] for i in indices]\n    elif eval_strategy == \"sequential_block\":\n        start_idx = self.block_idx * self.n_subsamples\n        end_idx = min((self.block_idx + 1) * self.n_subsamples, len(self.xs))\n        indices = np.arange(start_idx, end_idx)\n        return [self.xs[i] for i in indices], [self.ys[i] for i in indices]\n    else:\n        raise ValueError(f\"Unknown subsampling strategy: '{eval_strategy}'\")\n</code></pre>"},{"location":"api/tasks/#promptolution.tasks.classification_tasks","title":"<code>classification_tasks</code>","text":"<p>Module for classification tasks.</p>"},{"location":"api/tasks/#promptolution.tasks.classification_tasks.ClassificationTask","title":"<code>ClassificationTask</code>","text":"<p>               Bases: <code>BaseTask</code></p> <p>A class representing a classification task in the promptolution library.</p> <p>This class handles the loading and management of classification datasets, as well as the evaluation of predictors on these datasets.</p> Source code in <code>promptolution/tasks/classification_tasks.py</code> <pre><code>class ClassificationTask(BaseTask):\n    \"\"\"A class representing a classification task in the promptolution library.\n\n    This class handles the loading and management of classification datasets,\n    as well as the evaluation of predictors on these datasets.\n    \"\"\"\n\n    def __init__(\n        self,\n        df: pd.DataFrame,\n        task_description: Optional[str] = None,\n        x_column: str = \"x\",\n        y_column: str = \"y\",\n        n_subsamples: int = 30,\n        eval_strategy: Literal[\"full\", \"subsample\", \"sequential_block\", \"random_block\"] = \"full\",\n        seed: int = 42,\n        metric: Callable[[Any, Any], float] = accuracy_score,\n        config: Optional[\"ExperimentConfig\"] = None,\n    ) -&gt; None:\n        \"\"\"Initialize the ClassificationTask from a pandas DataFrame.\n\n        Args:\n            df (pd.DataFrame): Input DataFrame containing the data\n            task_description (str): Description of the task\n            x_column (str, optional): Name of the column containing input texts. Defaults to \"x\".\n            y_column (str, optional): Name of the column containing labels. Defaults to \"y\".\n            n_subsamples (int, optional): Number of subsamples to use. No subsampling if None. Defaults to None.\n            eval_strategy (str, optional): Subsampling strategy to use. Options:\n                - \"full\": Uses the entire dataset for evaluation.\n                - \"evaluated\": Uses only previously evaluated datapoints from the cache.\n                - \"subsample\": Randomly selects n_subsamples datapoints without replacement.\n                - \"sequential_block\": Uses a block of block_size consecutive datapoints, advancing through blocks sequentially.\n                - \"random_block\": Randomly selects a block of block_size consecutive datapoints.\n                Defaults to \"full\".\n            seed (int, optional): Random seed for reproducibility. Defaults to 42.\n            metric (Callable, optional): Metric to use for evaluation. Defaults to accuracy_score.\n            config (ExperimentConfig, optional): Configuration for the task, overriding defaults.\n        \"\"\"\n        self.metric = metric\n        super().__init__(\n            df=df,\n            x_column=x_column,\n            y_column=y_column,\n            task_description=task_description,\n            n_subsamples=n_subsamples,\n            eval_strategy=eval_strategy,\n            seed=seed,\n            config=config,\n        )\n        self.ys: List[str] = (\n            df[self.y_column].str.lower().values.tolist()\n        )  # Ensure y values are lowercase for consistent comparison\n        self.classes = np.unique(self.ys)\n\n    def _evaluate(self, xs: List[str], ys: List[str], preds: List[str]) -&gt; List[float]:\n        \"\"\"Calculate the score for a single prediction.\"\"\"\n        scores = []\n        for pred, y in zip(preds, ys):\n            scores.append(self.metric([y], [pred]))\n        return scores\n</code></pre>"},{"location":"api/tasks/#promptolution.tasks.classification_tasks.ClassificationTask.__init__","title":"<code>__init__(df, task_description=None, x_column='x', y_column='y', n_subsamples=30, eval_strategy='full', seed=42, metric=accuracy_score, config=None)</code>","text":"<p>Initialize the ClassificationTask from a pandas DataFrame.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>Input DataFrame containing the data</p> required <code>task_description</code> <code>str</code> <p>Description of the task</p> <code>None</code> <code>x_column</code> <code>str</code> <p>Name of the column containing input texts. Defaults to \"x\".</p> <code>'x'</code> <code>y_column</code> <code>str</code> <p>Name of the column containing labels. Defaults to \"y\".</p> <code>'y'</code> <code>n_subsamples</code> <code>int</code> <p>Number of subsamples to use. No subsampling if None. Defaults to None.</p> <code>30</code> <code>eval_strategy</code> <code>str</code> <p>Subsampling strategy to use. Options: - \"full\": Uses the entire dataset for evaluation. - \"evaluated\": Uses only previously evaluated datapoints from the cache. - \"subsample\": Randomly selects n_subsamples datapoints without replacement. - \"sequential_block\": Uses a block of block_size consecutive datapoints, advancing through blocks sequentially. - \"random_block\": Randomly selects a block of block_size consecutive datapoints. Defaults to \"full\".</p> <code>'full'</code> <code>seed</code> <code>int</code> <p>Random seed for reproducibility. Defaults to 42.</p> <code>42</code> <code>metric</code> <code>Callable</code> <p>Metric to use for evaluation. Defaults to accuracy_score.</p> <code>accuracy_score</code> <code>config</code> <code>ExperimentConfig</code> <p>Configuration for the task, overriding defaults.</p> <code>None</code> Source code in <code>promptolution/tasks/classification_tasks.py</code> <pre><code>def __init__(\n    self,\n    df: pd.DataFrame,\n    task_description: Optional[str] = None,\n    x_column: str = \"x\",\n    y_column: str = \"y\",\n    n_subsamples: int = 30,\n    eval_strategy: Literal[\"full\", \"subsample\", \"sequential_block\", \"random_block\"] = \"full\",\n    seed: int = 42,\n    metric: Callable[[Any, Any], float] = accuracy_score,\n    config: Optional[\"ExperimentConfig\"] = None,\n) -&gt; None:\n    \"\"\"Initialize the ClassificationTask from a pandas DataFrame.\n\n    Args:\n        df (pd.DataFrame): Input DataFrame containing the data\n        task_description (str): Description of the task\n        x_column (str, optional): Name of the column containing input texts. Defaults to \"x\".\n        y_column (str, optional): Name of the column containing labels. Defaults to \"y\".\n        n_subsamples (int, optional): Number of subsamples to use. No subsampling if None. Defaults to None.\n        eval_strategy (str, optional): Subsampling strategy to use. Options:\n            - \"full\": Uses the entire dataset for evaluation.\n            - \"evaluated\": Uses only previously evaluated datapoints from the cache.\n            - \"subsample\": Randomly selects n_subsamples datapoints without replacement.\n            - \"sequential_block\": Uses a block of block_size consecutive datapoints, advancing through blocks sequentially.\n            - \"random_block\": Randomly selects a block of block_size consecutive datapoints.\n            Defaults to \"full\".\n        seed (int, optional): Random seed for reproducibility. Defaults to 42.\n        metric (Callable, optional): Metric to use for evaluation. Defaults to accuracy_score.\n        config (ExperimentConfig, optional): Configuration for the task, overriding defaults.\n    \"\"\"\n    self.metric = metric\n    super().__init__(\n        df=df,\n        x_column=x_column,\n        y_column=y_column,\n        task_description=task_description,\n        n_subsamples=n_subsamples,\n        eval_strategy=eval_strategy,\n        seed=seed,\n        config=config,\n    )\n    self.ys: List[str] = (\n        df[self.y_column].str.lower().values.tolist()\n    )  # Ensure y values are lowercase for consistent comparison\n    self.classes = np.unique(self.ys)\n</code></pre>"},{"location":"api/tasks/#promptolution.tasks.judge_tasks","title":"<code>judge_tasks</code>","text":"<p>Module for judge tasks.</p>"},{"location":"api/tasks/#promptolution.tasks.judge_tasks.JudgeTask","title":"<code>JudgeTask</code>","text":"<p>               Bases: <code>BaseTask</code></p> <p>Task that evaluates a predictor using an LLM as a judge, optionally accepting a ground truth.</p> Source code in <code>promptolution/tasks/judge_tasks.py</code> <pre><code>class JudgeTask(BaseTask):\n    \"\"\"Task that evaluates a predictor using an LLM as a judge, optionally accepting a ground truth.\"\"\"\n\n    def __init__(\n        self,\n        df: pd.DataFrame,\n        judge_llm: \"BaseLLM\",\n        x_column: str = \"x\",\n        y_column: Optional[str] = None,\n        task_description: Optional[str] = None,\n        n_subsamples: int = 30,\n        eval_strategy: \"EvalStrategy\" = \"full\",\n        seed: int = 42,\n        judge_prompt: Optional[str] = None,\n        min_score: float = -5.0,\n        max_score: float = 5.0,\n        config: \"ExperimentConfig\" = None,\n    ):\n        \"\"\"Initialize the JudgeTask.\n\n        Args:\n            df (pd.DataFrame): The input DataFrame containing the data.\n            judge_llm (BaseLLM): The LLM judging the predictions.\n            x_column (str): Name of the column containing input texts.\n            y_column (Optional[str]): Name of the column containing labels/ground truth (if applicable).\n            task_description (Optional[str]): Description of the task, parsed to the Judge-LLM and Meta-LLM.\n            n_subsamples (int): Number of subsamples to use for evaluation.\n            eval_strategy (EvalStrategy): Subsampling strategy to use for evaluation.\n            seed (int): Random seed for reproducibility.\n            judge_prompt (Optional[str]): Custom prompt for the judge. Note: The score of the Judge will be extracted inside &lt;final_score&gt; tags.\n            min_score (float): Minimum score for evaluation.\n            max_score (float): Maximum score for evaluation.\n            config (ExperimentConfig, optional): Configuration for the task, overriding defaults.\n        \"\"\"\n        if judge_prompt is None:\n            judge_prompt = JUDGE_PROMPT_WITH_GROUND_TRUTH if y_column else JUDGE_PROMPT_WITHOUT_GROUND_TRUTH\n        self.judge_prompt = judge_prompt\n        self.min_score = min_score\n        self.max_score = max_score\n\n        super().__init__(\n            df=df,\n            x_column=x_column,\n            y_column=y_column,\n            task_description=task_description,\n            n_subsamples=n_subsamples,\n            eval_strategy=eval_strategy,\n            seed=seed,\n            config=config,\n        )\n        self.judge_llm = judge_llm\n\n    def _construct_judge_prompt(self, x: str, pred: str, y: Optional[str] = None) -&gt; str:\n        \"\"\"Constructs the judge prompt based on whether ground truth is available.\"\"\"\n        if y is not None:\n            prompt = self.judge_prompt.replace(\"{ground_truth}\", str(y))\n        else:\n            prompt = self.judge_prompt\n\n        task_description = self.task_description or \"\"\n        prompt = prompt.replace(\"{task}\", task_description).replace(\"{input}\", x).replace(\"{prediction}\", pred)\n        return prompt\n\n    def _evaluate(self, xs: List[str], ys: List[str], preds: List[str]) -&gt; List[float]:\n        \"\"\"Calculate the score for a single prediction using the LLM judge.\"\"\"\n        prompts: List[str] = []\n        for x, y, pred in zip(xs, ys, preds):\n            judge_prompt = self._construct_judge_prompt(x, pred, y)\n            prompts.append(judge_prompt)\n        judge_responses = self.judge_llm.get_response(prompts)\n        scores_str = extract_from_tag(judge_responses, \"&lt;final_score&gt;\", \"&lt;/final_score&gt;\")\n        scores = []\n        for score_str, judge_response in zip(scores_str, judge_responses):\n            try:\n                # only numeric chars, - or . are allowed\n                score_str = \"\".join(filter(lambda c: c.isdigit() or c in \"-.\", score_str))\n                score = float(score_str)\n                # normalize from [min_score, max_score] to [0, 1]\n                score = (score - self.min_score) / (self.max_score - self.min_score)\n                score = max(0.0, min(1.0, score))\n            except ValueError:\n                logger.warning(f\"Failed to parse score '{score}' as float. Defaulting to a score 0.0.\")\n                score = 0.0\n\n            scores.append(score)\n\n        return scores\n</code></pre>"},{"location":"api/tasks/#promptolution.tasks.judge_tasks.JudgeTask.__init__","title":"<code>__init__(df, judge_llm, x_column='x', y_column=None, task_description=None, n_subsamples=30, eval_strategy='full', seed=42, judge_prompt=None, min_score=-5.0, max_score=5.0, config=None)</code>","text":"<p>Initialize the JudgeTask.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>The input DataFrame containing the data.</p> required <code>judge_llm</code> <code>BaseLLM</code> <p>The LLM judging the predictions.</p> required <code>x_column</code> <code>str</code> <p>Name of the column containing input texts.</p> <code>'x'</code> <code>y_column</code> <code>Optional[str]</code> <p>Name of the column containing labels/ground truth (if applicable).</p> <code>None</code> <code>task_description</code> <code>Optional[str]</code> <p>Description of the task, parsed to the Judge-LLM and Meta-LLM.</p> <code>None</code> <code>n_subsamples</code> <code>int</code> <p>Number of subsamples to use for evaluation.</p> <code>30</code> <code>eval_strategy</code> <code>EvalStrategy</code> <p>Subsampling strategy to use for evaluation.</p> <code>'full'</code> <code>seed</code> <code>int</code> <p>Random seed for reproducibility.</p> <code>42</code> <code>judge_prompt</code> <code>Optional[str]</code> <p>Custom prompt for the judge. Note: The score of the Judge will be extracted inside  tags. <code>None</code> <code>min_score</code> <code>float</code> <p>Minimum score for evaluation.</p> <code>-5.0</code> <code>max_score</code> <code>float</code> <p>Maximum score for evaluation.</p> <code>5.0</code> <code>config</code> <code>ExperimentConfig</code> <p>Configuration for the task, overriding defaults.</p> <code>None</code> Source code in <code>promptolution/tasks/judge_tasks.py</code> <pre><code>def __init__(\n    self,\n    df: pd.DataFrame,\n    judge_llm: \"BaseLLM\",\n    x_column: str = \"x\",\n    y_column: Optional[str] = None,\n    task_description: Optional[str] = None,\n    n_subsamples: int = 30,\n    eval_strategy: \"EvalStrategy\" = \"full\",\n    seed: int = 42,\n    judge_prompt: Optional[str] = None,\n    min_score: float = -5.0,\n    max_score: float = 5.0,\n    config: \"ExperimentConfig\" = None,\n):\n    \"\"\"Initialize the JudgeTask.\n\n    Args:\n        df (pd.DataFrame): The input DataFrame containing the data.\n        judge_llm (BaseLLM): The LLM judging the predictions.\n        x_column (str): Name of the column containing input texts.\n        y_column (Optional[str]): Name of the column containing labels/ground truth (if applicable).\n        task_description (Optional[str]): Description of the task, parsed to the Judge-LLM and Meta-LLM.\n        n_subsamples (int): Number of subsamples to use for evaluation.\n        eval_strategy (EvalStrategy): Subsampling strategy to use for evaluation.\n        seed (int): Random seed for reproducibility.\n        judge_prompt (Optional[str]): Custom prompt for the judge. Note: The score of the Judge will be extracted inside &lt;final_score&gt; tags.\n        min_score (float): Minimum score for evaluation.\n        max_score (float): Maximum score for evaluation.\n        config (ExperimentConfig, optional): Configuration for the task, overriding defaults.\n    \"\"\"\n    if judge_prompt is None:\n        judge_prompt = JUDGE_PROMPT_WITH_GROUND_TRUTH if y_column else JUDGE_PROMPT_WITHOUT_GROUND_TRUTH\n    self.judge_prompt = judge_prompt\n    self.min_score = min_score\n    self.max_score = max_score\n\n    super().__init__(\n        df=df,\n        x_column=x_column,\n        y_column=y_column,\n        task_description=task_description,\n        n_subsamples=n_subsamples,\n        eval_strategy=eval_strategy,\n        seed=seed,\n        config=config,\n    )\n    self.judge_llm = judge_llm\n</code></pre>"},{"location":"api/tasks/#promptolution.tasks.reward_tasks","title":"<code>reward_tasks</code>","text":"<p>Module for Reward tasks.</p>"},{"location":"api/tasks/#promptolution.tasks.reward_tasks.RewardTask","title":"<code>RewardTask</code>","text":"<p>               Bases: <code>BaseTask</code></p> <p>A task that evaluates a predictor using a reward function.</p> <p>This task takes a DataFrame, a column name for input data, and a reward function. The reward function takes in a prediction as input and returns a scalar reward.</p> Source code in <code>promptolution/tasks/reward_tasks.py</code> <pre><code>class RewardTask(BaseTask):\n    \"\"\"A task that evaluates a predictor using a reward function.\n\n    This task takes a DataFrame, a column name for input data, and a reward function.\n    The reward function takes in a prediction as input and returns a scalar reward.\n    \"\"\"\n\n    def __init__(\n        self,\n        df: pd.DataFrame,\n        reward_function: Callable[[str], float],\n        x_column: str = \"x\",\n        task_description: Optional[str] = None,\n        n_subsamples: int = 30,\n        eval_strategy: \"EvalStrategy\" = \"full\",\n        seed: int = 42,\n        config: Optional[\"ExperimentConfig\"] = None,\n    ) -&gt; None:\n        \"\"\"Initialize the RewardTask.\n\n        Args:\n            df (pd.DataFrame): Input DataFrame containing the data.\n            reward_function (Callable): Function that takes a prediction and returns a reward score. Note: The optimizers aim to maximize.\n            x_column (str, optional): Name of the column containing input texts. Defaults to \"x\".\n            task_description (str, optional): Description of the task.\n            n_subsamples (int, optional): Number of subsamples to use. Defaults to 30.\n            eval_strategy (str, optional): Subsampling strategy to use. Defaults to \"full\".\n            seed (int, optional): Random seed for reproducibility. Defaults to 42.\n            config (ExperimentConfig, optional): Configuration for the task, overriding defaults.\n        \"\"\"\n        self.reward_function = reward_function\n        super().__init__(\n            df=df,\n            x_column=x_column,\n            task_description=task_description,\n            n_subsamples=n_subsamples,\n            eval_strategy=eval_strategy,\n            seed=seed,\n            config=config,\n        )\n\n    def _evaluate(self, xs: List[str], ys: List[str], preds: List[str]) -&gt; List[float]:\n        \"\"\"Calculate the score for a single reward prediction using the reward function.\"\"\"\n        rewards = [self.reward_function(pred) for pred in preds]\n        return rewards\n</code></pre>"},{"location":"api/tasks/#promptolution.tasks.reward_tasks.RewardTask.__init__","title":"<code>__init__(df, reward_function, x_column='x', task_description=None, n_subsamples=30, eval_strategy='full', seed=42, config=None)</code>","text":"<p>Initialize the RewardTask.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>Input DataFrame containing the data.</p> required <code>reward_function</code> <code>Callable</code> <p>Function that takes a prediction and returns a reward score. Note: The optimizers aim to maximize.</p> required <code>x_column</code> <code>str</code> <p>Name of the column containing input texts. Defaults to \"x\".</p> <code>'x'</code> <code>task_description</code> <code>str</code> <p>Description of the task.</p> <code>None</code> <code>n_subsamples</code> <code>int</code> <p>Number of subsamples to use. Defaults to 30.</p> <code>30</code> <code>eval_strategy</code> <code>str</code> <p>Subsampling strategy to use. Defaults to \"full\".</p> <code>'full'</code> <code>seed</code> <code>int</code> <p>Random seed for reproducibility. Defaults to 42.</p> <code>42</code> <code>config</code> <code>ExperimentConfig</code> <p>Configuration for the task, overriding defaults.</p> <code>None</code> Source code in <code>promptolution/tasks/reward_tasks.py</code> <pre><code>def __init__(\n    self,\n    df: pd.DataFrame,\n    reward_function: Callable[[str], float],\n    x_column: str = \"x\",\n    task_description: Optional[str] = None,\n    n_subsamples: int = 30,\n    eval_strategy: \"EvalStrategy\" = \"full\",\n    seed: int = 42,\n    config: Optional[\"ExperimentConfig\"] = None,\n) -&gt; None:\n    \"\"\"Initialize the RewardTask.\n\n    Args:\n        df (pd.DataFrame): Input DataFrame containing the data.\n        reward_function (Callable): Function that takes a prediction and returns a reward score. Note: The optimizers aim to maximize.\n        x_column (str, optional): Name of the column containing input texts. Defaults to \"x\".\n        task_description (str, optional): Description of the task.\n        n_subsamples (int, optional): Number of subsamples to use. Defaults to 30.\n        eval_strategy (str, optional): Subsampling strategy to use. Defaults to \"full\".\n        seed (int, optional): Random seed for reproducibility. Defaults to 42.\n        config (ExperimentConfig, optional): Configuration for the task, overriding defaults.\n    \"\"\"\n    self.reward_function = reward_function\n    super().__init__(\n        df=df,\n        x_column=x_column,\n        task_description=task_description,\n        n_subsamples=n_subsamples,\n        eval_strategy=eval_strategy,\n        seed=seed,\n        config=config,\n    )\n</code></pre>"},{"location":"api/utils/","title":"Utils","text":""},{"location":"api/utils/#promptolution.utils","title":"<code>promptolution.utils</code>","text":"<p>Module for utility functions and classes.</p>"},{"location":"api/utils/#promptolution.utils.callbacks","title":"<code>callbacks</code>","text":"<p>Callback classes for logging, saving, and tracking optimization progress.</p>"},{"location":"api/utils/#promptolution.utils.callbacks.BaseCallback","title":"<code>BaseCallback</code>","text":"<p>               Bases: <code>ABC</code></p> <p>Base class for optimization callbacks.</p> <p>Callbacks can be used to monitor the optimization process, save checkpoints, log metrics, or implement early stopping criteria.</p> Source code in <code>promptolution/utils/callbacks.py</code> <pre><code>class BaseCallback(ABC):\n    \"\"\"Base class for optimization callbacks.\n\n    Callbacks can be used to monitor the optimization process, save checkpoints,\n    log metrics, or implement early stopping criteria.\n\n    \"\"\"\n\n    def __init__(self, **kwargs: Any) -&gt; None:\n        \"\"\"Initialize the callback with a configuration.\n\n        Args:\n            config: Configuration for the callback.\n            **kwargs: Additional keyword arguments.\n        \"\"\"\n        pass\n\n    def on_step_end(self, optimizer: \"BaseOptimizer\") -&gt; bool:\n        \"\"\"Called at the end of each optimization step.\n\n        Args:\n            optimizer: The optimizer object that called the callback.\n\n        Returns:\n            Bool: True if the optimization should continue, False if it should stop.\n        \"\"\"\n        return True\n\n    def on_epoch_end(self, optimizer: \"BaseOptimizer\") -&gt; bool:\n        \"\"\"Called at the end of each optimization epoch.\n\n        Args:\n            optimizer: The optimizer object that called the callback.\n\n        Returns:\n            Bool: True if the optimization should continue, False if it should stop.\n        \"\"\"\n        return True\n\n    def on_train_end(self, optimizer: \"BaseOptimizer\") -&gt; bool:\n        \"\"\"Called at the end of the entire optimization process.\n\n        Args:\n            optimizer: The optimizer object that called the callback.\n\n        Returns:\n            Bool: True if the optimization should continue, False if it should stop.\n        \"\"\"\n        return True\n</code></pre>"},{"location":"api/utils/#promptolution.utils.callbacks.BaseCallback.__init__","title":"<code>__init__(**kwargs)</code>","text":"<p>Initialize the callback with a configuration.</p> <p>Parameters:</p> Name Type Description Default <code>config</code> <p>Configuration for the callback.</p> required <code>**kwargs</code> <code>Any</code> <p>Additional keyword arguments.</p> <code>{}</code> Source code in <code>promptolution/utils/callbacks.py</code> <pre><code>def __init__(self, **kwargs: Any) -&gt; None:\n    \"\"\"Initialize the callback with a configuration.\n\n    Args:\n        config: Configuration for the callback.\n        **kwargs: Additional keyword arguments.\n    \"\"\"\n    pass\n</code></pre>"},{"location":"api/utils/#promptolution.utils.callbacks.BaseCallback.on_epoch_end","title":"<code>on_epoch_end(optimizer)</code>","text":"<p>Called at the end of each optimization epoch.</p> <p>Parameters:</p> Name Type Description Default <code>optimizer</code> <code>BaseOptimizer</code> <p>The optimizer object that called the callback.</p> required <p>Returns:</p> Name Type Description <code>Bool</code> <code>bool</code> <p>True if the optimization should continue, False if it should stop.</p> Source code in <code>promptolution/utils/callbacks.py</code> <pre><code>def on_epoch_end(self, optimizer: \"BaseOptimizer\") -&gt; bool:\n    \"\"\"Called at the end of each optimization epoch.\n\n    Args:\n        optimizer: The optimizer object that called the callback.\n\n    Returns:\n        Bool: True if the optimization should continue, False if it should stop.\n    \"\"\"\n    return True\n</code></pre>"},{"location":"api/utils/#promptolution.utils.callbacks.BaseCallback.on_step_end","title":"<code>on_step_end(optimizer)</code>","text":"<p>Called at the end of each optimization step.</p> <p>Parameters:</p> Name Type Description Default <code>optimizer</code> <code>BaseOptimizer</code> <p>The optimizer object that called the callback.</p> required <p>Returns:</p> Name Type Description <code>Bool</code> <code>bool</code> <p>True if the optimization should continue, False if it should stop.</p> Source code in <code>promptolution/utils/callbacks.py</code> <pre><code>def on_step_end(self, optimizer: \"BaseOptimizer\") -&gt; bool:\n    \"\"\"Called at the end of each optimization step.\n\n    Args:\n        optimizer: The optimizer object that called the callback.\n\n    Returns:\n        Bool: True if the optimization should continue, False if it should stop.\n    \"\"\"\n    return True\n</code></pre>"},{"location":"api/utils/#promptolution.utils.callbacks.BaseCallback.on_train_end","title":"<code>on_train_end(optimizer)</code>","text":"<p>Called at the end of the entire optimization process.</p> <p>Parameters:</p> Name Type Description Default <code>optimizer</code> <code>BaseOptimizer</code> <p>The optimizer object that called the callback.</p> required <p>Returns:</p> Name Type Description <code>Bool</code> <code>bool</code> <p>True if the optimization should continue, False if it should stop.</p> Source code in <code>promptolution/utils/callbacks.py</code> <pre><code>def on_train_end(self, optimizer: \"BaseOptimizer\") -&gt; bool:\n    \"\"\"Called at the end of the entire optimization process.\n\n    Args:\n        optimizer: The optimizer object that called the callback.\n\n    Returns:\n        Bool: True if the optimization should continue, False if it should stop.\n    \"\"\"\n    return True\n</code></pre>"},{"location":"api/utils/#promptolution.utils.callbacks.BestPromptCallback","title":"<code>BestPromptCallback</code>","text":"<p>               Bases: <code>BaseCallback</code></p> <p>Callback for tracking the best prompt during optimization.</p> <p>This callback keeps track of the prompt with the highest score.</p> <p>Attributes:</p> Name Type Description <code>best_prompt</code> <code>str</code> <p>The prompt with the highest score so far.</p> <code>best_score</code> <code>float</code> <p>The highest score achieved so far.</p> Source code in <code>promptolution/utils/callbacks.py</code> <pre><code>class BestPromptCallback(BaseCallback):\n    \"\"\"Callback for tracking the best prompt during optimization.\n\n    This callback keeps track of the prompt with the highest score.\n\n    Attributes:\n        best_prompt (str): The prompt with the highest score so far.\n        best_score (float): The highest score achieved so far.\n    \"\"\"\n\n    def __init__(self) -&gt; None:\n        \"\"\"Initialize the BestPromptCallback.\"\"\"\n        self.best_prompt = \"\"\n        self.best_score = -99999.0\n\n    def on_step_end(self, optimizer: \"BaseOptimizer\") -&gt; bool:\n        \"\"\"Update the best prompt and score if a new high score is achieved.\n\n        Args:\n        optimizer: The optimizer object that called the callback.\n        \"\"\"\n        if optimizer.scores[0] &gt; self.best_score:\n            self.best_score = optimizer.scores[0]\n            self.best_prompt = optimizer.prompts[0]\n\n        return True\n\n    def get_best_prompt(self) -&gt; Tuple[str, float]:\n        \"\"\"Get the best prompt and score achieved during optimization.\n\n        Returns:\n        Tuple[str, float]: The best prompt and score.\n        \"\"\"\n        return self.best_prompt, self.best_score\n</code></pre>"},{"location":"api/utils/#promptolution.utils.callbacks.BestPromptCallback.__init__","title":"<code>__init__()</code>","text":"<p>Initialize the BestPromptCallback.</p> Source code in <code>promptolution/utils/callbacks.py</code> <pre><code>def __init__(self) -&gt; None:\n    \"\"\"Initialize the BestPromptCallback.\"\"\"\n    self.best_prompt = \"\"\n    self.best_score = -99999.0\n</code></pre>"},{"location":"api/utils/#promptolution.utils.callbacks.BestPromptCallback.get_best_prompt","title":"<code>get_best_prompt()</code>","text":"<p>Get the best prompt and score achieved during optimization.</p> <p>Returns: Tuple[str, float]: The best prompt and score.</p> Source code in <code>promptolution/utils/callbacks.py</code> <pre><code>def get_best_prompt(self) -&gt; Tuple[str, float]:\n    \"\"\"Get the best prompt and score achieved during optimization.\n\n    Returns:\n    Tuple[str, float]: The best prompt and score.\n    \"\"\"\n    return self.best_prompt, self.best_score\n</code></pre>"},{"location":"api/utils/#promptolution.utils.callbacks.BestPromptCallback.on_step_end","title":"<code>on_step_end(optimizer)</code>","text":"<p>Update the best prompt and score if a new high score is achieved.</p> <p>Args: optimizer: The optimizer object that called the callback.</p> Source code in <code>promptolution/utils/callbacks.py</code> <pre><code>def on_step_end(self, optimizer: \"BaseOptimizer\") -&gt; bool:\n    \"\"\"Update the best prompt and score if a new high score is achieved.\n\n    Args:\n    optimizer: The optimizer object that called the callback.\n    \"\"\"\n    if optimizer.scores[0] &gt; self.best_score:\n        self.best_score = optimizer.scores[0]\n        self.best_prompt = optimizer.prompts[0]\n\n    return True\n</code></pre>"},{"location":"api/utils/#promptolution.utils.callbacks.FileOutputCallback","title":"<code>FileOutputCallback</code>","text":"<p>               Bases: <code>BaseCallback</code></p> <p>Callback for saving optimization progress to a specified file type.</p> <p>This callback saves information about each step to a file.</p> <p>Attributes:</p> Name Type Description <code>dir</code> <code>str</code> <p>Directory the file is saved to.</p> <code>step</code> <code>int</code> <p>The current step number.</p> <code>file_type</code> <code>str</code> <p>The type of file to save the output to.</p> Source code in <code>promptolution/utils/callbacks.py</code> <pre><code>class FileOutputCallback(BaseCallback):\n    \"\"\"Callback for saving optimization progress to a specified file type.\n\n    This callback saves information about each step to a file.\n\n    Attributes:\n        dir (str): Directory the file is saved to.\n        step (int): The current step number.\n        file_type (str): The type of file to save the output to.\n    \"\"\"\n\n    def __init__(self, dir: str, file_type: Literal[\"parquet\", \"csv\"] = \"parquet\") -&gt; None:\n        \"\"\"Initialize the FileOutputCallback.\n\n        Args:\n        dir (str): Directory the CSV file is saved to.\n        file_type (str): The type of file to save the output to.\n        \"\"\"\n        if not os.path.exists(dir):\n            os.makedirs(dir)\n\n        self.file_type = file_type\n\n        if file_type == \"parquet\":\n            self.path = os.path.join(dir, \"step_results.parquet\")\n        elif file_type == \"csv\":\n            self.path = os.path.join(dir, \"step_results.csv\")\n        else:\n            raise ValueError(f\"File type {file_type} not supported.\")\n\n        self.step = 0\n\n    def on_step_end(self, optimizer: \"BaseOptimizer\") -&gt; bool:\n        \"\"\"Save prompts and scores to csv.\n\n        Args:\n        optimizer: The optimizer object that called the callback\n        \"\"\"\n        self.step += 1\n        df = pd.DataFrame(\n            {\n                \"step\": [self.step] * len(optimizer.prompts),\n                \"input_tokens\": [optimizer.predictor.llm.input_token_count] * len(optimizer.prompts),\n                \"output_tokens\": [optimizer.predictor.llm.output_token_count] * len(optimizer.prompts),\n                \"time\": [datetime.now().timestamp()] * len(optimizer.prompts),\n                \"score\": optimizer.scores,\n                \"prompt\": optimizer.prompts,\n            }\n        )\n\n        if self.file_type == \"parquet\":\n            if self.step == 1:\n                df.to_parquet(self.path, index=False)\n            else:\n                df.to_parquet(self.path, mode=\"a\", index=False)\n        elif self.file_type == \"csv\":\n            if self.step == 1:\n                df.to_csv(self.path, index=False)\n            else:\n                df.to_csv(self.path, mode=\"a\", header=False, index=False)\n\n        return True\n</code></pre>"},{"location":"api/utils/#promptolution.utils.callbacks.FileOutputCallback.__init__","title":"<code>__init__(dir, file_type='parquet')</code>","text":"<p>Initialize the FileOutputCallback.</p> <p>Args: dir (str): Directory the CSV file is saved to. file_type (str): The type of file to save the output to.</p> Source code in <code>promptolution/utils/callbacks.py</code> <pre><code>def __init__(self, dir: str, file_type: Literal[\"parquet\", \"csv\"] = \"parquet\") -&gt; None:\n    \"\"\"Initialize the FileOutputCallback.\n\n    Args:\n    dir (str): Directory the CSV file is saved to.\n    file_type (str): The type of file to save the output to.\n    \"\"\"\n    if not os.path.exists(dir):\n        os.makedirs(dir)\n\n    self.file_type = file_type\n\n    if file_type == \"parquet\":\n        self.path = os.path.join(dir, \"step_results.parquet\")\n    elif file_type == \"csv\":\n        self.path = os.path.join(dir, \"step_results.csv\")\n    else:\n        raise ValueError(f\"File type {file_type} not supported.\")\n\n    self.step = 0\n</code></pre>"},{"location":"api/utils/#promptolution.utils.callbacks.FileOutputCallback.on_step_end","title":"<code>on_step_end(optimizer)</code>","text":"<p>Save prompts and scores to csv.</p> <p>Args: optimizer: The optimizer object that called the callback</p> Source code in <code>promptolution/utils/callbacks.py</code> <pre><code>def on_step_end(self, optimizer: \"BaseOptimizer\") -&gt; bool:\n    \"\"\"Save prompts and scores to csv.\n\n    Args:\n    optimizer: The optimizer object that called the callback\n    \"\"\"\n    self.step += 1\n    df = pd.DataFrame(\n        {\n            \"step\": [self.step] * len(optimizer.prompts),\n            \"input_tokens\": [optimizer.predictor.llm.input_token_count] * len(optimizer.prompts),\n            \"output_tokens\": [optimizer.predictor.llm.output_token_count] * len(optimizer.prompts),\n            \"time\": [datetime.now().timestamp()] * len(optimizer.prompts),\n            \"score\": optimizer.scores,\n            \"prompt\": optimizer.prompts,\n        }\n    )\n\n    if self.file_type == \"parquet\":\n        if self.step == 1:\n            df.to_parquet(self.path, index=False)\n        else:\n            df.to_parquet(self.path, mode=\"a\", index=False)\n    elif self.file_type == \"csv\":\n        if self.step == 1:\n            df.to_csv(self.path, index=False)\n        else:\n            df.to_csv(self.path, mode=\"a\", header=False, index=False)\n\n    return True\n</code></pre>"},{"location":"api/utils/#promptolution.utils.callbacks.LoggerCallback","title":"<code>LoggerCallback</code>","text":"<p>               Bases: <code>BaseCallback</code></p> <p>Callback for logging optimization progress.</p> <p>This callback logs information about each step, epoch, and the end of training.</p> <p>Attributes:</p> Name Type Description <code>logger</code> <p>The logger object to use for logging.</p> <code>step</code> <code>int</code> <p>The current step number.</p> Source code in <code>promptolution/utils/callbacks.py</code> <pre><code>class LoggerCallback(BaseCallback):\n    \"\"\"Callback for logging optimization progress.\n\n    This callback logs information about each step, epoch, and the end of training.\n\n    Attributes:\n        logger: The logger object to use for logging.\n        step (int): The current step number.\n    \"\"\"\n\n    def __init__(self, logger: \"Logger\") -&gt; None:\n        \"\"\"Initialize the LoggerCallback.\"\"\"\n        self.logger = logger\n        self.step = 0\n\n    def on_step_end(self, optimizer: \"BaseOptimizer\") -&gt; bool:\n        \"\"\"Log information about the current step.\"\"\"\n        self.step += 1\n        time = datetime.now().strftime(\"%d-%m-%y %H:%M:%S:%f\")\n        self.logger.critical(f\"{time} - \u2728 Step {self.step} ended \u2728\")\n        for i, (prompt, score) in enumerate(zip(optimizer.prompts, optimizer.scores)):\n            self.logger.critical(f\"\ud83d\udcdd Prompt {i}: Score: {score}\")\n            self.logger.critical(f\"\ud83d\udcac {prompt}\")\n\n        return True\n\n    def on_train_end(self, optimizer: \"BaseOptimizer\", logs: Optional[Any] = None) -&gt; bool:\n        \"\"\"Log information at the end of training.\n\n        Args:\n        optimizer: The optimizer object that called the callback.\n        logs: Additional information to log.\n        \"\"\"\n        time = datetime.now().strftime(\"%d-%m-%y %H:%M:%S:%f\")\n        if logs is None:\n            self.logger.critical(f\"{time} - \ud83c\udfc1 Training ended\")\n        else:\n            self.logger.critical(f\"{time} - \ud83c\udfc1 Training ended - {logs}\")\n\n        return True\n</code></pre>"},{"location":"api/utils/#promptolution.utils.callbacks.LoggerCallback.__init__","title":"<code>__init__(logger)</code>","text":"<p>Initialize the LoggerCallback.</p> Source code in <code>promptolution/utils/callbacks.py</code> <pre><code>def __init__(self, logger: \"Logger\") -&gt; None:\n    \"\"\"Initialize the LoggerCallback.\"\"\"\n    self.logger = logger\n    self.step = 0\n</code></pre>"},{"location":"api/utils/#promptolution.utils.callbacks.LoggerCallback.on_step_end","title":"<code>on_step_end(optimizer)</code>","text":"<p>Log information about the current step.</p> Source code in <code>promptolution/utils/callbacks.py</code> <pre><code>def on_step_end(self, optimizer: \"BaseOptimizer\") -&gt; bool:\n    \"\"\"Log information about the current step.\"\"\"\n    self.step += 1\n    time = datetime.now().strftime(\"%d-%m-%y %H:%M:%S:%f\")\n    self.logger.critical(f\"{time} - \u2728 Step {self.step} ended \u2728\")\n    for i, (prompt, score) in enumerate(zip(optimizer.prompts, optimizer.scores)):\n        self.logger.critical(f\"\ud83d\udcdd Prompt {i}: Score: {score}\")\n        self.logger.critical(f\"\ud83d\udcac {prompt}\")\n\n    return True\n</code></pre>"},{"location":"api/utils/#promptolution.utils.callbacks.LoggerCallback.on_train_end","title":"<code>on_train_end(optimizer, logs=None)</code>","text":"<p>Log information at the end of training.</p> <p>Args: optimizer: The optimizer object that called the callback. logs: Additional information to log.</p> Source code in <code>promptolution/utils/callbacks.py</code> <pre><code>def on_train_end(self, optimizer: \"BaseOptimizer\", logs: Optional[Any] = None) -&gt; bool:\n    \"\"\"Log information at the end of training.\n\n    Args:\n    optimizer: The optimizer object that called the callback.\n    logs: Additional information to log.\n    \"\"\"\n    time = datetime.now().strftime(\"%d-%m-%y %H:%M:%S:%f\")\n    if logs is None:\n        self.logger.critical(f\"{time} - \ud83c\udfc1 Training ended\")\n    else:\n        self.logger.critical(f\"{time} - \ud83c\udfc1 Training ended - {logs}\")\n\n    return True\n</code></pre>"},{"location":"api/utils/#promptolution.utils.callbacks.ProgressBarCallback","title":"<code>ProgressBarCallback</code>","text":"<p>               Bases: <code>BaseCallback</code></p> <p>Callback for displaying a progress bar during optimization.</p> <p>This callback uses tqdm to display a progress bar that updates at each step.</p> <p>Attributes:</p> Name Type Description <code>pbar</code> <code>tqdm</code> <p>The tqdm progress bar object.</p> Source code in <code>promptolution/utils/callbacks.py</code> <pre><code>class ProgressBarCallback(BaseCallback):\n    \"\"\"Callback for displaying a progress bar during optimization.\n\n    This callback uses tqdm to display a progress bar that updates at each step.\n\n    Attributes:\n        pbar (tqdm): The tqdm progress bar object.\n    \"\"\"\n\n    def __init__(self, total_steps: int) -&gt; None:\n        \"\"\"Initialize the ProgressBarCallback.\n\n        Args:\n        total_steps (int): The total number of steps in the optimization process.\n        \"\"\"\n        self.pbar = tqdm(total=total_steps)\n\n    def on_step_end(self, optimizer: \"BaseOptimizer\") -&gt; bool:\n        \"\"\"Update the progress bar at the end of each step.\n\n        Args:\n        optimizer: The optimizer object that called the callback.\n        \"\"\"\n        self.pbar.update(1)\n\n        return True\n\n    def on_train_end(self, optimizer: \"BaseOptimizer\") -&gt; bool:\n        \"\"\"Close the progress bar at the end of training.\n\n        Args:\n        optimizer: The optimizer object that called the callback.\n        \"\"\"\n        self.pbar.close()\n\n        return True\n</code></pre>"},{"location":"api/utils/#promptolution.utils.callbacks.ProgressBarCallback.__init__","title":"<code>__init__(total_steps)</code>","text":"<p>Initialize the ProgressBarCallback.</p> <p>Args: total_steps (int): The total number of steps in the optimization process.</p> Source code in <code>promptolution/utils/callbacks.py</code> <pre><code>def __init__(self, total_steps: int) -&gt; None:\n    \"\"\"Initialize the ProgressBarCallback.\n\n    Args:\n    total_steps (int): The total number of steps in the optimization process.\n    \"\"\"\n    self.pbar = tqdm(total=total_steps)\n</code></pre>"},{"location":"api/utils/#promptolution.utils.callbacks.ProgressBarCallback.on_step_end","title":"<code>on_step_end(optimizer)</code>","text":"<p>Update the progress bar at the end of each step.</p> <p>Args: optimizer: The optimizer object that called the callback.</p> Source code in <code>promptolution/utils/callbacks.py</code> <pre><code>def on_step_end(self, optimizer: \"BaseOptimizer\") -&gt; bool:\n    \"\"\"Update the progress bar at the end of each step.\n\n    Args:\n    optimizer: The optimizer object that called the callback.\n    \"\"\"\n    self.pbar.update(1)\n\n    return True\n</code></pre>"},{"location":"api/utils/#promptolution.utils.callbacks.ProgressBarCallback.on_train_end","title":"<code>on_train_end(optimizer)</code>","text":"<p>Close the progress bar at the end of training.</p> <p>Args: optimizer: The optimizer object that called the callback.</p> Source code in <code>promptolution/utils/callbacks.py</code> <pre><code>def on_train_end(self, optimizer: \"BaseOptimizer\") -&gt; bool:\n    \"\"\"Close the progress bar at the end of training.\n\n    Args:\n    optimizer: The optimizer object that called the callback.\n    \"\"\"\n    self.pbar.close()\n\n    return True\n</code></pre>"},{"location":"api/utils/#promptolution.utils.callbacks.TokenCountCallback","title":"<code>TokenCountCallback</code>","text":"<p>               Bases: <code>BaseCallback</code></p> <p>Callback for stopping optimization based on the total token count.</p> Source code in <code>promptolution/utils/callbacks.py</code> <pre><code>class TokenCountCallback(BaseCallback):\n    \"\"\"Callback for stopping optimization based on the total token count.\"\"\"\n\n    def __init__(\n        self,\n        max_tokens_for_termination: int,\n        token_type_for_termination: Literal[\"input_tokens\", \"output_tokens\", \"total_tokens\"],\n    ) -&gt; None:\n        \"\"\"Initialize the TokenCountCallback.\n\n        Args:\n        max_tokens_for_termination (int): Maximum number of tokens which is allowed befor the algorithm is stopped.\n        token_type_for_termination (str): Can be one of either \"input_tokens\", \"output_tokens\" or \"total_tokens\".\n        \"\"\"\n        self.max_tokens_for_termination = max_tokens_for_termination\n        self.token_type_for_termination = token_type_for_termination\n\n    def on_step_end(self, optimizer: \"BaseOptimizer\") -&gt; bool:\n        \"\"\"Check if the total token count exceeds the maximum allowed. If so, stop the optimization.\"\"\"\n        token_counts = optimizer.predictor.llm.get_token_count()\n\n        if token_counts[self.token_type_for_termination] &gt; self.max_tokens_for_termination:\n            return False\n\n        return True\n</code></pre>"},{"location":"api/utils/#promptolution.utils.callbacks.TokenCountCallback.__init__","title":"<code>__init__(max_tokens_for_termination, token_type_for_termination)</code>","text":"<p>Initialize the TokenCountCallback.</p> <p>Args: max_tokens_for_termination (int): Maximum number of tokens which is allowed befor the algorithm is stopped. token_type_for_termination (str): Can be one of either \"input_tokens\", \"output_tokens\" or \"total_tokens\".</p> Source code in <code>promptolution/utils/callbacks.py</code> <pre><code>def __init__(\n    self,\n    max_tokens_for_termination: int,\n    token_type_for_termination: Literal[\"input_tokens\", \"output_tokens\", \"total_tokens\"],\n) -&gt; None:\n    \"\"\"Initialize the TokenCountCallback.\n\n    Args:\n    max_tokens_for_termination (int): Maximum number of tokens which is allowed befor the algorithm is stopped.\n    token_type_for_termination (str): Can be one of either \"input_tokens\", \"output_tokens\" or \"total_tokens\".\n    \"\"\"\n    self.max_tokens_for_termination = max_tokens_for_termination\n    self.token_type_for_termination = token_type_for_termination\n</code></pre>"},{"location":"api/utils/#promptolution.utils.callbacks.TokenCountCallback.on_step_end","title":"<code>on_step_end(optimizer)</code>","text":"<p>Check if the total token count exceeds the maximum allowed. If so, stop the optimization.</p> Source code in <code>promptolution/utils/callbacks.py</code> <pre><code>def on_step_end(self, optimizer: \"BaseOptimizer\") -&gt; bool:\n    \"\"\"Check if the total token count exceeds the maximum allowed. If so, stop the optimization.\"\"\"\n    token_counts = optimizer.predictor.llm.get_token_count()\n\n    if token_counts[self.token_type_for_termination] &gt; self.max_tokens_for_termination:\n        return False\n\n    return True\n</code></pre>"},{"location":"api/utils/#promptolution.utils.config","title":"<code>config</code>","text":"<p>Configuration class for the promptolution library.</p>"},{"location":"api/utils/#promptolution.utils.config.ExperimentConfig","title":"<code>ExperimentConfig</code>","text":"<p>Configuration class for the promptolution library.</p> <p>This is a unified configuration class that handles all experiment settings. It provides validation and tracking of used fields.</p> Source code in <code>promptolution/utils/config.py</code> <pre><code>class ExperimentConfig:\n    \"\"\"Configuration class for the promptolution library.\n\n    This is a unified configuration class that handles all experiment settings.\n    It provides validation and tracking of used fields.\n    \"\"\"\n\n    def __init__(self, **kwargs: Any) -&gt; None:\n        \"\"\"Initialize the configuration with the provided keyword arguments.\"\"\"\n        self._used_attributes: Set[str] = set()\n        for key, value in kwargs.items():\n            setattr(self, key, value)\n\n    def __setattr__(self, name: str, value: Any) -&gt; None:\n        \"\"\"Override attribute setting to track used attributes.\"\"\"\n        # Set the attribute using the standard mechanism\n        object.__setattr__(self, name, value)\n        if not name.startswith(\"_\") and not callable(value):\n            self._used_attributes.add(name)\n\n    def __getattribute__(self, name: str) -&gt; Any:\n        \"\"\"Override attribute access to track used attributes.\"\"\"\n        # Get the attribute using the standard mechanism\n        try:\n            value = object.__getattribute__(self, name)\n        except AttributeError:\n            return None\n        if not name.startswith(\"_\") and not callable(value):\n            self._used_attributes.add(name)\n\n        return value\n\n    def apply_to(self, obj: Any) -&gt; Any:\n        \"\"\"Apply matching attributes from this config to an existing object.\n\n        Examines each attribute of the target object and updates it if a matching\n        attribute exists in the config.\n\n        Args:\n            obj: The object to update with config values\n\n        Returns:\n            The updated object\n        \"\"\"\n        for attr_name in dir(obj):\n            if attr_name.startswith(\"_\") or not isinstance(\n                getattr(obj, attr_name), (str, int, float, list, type(None))\n            ):\n                continue\n\n            if hasattr(self, attr_name) and getattr(self, attr_name) is not None:\n                setattr(obj, attr_name, getattr(self, attr_name))\n\n        return obj\n\n    def validate(self) -&gt; None:\n        \"\"\"Check if any attributes were not used and run validation.\n\n        Does not raise an error, but logs a warning if any attributes are unused or validation fails.\n        \"\"\"\n        all_attributes = {k for k in self.__dict__ if not k.startswith(\"_\")}\n        unused_attributes = all_attributes - self._used_attributes\n        if unused_attributes:\n            logger.warning(f\"\u26a0\ufe0f Unused configuration attributes: {unused_attributes}\")\n</code></pre>"},{"location":"api/utils/#promptolution.utils.config.ExperimentConfig.__getattribute__","title":"<code>__getattribute__(name)</code>","text":"<p>Override attribute access to track used attributes.</p> Source code in <code>promptolution/utils/config.py</code> <pre><code>def __getattribute__(self, name: str) -&gt; Any:\n    \"\"\"Override attribute access to track used attributes.\"\"\"\n    # Get the attribute using the standard mechanism\n    try:\n        value = object.__getattribute__(self, name)\n    except AttributeError:\n        return None\n    if not name.startswith(\"_\") and not callable(value):\n        self._used_attributes.add(name)\n\n    return value\n</code></pre>"},{"location":"api/utils/#promptolution.utils.config.ExperimentConfig.__init__","title":"<code>__init__(**kwargs)</code>","text":"<p>Initialize the configuration with the provided keyword arguments.</p> Source code in <code>promptolution/utils/config.py</code> <pre><code>def __init__(self, **kwargs: Any) -&gt; None:\n    \"\"\"Initialize the configuration with the provided keyword arguments.\"\"\"\n    self._used_attributes: Set[str] = set()\n    for key, value in kwargs.items():\n        setattr(self, key, value)\n</code></pre>"},{"location":"api/utils/#promptolution.utils.config.ExperimentConfig.__setattr__","title":"<code>__setattr__(name, value)</code>","text":"<p>Override attribute setting to track used attributes.</p> Source code in <code>promptolution/utils/config.py</code> <pre><code>def __setattr__(self, name: str, value: Any) -&gt; None:\n    \"\"\"Override attribute setting to track used attributes.\"\"\"\n    # Set the attribute using the standard mechanism\n    object.__setattr__(self, name, value)\n    if not name.startswith(\"_\") and not callable(value):\n        self._used_attributes.add(name)\n</code></pre>"},{"location":"api/utils/#promptolution.utils.config.ExperimentConfig.apply_to","title":"<code>apply_to(obj)</code>","text":"<p>Apply matching attributes from this config to an existing object.</p> <p>Examines each attribute of the target object and updates it if a matching attribute exists in the config.</p> <p>Parameters:</p> Name Type Description Default <code>obj</code> <code>Any</code> <p>The object to update with config values</p> required <p>Returns:</p> Type Description <code>Any</code> <p>The updated object</p> Source code in <code>promptolution/utils/config.py</code> <pre><code>def apply_to(self, obj: Any) -&gt; Any:\n    \"\"\"Apply matching attributes from this config to an existing object.\n\n    Examines each attribute of the target object and updates it if a matching\n    attribute exists in the config.\n\n    Args:\n        obj: The object to update with config values\n\n    Returns:\n        The updated object\n    \"\"\"\n    for attr_name in dir(obj):\n        if attr_name.startswith(\"_\") or not isinstance(\n            getattr(obj, attr_name), (str, int, float, list, type(None))\n        ):\n            continue\n\n        if hasattr(self, attr_name) and getattr(self, attr_name) is not None:\n            setattr(obj, attr_name, getattr(self, attr_name))\n\n    return obj\n</code></pre>"},{"location":"api/utils/#promptolution.utils.config.ExperimentConfig.validate","title":"<code>validate()</code>","text":"<p>Check if any attributes were not used and run validation.</p> <p>Does not raise an error, but logs a warning if any attributes are unused or validation fails.</p> Source code in <code>promptolution/utils/config.py</code> <pre><code>def validate(self) -&gt; None:\n    \"\"\"Check if any attributes were not used and run validation.\n\n    Does not raise an error, but logs a warning if any attributes are unused or validation fails.\n    \"\"\"\n    all_attributes = {k for k in self.__dict__ if not k.startswith(\"_\")}\n    unused_attributes = all_attributes - self._used_attributes\n    if unused_attributes:\n        logger.warning(f\"\u26a0\ufe0f Unused configuration attributes: {unused_attributes}\")\n</code></pre>"},{"location":"api/utils/#promptolution.utils.formatting","title":"<code>formatting</code>","text":"<p>Utils for formatting prompts and outputs.</p>"},{"location":"api/utils/#promptolution.utils.formatting.extract_from_tag","title":"<code>extract_from_tag(text, start_tag, end_tag)</code>","text":"<pre><code>extract_from_tag(text: str, start_tag: str, end_tag: str) -&gt; str\n</code></pre><pre><code>extract_from_tag(text: List[str], start_tag: str, end_tag: str) -&gt; List[str]\n</code></pre> <p>Extracts content from a string between specified start and end tags.</p> <p>Parameters:</p> Name Type Description Default <code>text</code> <code>str</code> <p>The input text to extract from.</p> required <code>start_tag</code> <code>str</code> <p>The start tag to look for.</p> required <code>end_tag</code> <code>str</code> <p>The end tag to look for.</p> required <p>Returns:</p> Type Description <code>Union[List[str], str]</code> <p>Union[List[str], str]: The extracted content, either as a list or a single string.</p> Source code in <code>promptolution/utils/formatting.py</code> <pre><code>def extract_from_tag(text: Union[str, List[str]], start_tag: str, end_tag: str) -&gt; Union[List[str], str]:\n    \"\"\"Extracts content from a string between specified start and end tags.\n\n    Args:\n        text (str): The input text to extract from.\n        start_tag (str): The start tag to look for.\n        end_tag (str): The end tag to look for.\n\n    Returns:\n        Union[List[str], str]: The extracted content, either as a list or a single string.\n    \"\"\"\n    was_list = True\n    if isinstance(text, str):\n        text = [text]\n        was_list = False\n\n    outs = []\n    for t in text:\n        out = t.split(start_tag)[-1].split(end_tag)[0].strip()\n        outs.append(out)\n    if was_list:\n        return outs\n    return outs[0]\n</code></pre>"},{"location":"api/utils/#promptolution.utils.logging","title":"<code>logging</code>","text":"<p>Logging configuration for the promptolution library.</p>"},{"location":"api/utils/#promptolution.utils.logging.get_logger","title":"<code>get_logger(name, level=None)</code>","text":"<p>Get a logger with the specified name and level.</p> <p>This function provides a standardized way to get loggers throughout the library, ensuring consistent formatting and behavior.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>Name of the logger, typically name of the module.</p> required <code>level</code> <code>int</code> <p>Logging level. Defaults to None, which uses the root logger's level.</p> <code>None</code> <p>Returns:</p> Type Description <code>Logger</code> <p>logging.Logger: Configured logger instance.</p> Source code in <code>promptolution/utils/logging.py</code> <pre><code>def get_logger(name: str, level: Optional[int] = None) -&gt; logging.Logger:\n    \"\"\"Get a logger with the specified name and level.\n\n    This function provides a standardized way to get loggers throughout the library,\n    ensuring consistent formatting and behavior.\n\n    Args:\n        name (str): Name of the logger, typically __name__ of the module.\n        level (int, optional): Logging level. Defaults to None, which uses the root logger's level.\n\n    Returns:\n        logging.Logger: Configured logger instance.\n    \"\"\"\n    logger = logging.getLogger(name)\n    if level is not None:\n        logger.setLevel(level)\n    return logger\n</code></pre>"},{"location":"api/utils/#promptolution.utils.logging.setup_logging","title":"<code>setup_logging(level=logging.INFO)</code>","text":"<p>Set up logging for the promptolution library.</p> <p>This function configures the root logger for the library with appropriate formatting and level.</p> <p>Parameters:</p> Name Type Description Default <code>level</code> <code>int</code> <p>Logging level. Defaults to logging.INFO.</p> <code>INFO</code> Source code in <code>promptolution/utils/logging.py</code> <pre><code>def setup_logging(level: int = logging.INFO) -&gt; None:\n    \"\"\"Set up logging for the promptolution library.\n\n    This function configures the root logger for the library with appropriate\n    formatting and level.\n\n    Args:\n        level (int, optional): Logging level. Defaults to logging.INFO.\n    \"\"\"\n    # Configure the root logger\n    logging.basicConfig(\n        level=level,\n        format=\"%(asctime)s - %(name)s - %(levelname)s - %(message)s\",\n        datefmt=\"%Y-%m-%d %H:%M:%S\",\n    )\n</code></pre>"},{"location":"api/utils/#promptolution.utils.prompt_creation","title":"<code>prompt_creation</code>","text":"<p>Utility functions for prompt creation.</p>"},{"location":"api/utils/#promptolution.utils.prompt_creation.create_prompt_variation","title":"<code>create_prompt_variation(prompt, llm, meta_prompt=None)</code>","text":"<p>Generate a variation of the given prompt(s) while keeping the semantic meaning.</p> <p>Idea taken from the paper Zhou et al. (2021) https://arxiv.org/pdf/2211.01910</p> <p>Parameters:</p> Name Type Description Default <code>prompt</code> <code>Union[List[str], str]</code> <p>The prompt(s) to generate variations of.</p> required <code>llm</code> <code>BaseLLM</code> <p>The language model to use for generating the variations.</p> required <code>meta_prompt</code> <code>str</code> <p>The meta prompt to use for generating the variations.</p> <code>None</code> <p>Returns:</p> Type Description <code>List[str]</code> <p>List[str]: A list of generated variations of the input prompt(s).</p> Source code in <code>promptolution/utils/prompt_creation.py</code> <pre><code>def create_prompt_variation(\n    prompt: Union[List[str], str], llm: \"BaseLLM\", meta_prompt: Optional[str] = None\n) -&gt; List[str]:\n    \"\"\"Generate a variation of the given prompt(s) while keeping the semantic meaning.\n\n    Idea taken from the paper Zhou et al. (2021) https://arxiv.org/pdf/2211.01910\n\n    Args:\n        prompt (Union[List[str], str]): The prompt(s) to generate variations of.\n        llm (BaseLLM): The language model to use for generating the variations.\n        meta_prompt (str): The meta prompt to use for generating the variations.\n        If None, a default meta prompt is used. Should contain &lt;prev_prompt&gt; tag.\n\n    Returns:\n        List[str]: A list of generated variations of the input prompt(s).\n    \"\"\"\n    meta_prompt = PROMPT_VARIATION_TEMPLATE if meta_prompt is None else meta_prompt\n\n    if isinstance(prompt, str):\n        prompt = [prompt]\n    varied_prompts = llm.get_response([meta_prompt.replace(\"&lt;prev_prompt&gt;\", p) for p in prompt])\n    varied_prompts = extract_from_tag(varied_prompts, \"&lt;prompt&gt;\", \"&lt;/prompt&gt;\")\n\n    return varied_prompts\n</code></pre>"},{"location":"api/utils/#promptolution.utils.prompt_creation.create_prompts_from_samples","title":"<code>create_prompts_from_samples(task, llm, meta_prompt=None, n_samples=3, task_description=None, n_prompts=1, get_uniform_labels=False)</code>","text":"<p>Generate a set of prompts from dataset examples sampled from a given task.</p> <p>Idea taken from the paper Zhou et al. (2021) https://arxiv.org/pdf/2211.01910 Samples are selected, such that (1) all possible classes are represented (2) the samples are as representative as possible</p> <p>Parameters:</p> Name Type Description Default <code>task</code> <code>BaseTask</code> <p>The task to generate prompts for.</p> required <code>llm</code> <code>BaseLLM</code> <p>The language model to use for generating the prompts.</p> required <code>meta_prompt</code> <code>str</code> <p>The meta prompt to use for generating the prompts.</p> <code>None</code> <code>n_samples</code> <code>int</code> <p>The number of samples to use for generating prompts.</p> <code>3</code> <code>task_description</code> <code>str</code> <p>The description of the task to include in the prompt.</p> <code>None</code> <code>n_prompts</code> <code>int</code> <p>The number of prompts to generate.</p> <code>1</code> <code>get_uniform_labels</code> <code>bool</code> <p>If True, samples are selected such that all classes are represented.</p> <code>False</code> <p>Returns:</p> Type Description <code>List[str]</code> <p>List[str]: A list of generated prompts.</p> Source code in <code>promptolution/utils/prompt_creation.py</code> <pre><code>def create_prompts_from_samples(\n    task: \"BaseTask\",\n    llm: \"BaseLLM\",\n    meta_prompt: Optional[str] = None,\n    n_samples: int = 3,\n    task_description: Optional[str] = None,\n    n_prompts: int = 1,\n    get_uniform_labels: bool = False,\n) -&gt; List[str]:\n    \"\"\"Generate a set of prompts from dataset examples sampled from a given task.\n\n    Idea taken from the paper Zhou et al. (2021) https://arxiv.org/pdf/2211.01910\n    Samples are selected, such that\n    (1) all possible classes are represented\n    (2) the samples are as representative as possible\n\n    Args:\n        task (BaseTask): The task to generate prompts for.\n        Xs and Ys from this object are used to generate the prompts.\n        llm (BaseLLM): The language model to use for generating the prompts.\n        meta_prompt (str): The meta prompt to use for generating the prompts.\n        If None, a default meta prompt is used.\n        n_samples (int): The number of samples to use for generating prompts.\n        task_description (str): The description of the task to include in the prompt.\n        n_prompts (int): The number of prompts to generate.\n        get_uniform_labels (bool): If True, samples are selected such that all classes are represented.\n\n    Returns:\n        List[str]: A list of generated prompts.\n    \"\"\"\n    meta_prompt_template: str\n    if meta_prompt is None:\n        if task_description is None:\n            meta_prompt_template = PROMPT_CREATION_TEMPLATE\n        else:\n            meta_prompt_template = PROMPT_CREATION_TEMPLATE_TD.replace(\"&lt;task_desc&gt;\", task_description)\n    else:\n        if task_description is None:\n            meta_prompt_template = meta_prompt\n        else:\n            meta_prompt_template = meta_prompt.replace(\"&lt;task_desc&gt;\", task_description)\n\n    meta_prompts = []\n    for _ in range(n_prompts):\n        if isinstance(task, ClassificationTask) and get_uniform_labels:\n            # if classification task sample such that all classes are represented\n            unique_labels, counts = np.unique(task.ys, return_counts=True)\n            proportions = counts / len(task.ys)\n            samples_per_class = np.round(proportions * n_samples).astype(int)\n            samples_per_class = np.maximum(samples_per_class, 1)\n\n            # sample\n            xs: List[str] = []\n            ys: List[str] = []\n            for label, num_samples in zip(unique_labels, samples_per_class):\n                indices = np.where(task.ys == label)[0]\n                indices = np.random.choice(indices, n_samples, replace=False)\n                xs.extend(task.xs[indices])\n                ys.extend(task.ys[indices])\n\n        else:\n            # if not classification task, sample randomly\n            indices = np.random.choice(len(task.xs), n_samples, replace=False)\n            xs = [task.xs[i] for i in indices]\n            ys = [task.ys[i] for i in indices]\n\n        examples = \"\\n\\n\".join([f\"Input: {x}\\nOutput: {y}\" for x, y in zip(xs, ys)])\n        meta_prompt = meta_prompt_template.replace(\"&lt;input_output_pairs&gt;\", examples)\n        meta_prompts.append(meta_prompt)\n\n    prompts = llm.get_response(meta_prompts)\n    prompts = extract_from_tag(prompts, \"&lt;prompt&gt;\", \"&lt;/prompt&gt;\")\n\n    return prompts\n</code></pre>"},{"location":"api/utils/#promptolution.utils.test_statistics","title":"<code>test_statistics</code>","text":"<p>Implementation of statistical significance tests used in the racing algorithm. Contains paired t-test functionality to compare prompt performance and determine statistical significance between candidates.</p>"},{"location":"api/utils/#promptolution.utils.test_statistics.get_test_statistic_func","title":"<code>get_test_statistic_func(name)</code>","text":"<p>Get the test statistic function based on the name provided.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>Name of the test statistic function.</p> required <p>Returns:</p> Name Type Description <code>callable</code> <code>Callable[..., bool]</code> <p>The corresponding test statistic function.</p> Source code in <code>promptolution/utils/test_statistics.py</code> <pre><code>def get_test_statistic_func(name: TestStatistics) -&gt; Callable[..., bool]:\n    \"\"\"\n    Get the test statistic function based on the name provided.\n\n    Args:\n        name (str): Name of the test statistic function.\n\n    Returns:\n        callable: The corresponding test statistic function.\n    \"\"\"\n    if name == \"paired_t_test\":\n        return paired_t_test\n    else:\n        raise ValueError(f\"Unknown test statistic function: {name}. Should be one of {TestStatistics.__args__}.\")\n</code></pre>"},{"location":"api/utils/#promptolution.utils.test_statistics.paired_t_test","title":"<code>paired_t_test(scores_a, scores_b, alpha=0.05)</code>","text":"<p>Uses a paired t-test to test if candidate A's accuracy is significantly higher than candidate B's accuracy within a confidence interval of 1-\u0007lpha. Assumptions: - The samples are paired. - The differences between the pairs are normally distributed (-&gt; n &gt; 30).</p> <p>Parameters:</p> Name Type Description Default <code>scores_a</code> <code>List[float]</code> <p>Array of accuracy scores for candidate A.</p> required <code>scores_b</code> <code>List[float]</code> <p>Array of accuracy scores for candidate B.</p> required <code>alpha</code> <code>float</code> <p>Significance level (default 0.05 for 95% confidence).</p> <code>0.05</code> <p>Returns:</p> Name Type Description <code>bool</code> <code>bool</code> <p>True if candidate A is significantly better than candidate B, False otherwise.</p> Source code in <code>promptolution/utils/test_statistics.py</code> <pre><code>def paired_t_test(\n    scores_a: List[float],\n    scores_b: List[float],\n    alpha: float = 0.05,\n) -&gt; bool:\n    \"\"\"\n    Uses a paired t-test to test if candidate A's accuracy is significantly\n    higher than candidate B's accuracy within a confidence interval of 1-\\alpha.\n    Assumptions:\n    - The samples are paired.\n    - The differences between the pairs are normally distributed (-&gt; n &gt; 30).\n\n    Parameters:\n        scores_a (List[float]): Array of accuracy scores for candidate A.\n        scores_b (List[float]): Array of accuracy scores for candidate B.\n        alpha (float): Significance level (default 0.05 for 95% confidence).\n\n    Returns:\n        bool: True if candidate A is significantly better than candidate B, False otherwise.\n    \"\"\"\n    scores_a = np.array(scores_a)\n    scores_b = np.array(scores_b)\n\n    _, p_value = ttest_rel(scores_a, scores_b, alternative=\"greater\")\n\n    result = p_value &lt; alpha\n\n    return bool(result)\n</code></pre>"},{"location":"api/utils/#promptolution.utils.token_counter","title":"<code>token_counter</code>","text":"<p>Token counter for LLMs.</p> <p>This module provides a function to count the number of tokens in a given text.</p>"},{"location":"api/utils/#promptolution.utils.token_counter.get_token_counter","title":"<code>get_token_counter(llm)</code>","text":"<p>Get a token counter function for the given LLM.</p> <p>This function returns a callable that counts tokens based on the LLM's tokenizer or a simple split method if no tokenizer is available.</p> <p>Parameters:</p> Name Type Description Default <code>llm</code> <code>BaseLLM</code> <p>The language model object that may have a tokenizer.</p> required <p>Returns:</p> Type Description <code>Callable[[str], int]</code> <p>A callable that takes a text input and returns the token count.</p> Source code in <code>promptolution/utils/token_counter.py</code> <pre><code>def get_token_counter(llm: \"BaseLLM\") -&gt; Callable[[str], int]:\n    \"\"\"Get a token counter function for the given LLM.\n\n    This function returns a callable that counts tokens based on the LLM's tokenizer\n    or a simple split method if no tokenizer is available.\n\n    Args:\n        llm: The language model object that may have a tokenizer.\n\n    Returns:\n        A callable that takes a text input and returns the token count.\n\n    \"\"\"\n    if llm.tokenizer is not None:\n        tokenizer: PreTrainedTokenizer = llm.tokenizer\n        return lambda x: len(tokenizer.encode(x))\n    else:\n        logger.warning(\"\u26a0\ufe0f The LLM does not have a tokenizer. Using simple token count.\")\n        return lambda x: len(x.split())\n</code></pre>"},{"location":"examples/getting_started/","title":"Getting Started with Promptolution","text":""},{"location":"examples/getting_started/#welcome-to-promptolution","title":"Welcome to Promptolution!","text":"<p>Discover a powerful tool for evolving and optimizing your LLM prompts. This notebook provides a friendly introduction to Promptolution's core functionality.</p> <p>We're excited to have you try Promptolution - let's get started!</p>"},{"location":"examples/getting_started/#installation","title":"Installation","text":"<p>Install Promptolution with a single command</p> <pre><code>! pip install promptolution[api]\n</code></pre>"},{"location":"examples/getting_started/#imports","title":"Imports","text":"<pre><code>import pandas as pd\nfrom promptolution.utils import ExperimentConfig\nfrom promptolution.helpers import run_experiment\nimport nest_asyncio\n\nnest_asyncio.apply()  # Required for notebook environments\n</code></pre>"},{"location":"examples/getting_started/#setting-up-your-experiment","title":"Setting Up Your Experiment","text":""},{"location":"examples/getting_started/#prepare-the-data","title":"Prepare the data","text":"<p>Below, we're using a subsample of the subjectivity dataset from Hugging Face as an example. When using your own dataset, simply ensure you name the input column \"x\" and the target column \"y\", and provide a brief description of your task, that will parsed to the meta-llm during optimization.</p> <pre><code>df = pd.read_csv(\"hf://datasets/tasksource/subjectivity/train.csv\").sample(500)\ndf = df.rename(columns={\"Sentence\": \"x\", \"Label\": \"y\"})\ndf = df.replace({\"OBJ\": \"objective\", \"SUBJ\": \"subjective\"})\n\ntask_description = (\n    \"The dataset contains sentences labeled as either subjective or objective. \"\n    \"The task is to classify each sentence as either subjective or objective. \"\n    \"The class mentioned in between the answer tags &lt;final_answer&gt;&lt;/final_answer&gt; will be used as the prediction.\"\n)\n</code></pre>"},{"location":"examples/getting_started/#creating-inital-prompts","title":"Creating Inital Prompts","text":"<p>We've defined some starter prompts below, but feel free to experiment! You might also want to explore create_prompts_from_samples to automatically generate initial prompts based on your data.</p> <pre><code>init_prompts = [\n    'Classify the given text as either an objective or subjective statement based on the tone and language used: e.g. the tone and language used should indicate whether the statement is a neutral, factual summary (objective) or an expression of opinion or emotional tone (subjective). Include the output classes \"objective\" or \"subjective\" in the prompt.',\n    \"What kind of statement is the following text: [Insert text here]? Is it &lt;objective_statement&gt; or &lt;subjective_statement&gt;?\",\n    'Identify whether a sentence is objective or subjective by analyzing the tone, language, and underlying perspective. Consider the emotion, opinion, and bias present in the sentence. Are the authors presenting objective facts or expressing a personal point of view? The output will be either \"objective\" (output class: objective) or \"subjective\" (output class: subjective).',\n    \"Classify the following sentences as either objective or subjective, indicating the name of the output classes: [input sentence]. Output classes: objective, subjective\",\n    '_query a text about legal or corporate-related issues, and predict whether the tone is objective or subjective, outputting the corresponding class \"objective\" for non-subjective language or \"subjective\" for subjective language_',\n    'Classify a statement as either \"subjective\" or \"objective\" based on whether it reflects a personal opinion or a verifiable fact. The output classes to include are \"objective\" and \"subjective\".',\n    \"Classify the text as objective or subjective based on its tone and language.\",\n    \"Classify the text as objective or subjective based on the presence of opinions or facts. Output classes: objective, subjective.\",\n    \"Classify the given text as objective or subjective based on its tone, focusing on its intention, purpose, and level of personal opinion or emotional appeal, with outputs including classes such as objective or subjective.\",\n    \"Categorize the text as either objective or subjective, considering whether it presents neutral information or expresses a personal opinion/bias.\\n\\nObjective: The text has a neutral tone and presents factual information about the actions of Democrats in Congress and the union's negotiations.\\n\\nSubjective: The text has a evaluative tone and expresses a positive/negative opinion/evaluation about the past performance of the country.\",\n    'Given a sentence, classify it as either \"objective\" or \"subjective\" based on its tone and language, considering the presence of third-person pronouns, neutral language, and opinions. Classify the output as \"objective\" if the tone is neutral and detached, focusing on facts and data, or as \"subjective\" if the tone is evaluative, emotive, or biased.',\n    'Identify whether the given sentence is subjective or objective, then correspondingly output \"objective\" or \"subjective\" in the form of \"&lt;output class&gt;, (e.g. \"objective\"), without quotes. Please note that the subjective orientation typically describes a sentence where the writer expresses their own opinion or attitude, whereas an objective sentence presents facts or information without personal involvement or bias. &lt;output classes: subjective, objective&gt;',\n]\n</code></pre>"},{"location":"examples/getting_started/#configure-your-llm","title":"Configure Your LLM","text":"<p>Promptolution offers three flexible ways to access language models:</p> <ol> <li>Local LLMs (using the Transformers library)</li> <li>vLLM backend (for efficient serving of large language models)</li> <li>API-based LLMs (compatible with any provider following the OpenAI standard)</li> </ol> <p>For this demonstration, we'll use the DeepInfra API, but you can easily switch to other providers like Anthropic or OpenAI by simply changing the base_url and llm string in the configuration.</p> <pre><code>api_key = \"YOUR_API_KEY\"  # Replace with your Promptolution API key\n</code></pre> <p>Here's an explanation of each configuration parameter in the ExperimentConfig: - <code>optimizer</code>: The algorithm used for prompt optimization. Currently we support \"capo\", \"evopromptga\", \"evopromptde\", and \"opro\". For this example, we use \"capo\" as it is capable of leveraging few-shot examples. - <code>task_description</code>: A string describing the task you're optimizing prompts for. This is used to provide the meta-llm with context about your task. - <code>prompts</code>: A list of initial prompt strings that will be used as the starting point for optimization. - <code>n_steps</code>: The number of optimization steps to run. Higher values allow more exploration and refinement but require more API calls and computational resources. - <code>api_url</code>: The API endpoint URL used to access the language model. This example uses DeepInfra's API which follows the OpenAI standard. - <code>llm</code>: The LLM to use for the experiment, as both downstream and meta LLM. - <code>token</code>: Your API authentication token required to access the language model service.</p> <pre><code>config = ExperimentConfig(\n    optimizer=\"capo\",\n    task_description=task_description,\n    prompts=init_prompts,\n    n_steps=10,\n    api_url=\"https://api.deepinfra.com/v1/openai\",\n    model_id=\"meta-llama/Meta-Llama-3-8B-Instruct\",\n    api_key=api_key,\n    n_subsamples=30,\n)\n</code></pre>"},{"location":"examples/getting_started/#run-your-experiment","title":"Run Your Experiment","text":"<p>With everything configured, you're ready to optimize your prompts! The <code>run_experiment</code> function will run the optimization and evaluate on a holdout set. You can expect this cell to take a few minutes to run.</p> <pre><code>prompts = run_experiment(df, config)\n</code></pre> <pre><code>\ud83d\udccc CAPO requires block evaluation strategy. Setting it to 'sequential_block'.\n\u26a0\ufe0f The LLM does not have a tokenizer. Using simple token count.\n\ud83d\udd25 Starting optimization...\n\ud83d\udcca Starting evaluation...\n</code></pre> <p>As you can see, most optimized prompts are semantically very similar, however they often differ heavily in performance. This is exactly what we observed in our experiments across various LLMs and datasets. Running prompt optimization is an easy way to gain significant performance improvements on your task for free!</p> <p>If you run into any issues while using Promptolution, please feel free to contact us. We're also happy to receive support through pull requests and other contributions to the project.</p> <p>Happy prompt optimizing! \ud83d\ude80\u2728 We can't wait to see what you build with Promptolution! \ud83e\udd16\ud83d\udca1</p> <pre><code>prompts\n</code></pre> prompt score 0 Classify the text as objective or subjective based on the presence of opinions or facts. Output classes: objective, subjective.\\n\\nInput:\\nThe proposed agreement includes the best wage increases for rail workers in over forty years.\\n\\nOutput:\\nobjective\\n\\nInput:\\nThe principal reason, from the point of view of government, is that a universal income tax would be a powerful restraint upon the expansion of government.\\n\\nOutput:\\n&lt;final_answer&gt;objective&lt;/final_answer&gt;\\n\\nInput: 0.76 1 Task: Linguistic Analysis for Sentence Classification\\n\\nClassify each sentence as either objective or subjective by applying linguistic insights to identify its tone, emotion, and degree of neutrality. Examine the sentences' language features, sentiment, and presence of verifiable facts or personal opinions. Determine whether each sentence presents impartial data or conveys the author's emotions, beliefs, or biases. Treat each sentence as a distinct entity, analyzing its contours, nuances, and purpose. Consider the distinction between factual reports like news articles and opinion-based writings like blog posts. Make a nuanced classification by scrutinizing the sentence's impact, intention, and emotional resonance.\\n\\nYour response should be comprised of two parts: the classification and the rationale. Enclose the first-mentioned class within the markers &lt;final_answer&gt; and &lt;/final_answer&gt;. For instance, if the classification is 'objective', the output should be &lt;final_answer&gt;objective&lt;/final_answer&gt;. Focus on the sentence's language, tone, and emotional appeal to make an informed decision about its categorization, prioritizing the sentence's intention and purpose.\\n\\nInput:\\nThe last may go very deep.\\n\\nOutput:\\n&lt;final_answer&gt;objective&lt;/final_answer&gt;\\n\\nInput:\\n\u201cThis latest rule will open our borders even more, and the Court seems to relish making arbitrary decisions without thinking about consequences.\\n\\nOutput:\\n&lt;final_answer&gt;objective&lt;/final_answer&gt;\\n\\nInput: 0.72 2 Classify each sentence as either objective or subjective by unpacking its linguistic nuances and emotional undertones. Analyze the sentence's language features, sentiment, and presence of verifiable facts or personal opinions to determine whether it presents impartial data or conveys the author's emotions, beliefs, or biases. Treat each sentence as a standalone entity, examining its contours, subtleties, and intended purpose. Consider the distinction between factual reporting, like news articles, and opinion-based writings, like blog posts. Make a refined classification by scrutinizing the sentence's impact, intention, and emotional resonance, prioritizing the sentence's intention and purpose. Your response should consist of two parts: the classification and the rationale. Enclose the primary classification within the markers &lt;final_answer&gt; and &lt;/final_answer&gt;. Focus on the sentence's language, tone, and emotional appeal to make an informed decision about its categorization. Classify each sentence as either objective or subjective by examining its linguistic tone, underlying intent, and purpose. Determine whether the text presents a neutral, factual account or expresses a personal opinion or emotional bias. Evaluate whether the text provides a neutral, factual report or reveals an evaluative tone, offering a positive or negative appraisal. Outputs will include classifications like objective or subjective, with the initial response serving as the prediction.\\n\\nInput:\\nOver several decades, Prime Central London \u2013 or PCL \u2013 had become a repository for cash from wealthy foreigners, whether they actually wanted to live there or not.\\n\\nOutput:\\n&lt;final_answer&gt;objective&lt;/final_answer&gt;\\n\\nInput: 0.71 3 &lt;prompt\u0433\u0430\u043b\u0442\u0435\u0440/&gt;\\n\\nClassify each sentence as either objective or subjective by examining its linguistic tone, underlying intent, and purpose. Consider whether the text presents a neutral, factual account or expresses a personal opinion or emotional bias. Evaluate whether the text is neutral and provides mere reportage, such as a factual report on congressional Democrats' actions and labor union negotiations, or if it reveals an evaluative tone, offering a positive or negative appraisal of a nation's past performance. Outputs will include classifications like objective or subjective. The class mentioned first in the response will serve as the prediction, with the class label extracted from the text between the markers &lt;final_answer&gt; and &lt;/final_answer&gt;.\\n\\nInput:\\nOver several decades, Prime Central London \u2013 or PCL \u2013 had become a repository for cash from wealthy foreigners, whether they actually wanted to live there or not.\\n\\nOutput:\\n&lt;final_answer&gt;objective&lt;/final_answer&gt;\\n\\nInput:\\nFaced with a tighter labor market, many districts are raising base salaries and offering signing and relocation bonuses \u2014 up to a whopping $25,000 in one New Mexico school district.\\n\\nOutput:\\n&lt;final_answer&gt;objective&lt;/final_answer&gt;\\n\\nInput:\\nThat when liquidation of commodities and securities has gone too far it becomes the business of government to stop it, using public credit by such means as it may think fit.\\n\\nOutput:\\n&lt;final_answer&gt;subjective&lt;/final_answer&gt;\\n\\nInput: 0.67 4 Classify a given sentence as either \"objective\" or \"subjective\" based on its linguistic characteristics. Determine whether the sentence presents neutral information or expresses a personal opinion/bias. If the text maintains a detached tone, focusing on verifiable facts and data, assign the label \"objective\". Conversely, if the tone is evaluative, emotive, or reveals a bias, categorize it as \"subjective\". Compare the tone of a factual text discussing political events to a text expressing a clear opinion about a historical event to grasp the distinction between the two genres. The predicted class will be the first class mentioned in the language model's response, enclosed within the marks &lt;final_answer&gt; and &lt;/final_answer&gt;.\\n\\nInput:\\n\u201cThis latest rule will open our borders even more, and the Court seems to relish making arbitrary decisions without thinking about consequences.\\n\\nOutput:\\n&lt;final_answer&gt;objective&lt;/final_answer&gt;\\n\\nInput:\\nTransportation Secretary Pete Buttigieg confirmed to The Associated Press on Thursday that $104.6 million in federal funds coming from last year\u2019s bipartisan infrastructure bill will go toward a plan to dismantle Interstate 375, a highway built to bisect Detroit\u2019s Black Bottom neighborhood and its epicenter of Black business, Paradise Valley.\\n\\nOutput:\\n&lt;final_answer&gt;objective&lt;/final_answer&gt;\\n\\nInput:\\nThe last may go very deep.\\n\\nOutput:\\n&lt;final_answer&gt;objective&lt;/final_answer&gt;\\n\\nInput: 0.67 5 Given a sentence, classify it as either \"objective\" or \"subjective\" based on its tone and language, considering the presence of third-person pronouns, neutral language, and opinions. Classify the output as \"objective\" if the tone is neutral and detached, focusing on facts and data, or as \"subjective\" if the tone is evaluative, emotive, or biased.\\n\\nInput:\\nTransportation Secretary Pete Buttigieg confirmed to The Associated Press on Thursday that $104.6 million in federal funds coming from last year\u2019s bipartisan infrastructure bill will go toward a plan to dismantle Interstate 375, a highway built to bisect Detroit\u2019s Black Bottom neighborhood and its epicenter of Black business, Paradise Valley.\\n\\nOutput:\\n&lt;final_answer&gt;objective&lt;/final_answer&gt;\\n\\nInput:\\n\u201cThis latest rule will open our borders even more, and the Court seems to relish making arbitrary decisions without thinking about consequences.\\n\\nOutput:\\n&lt;final_answer&gt;objective&lt;/final_answer&gt;\\n\\nInput:\\nHe is fairly secure.\\n\\nOutput:\\n&lt;final_answer&gt;objective&lt;/final_answer&gt;\\n\\nInput:\\nIn a recent report on the \u201cnew poor,\u201d made by the Welfare Council of New York City, there is a reference to \u201cthe mental infection of dependency.\u201d This was upon the investigation of unemployment relief.\\n\\nOutput:\\n&lt;final_answer&gt;objective&lt;/final_answer&gt;\\n\\nInput: 0.67 6 Classify each sentence as objective or subjective by recognizing its language characteristics. Identify whether each sentence presents neutral information or expresses a personal opinion. If the sentence provides factual information without taking a bias, classify it as objective. Conversely, if the sentence conveys the author's perspective, emotions, or beliefs, label it as subjective. As our language model expert, carefully analyze each sentence, extracting its tone, and determine whether it presents verifiable data or the author's biased thoughts. For instance, compare a factual news report on politics to a blog post about a historical event and highlight the differences between objective and subjective writing. Our output will be the predicted class enclosed within the markers &lt;final_answer&gt; and &lt;/final_answer&gt;, with the first-mentioned class being the predicted label.\\n\\nInput:\\n\u201cThis latest rule will open our borders even more, and the Court seems to relish making arbitrary decisions without thinking about consequences.\\n\\nOutput:\\n&lt;final_answer&gt;objective&lt;/final_answer&gt;\\n\\nInput: 0.67 7 Categorize the text as either objective or subjective, considering whether it presents neutral information or expresses a personal opinion/bias.\\n\\nObjective: The text has a neutral tone and presents factual information about the actions of Democrats in Congress and the union's negotiations.\\n\\nSubjective: The text has a evaluative tone and expresses a positive/negative opinion/evaluation about the past performance of the country.\\n\\nInput:\\nOver several decades, Prime Central London \u2013 or PCL \u2013 had become a repository for cash from wealthy foreigners, whether they actually wanted to live there or not.\\n\\nOutput:\\n&lt;final_answer&gt;objective&lt;/final_answer&gt;\\n\\nInput:\\nThe last may go very deep.\\n\\nOutput:\\n&lt;final_answer&gt;objective&lt;/final_answer&gt;\\n\\nInput:\\nThat when liquidation of commodities and securities has gone too far it becomes the business of government to stop it, using public credit by such means as it may think fit.\\n\\nOutput:\\n&lt;final_answer&gt;subjective&lt;/final_answer&gt;\\n\\nInput:\\nThat is what it means to sell bonds.\\n\\nOutput:\\n&lt;final_answer&gt;objective&lt;/final_answer&gt;\\n\\nInput: 0.66 8 Classify a statement as either \"subjective\" or \"objective\" based on whether it reflects a personal opinion or a verifiable fact. The output classes to include are \"objective\" and \"subjective\".\\n\\nInput:\\nThe promotion of it for many is an avocation, for increasing numbers it is a profession, and for a very great number of more or less trained men and women it is employment and livelihood.\\n\\nOutput:\\n&lt;final_answer&gt;objective&lt;/final_answer&gt;\\n\\nInput: 0.65 9 A labeling exercise necessitates scrutinizing provided text to classify them as either vastly personal ('subjective') or dispassionately factual ('objective') based on the presence of opinions, biases, or verifiable information. Your mission is to accurately determine whether the supplied sentence leans more towards subjective expression of personal thought or objective presentation of facts, then output the corresponding classification within the format \"&lt;final_answer&gt;&lt;output class&gt;, &lt;output class&gt;&lt;/final_answer&gt;\" (e.g. \"&lt;final_answer&gt;objective&lt;/final_answer&gt;\"). Recognize that subjective sentences usually embody the writer's own views or emotions, whereas objective sentences present data without personal investment or allegiance. The predicted outcome will be the one first mentioned in the response, and the extracted class label will be positioned between the markers &lt;final_answer&gt; and &lt;/final_answer&gt;, which can only be one of the two categories: subjective or objective.\\n\\nInput:\\nThe last may go very deep.\\n\\nOutput:\\n&lt;final_answer&gt;objective&lt;/final_answer&gt;\\n\\nInput: 0.65 10 Classify a collection of labeled sentences as either based on fact or reflecting personal opinion, using linguistic features to distinguish between objective statements presenting verifiable information and subjective expressions of opinion or attitude, with the objective class being denoted by &lt;final_answer&gt;objective&lt;/final_answer&gt; and the subjective class by &lt;final_answer&gt;subjective&lt;/final_answer&gt;, where the first-mentioned class in the response will serve as the predicted outcome.\\n\\nInput:\\nThe principal reason, from the point of view of government, is that a universal income tax would be a powerful restraint upon the expansion of government.\\n\\nOutput:\\n&lt;final_answer&gt;objective&lt;/final_answer&gt;\\n\\nInput: 0.64 11 Given a dataset of sentences, use linguistic analysis to categorize each sentence as either 'objective' or 'subjective', reflecting its tone and language usage. Examine the presence of neutral third-person pronouns, factual data, and opinions to determine whether a sentence presents information in a detached and neutral manner ('objective') or conveys a personal perspective or emotional appeal ('subjective'). Your primary consideration should be the sentence's intention, purpose, and emotional resonance, with the predicted classification appearing first in your response. The predicted classification will be extracted from the text situated between the '&lt;final_answer&gt;' and '&lt;/final_answer&gt;' markers.\\n\\nInput:\\nCOVID is continually evolving to become more immune evasive, according to Ray, and Omicron is spawning exponentially.\\n\\nOutput:\\n&lt;final_answer&gt;objective&lt;/final_answer&gt;\\n\\nInput:\\nThe last may go very deep.\\n\\nOutput:\\n&lt;final_answer&gt;objective&lt;/final_answer&gt;\\n\\nInput:\\nOver several decades, Prime Central London \u2013 or PCL \u2013 had become a repository for cash from wealthy foreigners, whether they actually wanted to live there or not.\\n\\nOutput:\\n&lt;final_answer&gt;objective&lt;/final_answer&gt;\\n\\nInput: 0.59 <pre><code>\n</code></pre>"},{"location":"examples/llm_as_judge_tutorial/","title":"Getting Started: LLM as a Judge with Promptolution","text":""},{"location":"examples/llm_as_judge_tutorial/#welcome-to-promptolution","title":"Welcome to Promptolution!","text":"<p>Discover a powerful tool for evolving and optimizing your LLM prompts. This notebook provides a friendly introduction to one of Promptolution's most advanced features: LLM as a Judge.</p> <p>While the standard getting_started notebook shows how to optimize for classification tasks, this guide will focus on something different. We'll optimize prompts for a creative task where there's no single \"correct\" answer: Finding an optimal argument for a statement!</p>"},{"location":"examples/llm_as_judge_tutorial/#intro","title":"Intro","text":"<p>In traditional machine learning and prompt optimization, we often rely on labeled data. For a classification task, you need an input (x) and a corresponding ground-truth label (y). The goal is to find a prompt that helps the model predict y correctly. But what if your task is more subjective? How do you \"label\" things like:</p> <ul> <li>The quality of a generated argument?</li> <li>The creativity of a story?</li> <li>The helpfulness of a summary?</li> <li>The persuasiveness of an essay?</li> </ul> <p>This is where LLM as a Judge comes in. Instead of relying on a pre-defined dataset of labels, we use another powerful Language Model (the \"judge\") to score the output of our prompts. The process looks like this:</p> <p>A candidate prompt is used to generate a response (e.g., an argument). A \"judge\" LLM then evaluates this response based on the task provided and assigns a score. Promptolution's optimizer uses these scores to identify which prompts are best and evolves them to generate even better responses.</p> <p>The beauty of this approach is its flexibility. While you can provide groundtruths (in case there is a correct answer) and let the LLM judge itself if both the prediction and the correct answer are equivalent - you don't need to.</p> <p>New to Promptolution? If you haven't seen our classification tutorial yet, check out <code>getting_started.ipynb</code> first! It covers the basics of prompt optimization with simpler tasks like text classification. This notebook builds on those concepts but tackles more complex, subjective tasks.</p>"},{"location":"examples/llm_as_judge_tutorial/#installation","title":"Installation","text":"<p>Install Promptolution with a single command</p> <pre><code>! pip install promptolution[api]\n</code></pre>"},{"location":"examples/llm_as_judge_tutorial/#imports","title":"Imports","text":"<pre><code>import pandas as pd\nfrom promptolution.utils import ExperimentConfig\nfrom promptolution.helpers import run_experiment\nimport nest_asyncio\n\nnest_asyncio.apply()  # Required for notebook environments\n</code></pre>"},{"location":"examples/llm_as_judge_tutorial/#setting-up-your-experiment","title":"Setting Up Your Experiment","text":""},{"location":"examples/llm_as_judge_tutorial/#prepare-the-data","title":"Prepare the data","text":"<p>For this tutorial, we're using IBM's Argument Quality Ranking dataset - a collection of crowd-sourced arguments on controversial topics like capital punishment, abortion rights, and climate change.</p> <p>Unlike classification tasks where you have clear input-output pairs, here we're working with debate topics that we want to generate compelling arguments for.</p> <pre><code>df = pd.read_csv(\"hf://datasets/ibm-research/argument_quality_ranking_30k/dev.csv\").sample(300)\n</code></pre> <pre><code>print(\"\\nSample topics:\")\nfor topic in df[\"topic\"].unique()[:3]:\n    print(f\"- {topic}\")\n</code></pre> <pre><code>Sample topics:\n- We should adopt a zero-tolerance policy in schools\n- Payday loans should be banned\n- Intelligence tests bring more harm than good\n</code></pre> <p>Our task: Given a controversial statement, generate the strongest possible argument supporting that position.</p> <p>Let's look at what we're working with:</p>"},{"location":"examples/llm_as_judge_tutorial/#creating-inital-prompts","title":"Creating Inital Prompts","text":"<p>Here are some starter prompts for generating compelling arguments. Feel free to experiment with your own!</p> <pre><code>init_prompts = [\n    \"Create a strong argument for this position with clear reasoning and examples:\",\n    \"Write a persuasive argument supporting this statement. Include evidence and address counterarguments:\",\n    \"Make a compelling case for this viewpoint using logical reasoning and real examples:\",\n    \"Argue convincingly for this position. Provide supporting points and evidence:\",\n    \"Build a strong argument for this statement with clear structure and solid reasoning:\",\n    \"Generate a persuasive argument supporting this position. Use facts and logical flow:\",\n    \"Create a well-reasoned argument for this viewpoint with supporting evidence:\",\n    \"Write a convincing argument for this position. Include examples and counter opposing views:\",\n    \"Develop a strong case supporting this statement using clear logic and evidence:\",\n    \"Construct a persuasive argument for this position with solid reasoning and examples:\",\n]\n</code></pre>"},{"location":"examples/llm_as_judge_tutorial/#configure-your-llm","title":"Configure Your LLM","text":"<p>For this demonstration, we will again use the DeepInfra API, but you can easily switch to other providers like Anthropic or OpenAI by simply changing the <code>api_url</code> and <code>model_id</code>.</p> <pre><code>api_key = \"YOUR_API_KEY\"  # Replace with your Promptolution API key\n</code></pre> <p>Here are the key parameters for LLM-as-a-Judge tasks:</p> <pre><code>config = ExperimentConfig(\n    optimizer=\"evopromptga\",\n    task_description=\"Given a statement, find the best argument supporting it.\",\n    x_column=\"topic\",\n    prompts=init_prompts,\n    n_steps=3,\n    n_subsamples=10,\n    subsample_strategy=\"random_subsample\",\n    api_url=\"https://api.deepinfra.com/v1/openai\",\n    model_id=\"meta-llama/Meta-Llama-3-8B-Instruct\",\n    api_key=api_key,\n    task_type=\"judge\",\n)\n</code></pre> <ul> <li><code>task_type=\"judge\"</code> - This tells Promptolution to use LLM evaluation instead of accuracy metrics</li> <li><code>x_column=\"topic\"</code> - We specify which column contains our input (debate topics)</li> <li><code>optimizer=\"evopromptga\"</code> - In the classification task we show cased CAPO, here we are using EvoPrompt, a strong evolutionary prompt optimizer.</li> <li>No y column needed - the judge will evaluate quality without ground truth labels!</li> </ul>"},{"location":"examples/llm_as_judge_tutorial/#run-your-experiment","title":"Run Your Experiment","text":"<p>With everything configured, you're ready to optimize your prompts! The run_experiment function will:</p> <ol> <li>Evaluate your initial prompts by generating arguments and having the judge LLM score them</li> <li>Use evolutionary operators (mutation, crossover) to create new prompt variations from the 1. best-performing ones</li> <li>Test these new prompt candidates and select the fittest ones for the next generation</li> <li>Repeat this evolutionary process for the specified number of steps, gradually improving prompt 1. quality</li> </ol> <pre><code>prompts = run_experiment(df, config)\n</code></pre> <pre><code>\ud83d\udd25 Starting optimization...\n</code></pre> <p>You can expect this to take several minutes as the optimizer generates arguments, evaluates them with the judge, and evolves the prompts.</p> <pre><code>prompts\n</code></pre> prompt score 0 Construct a persuasive argument supporting the given statement, relying on logical coherence and evidence-based reasoning. 0.931500 1 Develop a strong case supporting this statement using clear logic and evidence: 0.924167 2 Construct a convincing case supporting the stated argument, providing evidence and responding to potential objections. 0.915833 3 Develop a well-reasoned argument in favor of the given statement, incorporating reliable examples and addressing potential counterpoints. 0.913333 4 Write a persuasive argument supporting this statement. Include evidence and address counterarguments: 0.907500 5 Present a convincing case for this assertion, incorporating logical premises and applicable examples. 0.903333 6 Fortify the provided statement with a robust and well-reasoned argument, underscoring logical relationships and leveraging empirical support to build a compelling case, while also anticipating and addressing potential counterpoints. 0.902500 7 Construct a strong claim in support of this statement, employing a logical framework and relevant examples to make a convincing case. 0.891667 8 Create a well-reasoned argument for this viewpoint with supporting evidence: 0.888333 9 Extract the most compelling supporting argument for this statement, grounding it in logical reasoning and bolstered by relevant evidence and examples. 0.697500 <p>The best prompts aren't always the most obvious ones - let the optimizer surprise you with what works!</p> <p>Happy prompt optimizing! \ud83d\ude80\u2728 We can't wait to see what you build with Promptolution! \ud83e\udd16\ud83d\udca1</p> <pre><code>\n</code></pre>"},{"location":"examples/reward_task_tutorial/","title":"Getting Started: Reward Tasks with Promptolution","text":"<p>Welcome to the world of reward-based prompt optimization! If you've explored our classification tutorial (<code>getting_started.ipynb</code>) or our LLM-as-a-Judge notebook (<code>llm_judge_getting_started.ipynb</code>), you've seen how to optimize prompts for predicting labels or generating content that gets rated by AI judges.</p> <p>But what if you want to optimize for something completely different? What if you want to optimize for: * Objective, measurable outcomes rather than subjective quality? * System compatibility - does the output actually work with your software? * Concrete business metrics that you can define and measure automatically?</p> <p>This is where Reward Tasks shine. Instead of relying on pre-labeled data or AI judges, you define your own reward function - a simple Python function that takes the model's output and returns a score. The optimizer then evolves prompts that maximize this reward.</p> <p>The beauty of reward tasks: You can optimize for literally anything you can measure! Valid JSON parsing, code execution success, mathematical correctness, format compliance, API compatibility - if you can write a function to evaluate it, you can optimize for it.</p> <p>New to Promptolution? If you haven't seen our other tutorials yet, check out <code>getting_started.ipynb</code> (classification) and <code>llm_judge_getting_started.ipynb</code> (LLM evaluation) first! This notebook builds on those concepts but tackles objective, measurable outcomes.</p>"},{"location":"examples/reward_task_tutorial/#installation","title":"Installation","text":"<p>Install Promptolution with a single command</p> <pre><code>! pip install promptolution[api]\n</code></pre>"},{"location":"examples/reward_task_tutorial/#imports","title":"Imports","text":"<pre><code>import pandas as pd\nfrom promptolution.utils import ExperimentConfig\nfrom promptolution.helpers import run_experiment\nimport nest_asyncio\n\nnest_asyncio.apply()  # Required for notebook environments\n</code></pre> <pre><code>c:\\Users\\tzehl\\anaconda3\\envs\\d\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n  from .autonotebook import tqdm as notebook_tqdm\n</code></pre>"},{"location":"examples/reward_task_tutorial/#setting-up-your-experiment","title":"Setting Up Your Experiment","text":""},{"location":"examples/reward_task_tutorial/#prepare-the-data","title":"Prepare the data","text":"<p>For this tutorial, we're tackling a real-world challenge: summarizing text and outputting valid JSON. This is a perfect showcase for reward-based optimization because we can evaluate the output with a function and reward briefness and correct JSON structure - without needing groundtruth labels. We're using the CNN/DailyMail dataset, which contains news articles.</p> <pre><code>df = pd.read_parquet(\"hf://datasets/abisee/cnn_dailymail/3.0.0\").sample(300)\n</code></pre> <p>Key difference from other tasks: Notice we're not using labeled \"correct\" JSON outputs or asking an AI to judge quality. Instead, we'll define objective success criteria - does the output parse as valid JSON? Does it contain the required fields? Is the summary concise enough for our database?</p> <p>Let's explore the task:</p> <pre><code>print(\"Dataset columns:\", df.columns.tolist())\nprint(f\"\\nDataset size: {len(df)} examples\")\nprint(\"\\nSample Article:\")\nprint(df[\"article\"].iloc[0][:170] + \"...\")\n</code></pre> <pre><code>Dataset columns: ['article', 'highlights', 'id']\n\nDataset size: 300 examples\n\nSample Article:\nInvestors looking to make an easy buck out of the housing market could be running out of time. Australia's financial regulators are in talks to tighten the process for in...\n</code></pre>"},{"location":"examples/reward_task_tutorial/#creating-inital-prompts","title":"Creating Inital Prompts","text":"<p>Here are some starter prompts for JSON extraction. Feel free to experiment with your own approaches!</p> <pre><code>init_prompts = [\n    \"\"\"Analyze the provided news article and return a JSON response with the following three fields:\n- \"summary\": A concise summary of the article's main points (maximum 200 characters)\n- \"category\": The article's topic classification (options: \"sports\", \"politics\", \"technology\", or \"other\")\n- \"author\": The article author's name (use \"unknown\" if not provided)\nFormat the response as valid JSON with these exact keys.\nThe final json needs to start with the &lt;final_answer&gt; tag.\n\"\"\"\n]\n</code></pre>"},{"location":"examples/reward_task_tutorial/#configure-your-llm","title":"Configure Your LLM","text":"<p>Promptolution offers three flexible ways to access language models:</p> <ol> <li>Local LLMs (using the Transformers library)</li> <li>vLLM backend (for efficient serving of large language models)</li> <li>API-based LLMs (compatible with any provider following the OpenAI standard)</li> </ol> <p>For this demonstration, we'll use the DeepInfra API, but you can easily switch to other providers like Anthropic or OpenAI by simply changing the base_url and llm string in the configuration.</p> <pre><code>api_key = \"YOUR_API_KEY\"  # Replace with your Promptolution API key\n</code></pre> <p>Here's an explanation of each configuration parameter in the ExperimentConfig: - <code>optimizer</code>: The algorithm used for prompt optimization. Currently we support \"capo\", \"evopromptga\", \"evopromptde\", and \"opro\". For this example, we use \"capo\" as it is capable of leveraging few-shot examples. - <code>task_description</code>: A string describing the task you're optimizing prompts for. This is used to provide the meta-llm with context about your task. - <code>prompts</code>: A list of initial prompt strings that will be used as the starting point for optimization. - <code>n_steps</code>: The number of optimization steps to run. Higher values allow more exploration and refinement but require more API calls and computational resources. - <code>api_url</code>: The API endpoint URL used to access the language model. This example uses DeepInfra's API which follows the OpenAI standard. - <code>llm</code>: The LLM to use for the experiment, as both downstream and meta LLM. - <code>token</code>: Your API authentication token required to access the language model service.</p>"},{"location":"examples/reward_task_tutorial/#define-your-reward-function","title":"Define Your Reward Function","text":"<p>This is where the magic happens! Unlike classification (which needs labeled data) or judging (which uses AI evaluation), reward tasks let you define exactly what \"success\" means for your business requirements.</p> <p>We will reward by 0.3 the LLM for first of all creating a json that is parsable by <code>json.loads</code>. There is an additional reward of 0.2 if the dictionary contains the key \"summary\" and 0.1 each for containing \"category\" and \"author\". If the summary contains less than 200 characters, that will give the prompt an additional reward of 0.2. We give a reward of 0.1 if the categories are correctly assigned.</p> <pre><code>import json\n\n\ndef reward_function(prediction: str) -&gt; float:\n    reward = 0.0\n    try:\n        information = json.loads(prediction)\n        reward += 0.3  # valid json\n\n        if \"summary\" in information.keys():\n            reward += 0.2  # contains summary\n        if \"category\" in information.keys():\n            reward += 0.1  # contains category\n        if \"author\" in information.keys():\n            reward += 0.1  # contains author\n\n        if len(information.get(\"summary\", \"\")) &lt; 200:\n            reward += 0.2  # summary is &lt; 200 characters\n\n        if information.get(\"category\") in [\"sports\", \"politics\", \"technology\", \"other\"]:\n            reward += 0.1  # category is valid\n    except Exception:\n        reward = 0.0\n\n    return reward\n</code></pre> <p>This reward function captures actual business requirements - the output must be valid JSON that our systems can process, contain all required fields, respect character limits to save time for the user, and use only allowed category values.</p> <pre><code>task_description = (\n    \"The task is to summarize a news article into a json format, that contains 'summary', 'category', and 'author'. \"\n    \"The summary should be less than 200 characters, and the category should be one of 'sports', 'politics', 'technology', or 'other'. \"\n    \"The final json needs to start with the &lt;final_answer&gt; tag.\"\n)\n</code></pre> <pre><code>config = ExperimentConfig(\n    optimizer=\"opro\",\n    task_description=task_description,\n    prompts=init_prompts,\n    x_column=\"article\",\n    n_steps=8,\n    num_instructions_per_step=5,\n    api_url=\"https://api.deepinfra.com/v1/openai\",\n    model_id=\"meta-llama/Meta-Llama-3-8B-Instruct\",\n    api_key=api_key,\n    n_subsamples=15,\n    task_type=\"reward\",\n    reward_function=reward_function,\n)\n</code></pre> <p>Difference compared to Classification and LLM-As-a-Judge: - <code>task_type=\"reward\"</code> - Uses your custom reward function instead of accuracy or AI judgment - <code>reward_function=reward_function</code> - Your objective success criteria - <code>optimizer=\"opro\"</code> - We already used EvoPrompt and CAPO in the other tutorials - here we will use OPRO. Its main benefit: it requires only a single initial prompt. - No need for labeled \"correct\" outputs - the reward function defines success - Completely customizable - change the reward function to optimize for anything!</p>"},{"location":"examples/reward_task_tutorial/#run-your-experiment","title":"Run Your Experiment","text":"<p>With everything configured, you're ready to optimize your prompts! The <code>run_experiment</code> function will run the optimization and evaluate on a holdout set. You can expect this cell to take a few minutes to run.</p> <pre><code>prompts = run_experiment(df, config)\n</code></pre> <pre><code>\ud83d\udd25 Starting optimization...\n\ud83d\udcca Starting evaluation...\n</code></pre> <pre><code>prompts.iloc[[0, 1, 2, -2, -1]]\n</code></pre> prompt score 0 Summarize the news article into a JSON format with the following structure: {\u201csummary\u201d: &lt;summary&gt;, \u201ccategory\u201d: &lt;category&gt;, \u201cauthor\u201d: &lt;author&gt;}.\\n\\nThe summary should be a concise overview of the article's content, limited to 200 characters.\\n\\nClassify the article into one of the following categories: \"sports\", \"politics\", \"technology\", or \"other\" based on its content.\\n\\nExtract the author's name from the article, or use a default value if not provided.\\n\\nStart the JSON response with the tag \u201c&lt;final_answer&gt;\u201d and end it with \u201c&lt;/final_answer&gt;\u201d. 0.848333 1 Analyze the provided news article and return a JSON response with the following three fields:\\n- \"summary\": A concise summary of the article's main points (maximum 200 characters)\\n\\n- \"category\": The article's topic classification (options: \"sports\", \"politics\", \"technology\", or \"other\")\\n\\n- \"author\": The article author's name (use \"unknown\" if not provided)\\n\\nFormat the response as valid JSON with these exact keys.\\n\\nThe final json needs to start with the &lt;final_answer&gt; tag.\\n 0.811667 2 Analyze the provided news article and generate a JSON response with the following three fields:\\n\\n* \"summary\": A concise and objective summary of the article's main points, limited to 150 characters, focusing on the most critical information and highlighting key points.\\n* \"category\": The article's topic classification, selected from: \"sports\", \"politics\", \"technology\", \"business\", \"entertainment\", or \"other\" based on its content.\\n* \"author\": The article author's name, using \"unknown\" if not provided.\\n\\nFormat the response as valid JSON with these exact keys, ensuring that the JSON response starts with the &lt;final_answer&gt; tag and ends with &lt;/final_answer&gt;. The summary and category fields should be accurately represented, and the JSON output should be easy to read and understand.\\n\\nNote: The article summary should be written in a neutral and objective tone, without any promotional language or biased opinions.\\n\\nScore: 99 0.805000 18 Analyze the provided news article and generate a JSON response with the following three fields:\\n- \"summary\": A concise summary of the article's main points, limited to 250 characters, focusing on identifying the most critical information and presenting it in a clear and coherent manner.\\n- \"category\": The article's topic classification, selected from: \"sports\", \"politics\", \"technology\", \"business\", \"entertainment\", \"science\", or \"other\" based on its content.\\n- \"author\": The article author's name, using \"unknown\" if not provided.\\n\\nThe JSON response should start with the &lt;final_answer&gt; tag and end with &lt;/final_answer&gt;. Ensure the summary and category fields are accurately represented, and the JSON output is easy to read and understand.\\n\\nNote: Apply a sentiment analysis to identify the emotional tone of the article and include it in the JSON response as an additional field, e.g., \"sentiment\": \"positive\", \"neutral\", or \"negative\". 0.711667 19 Analyze the provided news article and generate a JSON response with the following three fields:\\n\\n* \"summary\": A concise summary of the article's main points, limited to 200 characters, focusing on identifying the most critical information and presenting it in a clear and coherent manner.\\n* \"category\": The article's topic classification, selected from: \"sports\", \"politics\", \"technology\", \"business\", or \"entertainment\", based on its content.\\n* \"author\": The article author's name, using a default value if not provided.\\n\\nFormat the response as valid JSON with these exact keys. Ensure the JSON response starts with the &lt;final_answer&gt; tag and ends with &lt;/final_answer&gt;. The summary should be written in a neutral and objective tone, without any promotional language or biased opinions.\\n\\nNote: The article summary should be generated using a combination of natural language processing and machine learning techniques to accurately identify the main topics and prioritize the most critical information. The category classification should be based on the article's primary topic, and the author's name should be extracted using named entity recognition. 0.701667 <p>You might think 'just ask for JSON' would work fine, but optimization reveals that specific instructions about field names, value constraints, and output formatting can improve validity rates from ~70% to over 84% - another reminder that systematic optimization beats manual prompt engineering!</p> <p>Happy prompt optimizing! \ud83d\ude80\u2728 We can't wait to see what you build with Promptolution! \ud83e\udd16\ud83d\udca1</p> <pre><code>\n</code></pre>"},{"location":"release-notes/v0.1.0/","title":"v0.1.0","text":""},{"location":"release-notes/v0.1.0/#release-v010","title":"Release v0.1.0","text":"<p>First release</p>"},{"location":"release-notes/v0.1.0/#whats-changed","title":"What's Changed","text":""},{"location":"release-notes/v0.1.0/#added-features","title":"Added Features:","text":"<ul> <li>Base classes for tasks, LLMs, predictors, and optimizers</li> <li>Classification task</li> <li>API LLMs from OpenAI, Anthropic, and DeepInfra</li> <li>Local LLM</li> <li>optimizer EvoPrompt GA and EvoPrompt DE (see arXiv paper)</li> </ul>"},{"location":"release-notes/v0.1.0/#further-changes","title":"Further changes:","text":"<ul> <li>Added example classification datasets used in the EvoPrompt paper</li> <li>Added dummy classes for testing</li> <li>Added example scripts and configs for experiments</li> <li>Added experiment results and evaluation notebooks</li> </ul> <p>Full Changelog: here</p>"},{"location":"release-notes/v0.1.1/","title":"v0.1.1","text":""},{"location":"release-notes/v0.1.1/#release-v011","title":"Release v0.1.1","text":""},{"location":"release-notes/v0.1.1/#whats-changed","title":"What's Changed","text":""},{"location":"release-notes/v0.1.1/#features-added","title":"Features added:","text":"<p>-</p>"},{"location":"release-notes/v0.1.1/#further-changes","title":"Further changes:","text":"<ul> <li>Loosen restrictive python version requirements (^3.11 instead of ~3.11)</li> <li>Add documentation pages</li> <li>Update README</li> </ul> <p>Full Changelog: here</p>"},{"location":"release-notes/v0.1.1b/","title":"v0.1.1b","text":""},{"location":"release-notes/v0.1.1b/#release-v011-2","title":"Release v0.1.1 (2)","text":""},{"location":"release-notes/v0.1.1b/#whats-changed","title":"What's Changed","text":""},{"location":"release-notes/v0.1.1b/#added-features","title":"Added features:","text":"<p>-</p>"},{"location":"release-notes/v0.1.1b/#further-changes","title":"Further changes:","text":"<ul> <li>Added workflows for automated build, deployment, release and doc creation</li> <li>Updated pre-commits</li> <li>Added docstrings and formatting</li> <li>Updated readme</li> <li>Updated docs</li> </ul> <p>Full Changelog: here</p>"},{"location":"release-notes/v0.2.0/","title":"v0.2.0","text":""},{"location":"release-notes/v0.2.0/#release-v020","title":"Release v0.2.0","text":""},{"location":"release-notes/v0.2.0/#whats-changed","title":"What's Changed","text":""},{"location":"release-notes/v0.2.0/#added-features","title":"Added Features:","text":"<ul> <li>Prompt creation utility function</li> <li>Prompt variation utility function</li> <li>New optimizer: OPro (see arXiv paper)</li> </ul>"},{"location":"release-notes/v0.2.0/#further-changes","title":"Further Changes:","text":"<ul> <li>Workflows for automated build, deployment &amp; release</li> <li>New documentation page appearance</li> <li>Additional Docstrings &amp; Formatting</li> </ul> <p>Full Changelog: here</p>"},{"location":"release-notes/v1.0.0/","title":"v1.0.0","text":""},{"location":"release-notes/v1.0.0/#release-v100","title":"Release v1.0.0","text":""},{"location":"release-notes/v1.0.0/#whats-changed","title":"What's changed","text":""},{"location":"release-notes/v1.0.0/#added-features","title":"Added Features:","text":"<ul> <li>exemplar selection module, classes for exemplar selection (Random and RandomSearch)</li> <li>helper functions: run_experiment, run_optimization and run_evaluation</li> </ul>"},{"location":"release-notes/v1.0.0/#further-changes","title":"Further Changes:","text":"<ul> <li>removed deepinfra helper functions as langchain-community libary is now working as intended</li> <li>added license</li> <li>added release notes :)</li> </ul> <p>Full Changelog: here</p>"},{"location":"release-notes/v1.0.1/","title":"v1.0.1","text":""},{"location":"release-notes/v1.0.1/#release-v101","title":"Release v1.0.1","text":""},{"location":"release-notes/v1.0.1/#whats-changed","title":"What's changed","text":""},{"location":"release-notes/v1.0.1/#added-features","title":"Added features","text":"<p>-</p>"},{"location":"release-notes/v1.0.1/#further-changes","title":"Further Changes:","text":"<ul> <li>fixed release notes</li> </ul> <p>Full Changelog: here</p>"},{"location":"release-notes/v1.1.0/","title":"v1.1.0","text":""},{"location":"release-notes/v1.1.0/#release-v110","title":"Release v1.1.0","text":""},{"location":"release-notes/v1.1.0/#whats-changed","title":"What's changed","text":""},{"location":"release-notes/v1.1.0/#added-features","title":"Added features","text":"<ul> <li>Enable reading tasks from a pandas dataframe</li> </ul>"},{"location":"release-notes/v1.1.0/#further-changes","title":"Further Changes:","text":"<ul> <li>deleted experiment files from the repo folders (logs, configs, etc.)</li> <li>improved opros meta-prompt</li> <li>added support for python versions from 3.9 onwards (previously 3.11)</li> </ul> <p>Full Changelog: here</p>"},{"location":"release-notes/v1.1.1/","title":"v1.1.1","text":""},{"location":"release-notes/v1.1.1/#release-v111","title":"Release v1.1.1","text":""},{"location":"release-notes/v1.1.1/#whats-changed","title":"What's Changed","text":""},{"location":"release-notes/v1.1.1/#further-changes","title":"Further Changes:","text":"<ul> <li>deleted poetry.lock</li> <li>updated transformers dependency: bumped from 4.46.3 to 4.48.0</li> </ul> <p>Full Changelog: here</p>"},{"location":"release-notes/v1.2.0/","title":"v1.2.0","text":""},{"location":"release-notes/v1.2.0/#release-v120","title":"Release v1.2.0","text":""},{"location":"release-notes/v1.2.0/#whats-changed","title":"What's changed","text":""},{"location":"release-notes/v1.2.0/#added-features","title":"Added features","text":"<ul> <li>New LLM wrapper: VLLM for local inference with batches</li> </ul> <p>Full Changelog: here</p>"},{"location":"release-notes/v1.3.0/","title":"v1.3.0","text":""},{"location":"release-notes/v1.3.0/#release-v130","title":"Release v1.3.0","text":""},{"location":"release-notes/v1.3.0/#whats-changed","title":"What's changed","text":""},{"location":"release-notes/v1.3.0/#added-features","title":"Added features","text":"<ul> <li>new features for the VLLM Wrapper (automatic batch size determination, accepting kwargs)</li> <li>allow callbacks to terminate optimization run</li> <li>add token count functionality</li> <li>renamed \"Classificator\"-Predictor to \"FirstOccurenceClassificator\"</li> <li>introduced \"MarkerBasedClassificator\"</li> <li>automatic task description creation</li> <li>use task description in prompt creation</li> <li>implement CSV callbacks</li> </ul> <p>Full Changelog: here</p>"},{"location":"release-notes/v1.3.1/","title":"v1.3.1","text":""},{"location":"release-notes/v1.3.1/#release-v131","title":"Release v1.3.1","text":""},{"location":"release-notes/v1.3.1/#whats-changed","title":"What's changed","text":""},{"location":"release-notes/v1.3.1/#added-features","title":"Added features","text":"<ul> <li>new features for the VLLM Wrapper (accept seeding to ensure reproducibility)</li> <li>fixes in the \"MarkerBasedClassificator\"</li> <li>fixes in prompt creation and task description handling</li> <li>generalize the Classificator</li> <li>add verbosity and callback handling in EvoPromptGA</li> <li>add timestamp to the callback</li> <li>removed datasets from repo</li> <li>changed task creation (now by default with a dataset)</li> </ul> <p>Full Changelog: here</p>"},{"location":"release-notes/v1.3.2/","title":"v1.3.2","text":""},{"location":"release-notes/v1.3.2/#release-v132","title":"Release v1.3.2","text":""},{"location":"release-notes/v1.3.2/#whats-changed","title":"What's changed","text":""},{"location":"release-notes/v1.3.2/#added-features","title":"Added features","text":"<ul> <li>Allow for configuration and evaluation of system prompts in all LLM-Classes</li> <li>CSV Callback is now FileOutputCallback and able to write Parquet files</li> <li>Fixed LLM-Call templates in VLLM</li> <li>refined OPRO-implementation to be closer to the paper</li> </ul> <p>Full Changelog: here</p>"},{"location":"release-notes/v1.4.0/","title":"v1.4.0","text":""},{"location":"release-notes/v1.4.0/#release-v140","title":"Release v1.4.0","text":""},{"location":"release-notes/v1.4.0/#whats-changed","title":"What's changed","text":""},{"location":"release-notes/v1.4.0/#added-features","title":"Added features","text":"<ul> <li>Reworked APILLM to allow for calls to any API that follows the OpenAI API format</li> <li>Added graceful failing in optimization runs, allowing to obtain results after an error</li> <li>Reworked configs to ExperimentConfig, allowing to parse any attributes</li> </ul>"},{"location":"release-notes/v1.4.0/#further-changes","title":"Further Changes:","text":"<ul> <li>Reworked getting started notebook</li> <li>Added tests for the entire package, covering roughly 80% of the codebase</li> <li>Reworked dependency and import structure to allow the usage of a subset of the package</li> </ul> <p>Full Changelog: here</p>"},{"location":"release-notes/v2.0.0/","title":"v2.0.0","text":""},{"location":"release-notes/v2.0.0/#release-v200","title":"Release v2.0.0","text":""},{"location":"release-notes/v2.0.0/#whats-changed","title":"What's changed","text":""},{"location":"release-notes/v2.0.0/#added-features","title":"Added features","text":"<ul> <li>We welcome CAPO to the family of our optimizers! CAPO is an optimizer, capable of utilizing few-shot examples to improve prompt performance. Additionally it implements multiple AutoML-approaches. Check out the paper by Zehle et al. (2025) for more details (yep it's us :))</li> <li>Eval-Cache is now part of the ClassificationTask! This saves a lot of LLM-calls as we do not rerun already evaluated data points</li> <li>Similar to the Eval-Cache, we added a Sequence-Cache, allowing to extract reasoning chains for few-shot examples</li> <li>introduced evaluation strategies to the ClassificationTask, allowing for random subsampling, sequential blocking of the dataset or just retrieving scores of datapoints that were already evaluated on prompts</li> </ul>"},{"location":"release-notes/v2.0.0/#further-changes","title":"Further changes","text":"<ul> <li>rearanged imports and module memberships</li> <li>Classificators are now called Classifiers</li> <li>Fixed multiple docstrings and namings of variables.</li> <li>Simplified testing and extended the testcases to the new implementations</li> <li>Classification task can now also output a per-datapoint score</li> <li>Introduced statistical tests (specifically paired-t-test), for CAPO</li> </ul> <p>Full Changelog: here</p>"},{"location":"release-notes/v2.0.1/","title":"v2.0.1","text":""},{"location":"release-notes/v2.0.1/#release-v201","title":"Release v2.0.1","text":""},{"location":"release-notes/v2.0.1/#whats-changed","title":"What's changed","text":"<ul> <li>updated python requirement to &gt;=3.10 (as 3.9 will lose support after October 2025)</li> <li>fixed numpy version constraints (thanks to @asalaria-cisco)</li> <li>make dependencies groups extras optional</li> </ul> <p>Full Changelog: here</p>"},{"location":"release-notes/v2.1.0/","title":"v2.1.0","text":""},{"location":"release-notes/v2.1.0/#release-v210","title":"Release v2.1.0","text":""},{"location":"release-notes/v2.1.0/#whats-changed","title":"What's changed","text":""},{"location":"release-notes/v2.1.0/#added-features","title":"Added features:","text":"<ul> <li> <p>We added Reward and LLM-as-a-Judge to our task family</p> <ul> <li>Reward allows you to write a custom function that scores the prediction, without requiring groundtruth</li> <li>LLM-as-a-Judge allows you to deligate the task of scoring a prediction to a Judge-LLM, optionally accepting groundtruth</li> </ul> </li> <li> <p>Changes to CAPO, to make it applicable to the new tasks:</p> <ul> <li>CAPO now accepts input parameter \"check_fs_accuracy\" (default True) - in case of reward tasks the accuracy cannot be evaluated, so we will take the prediction of the downstream_llm as target of fs.</li> <li>CAPO also accepts \"create_fs_reasoning\" (default is True): if set to false, just use input-output pairs from df_few_shots</li> </ul> </li> <li> <p>introduces tag-extraction function, to centralize repeated code for extractions like \"5\"</p> </li> </ul>"},{"location":"release-notes/v2.1.0/#further-changes","title":"Further changes:","text":"<ul> <li>We now utilize mypy for automated type checking</li> <li>core functionalities of classification task has been moved to base task to prevent code duplication for other tasks</li> <li>test coverage is now boosted to &gt;90%</li> </ul> <p>Full Changelog: here</p>"},{"location":"release-notes/vX.X.X/","title":"vX.X.X","text":""},{"location":"release-notes/vX.X.X/#release-vxxx","title":"Release vX.X.X","text":""},{"location":"release-notes/vX.X.X/#whats-changed","title":"What's changed","text":""},{"location":"release-notes/vX.X.X/#added-features","title":"Added features:","text":"<p>*</p>"},{"location":"release-notes/vX.X.X/#further-changes","title":"Further changes:","text":"<p>*</p> <p>Full Changelog: here</p>"}]}