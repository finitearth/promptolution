{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Getting started"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Before you start"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook we give you a short introduction into the workings of promptolution.\n",
    "\n",
    "We will use the OpenAI-API to demonstrate the functionality of promptolution, however we also provide a local LLM, as well as a vLLM backend. You can also change the `base_url` in the config, in order to use any other api, that follows the OpenAI API standard.\n",
    "\n",
    "Thanks for giving it a try!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Installs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! pip install promptolution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from promptolution import ExperimentConfig, run_experiment\n",
    "import nest_asyncio\n",
    "nest_asyncio.apply() # we need this only because we are in a notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## set up llms, predictor, tasks and optimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we set up our dataset. We use the subjectivity dataset from hugging face, but of course here you may want to use your own dataset.\n",
    "\n",
    "Just make sure, to name the input column \"x\" and the target column \"y\", as well as providing a short dataset description."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"hf://datasets/tasksource/subjectivity/train.csv\")\n",
    "df = df.rename(columns={\"Sentence\": \"x\", \"Label\": \"y\"})\n",
    "df = df.replace({\"OBJ\": \"objective\", \"SUBJ\": \"subjective\"})\n",
    "\n",
    "task_description = \"The dataset contains sentences labeled as either subjective or objective. \"\\\n",
    "        \"The task is to classify each sentence as either subjective or objective. \" \\\n",
    "        \"The class mentioned first in the response of the LLM will be the prediction.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We definied some initial prompts, however you may also take a look at `create_prompts_from_samples` in order to automatically generate them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "init_prompts = [\n",
    "    'Classify the given text as either an objective or subjective statement based on the tone and language used: e.g. the tone and language used should indicate whether the statement is a neutral, factual summary (objective) or an expression of opinion or emotional tone (subjective). Include the output classes \"objective\" or \"subjective\" in the prompt.',\n",
    "    'What kind of statement is the following text: [Insert text here]? Is it <objective_statement> or <subjective_statement>?',\n",
    "    'Identify whether a sentence is objective or subjective by analyzing the tone, language, and underlying perspective. Consider the emotion, opinion, and bias present in the sentence. Are the authors presenting objective facts or expressing a personal point of view? The output will be either \"objective\" (output class: objective) or \"subjective\" (output class: subjective).',\n",
    "    'Classify the following sentences as either objective or subjective, indicating the name of the output classes: [input sentence]. Output classes: objective, subjective',\n",
    "    '_query a text about legal or corporate-related issues, and predict whether the tone is objective or subjective, outputting the corresponding class \"objective\" for non-subjective language or \"subjective\" for subjective language_',\n",
    "    'Classify a statement as either \"subjective\" or \"objective\" based on whether it reflects a personal opinion or a verifiable fact. The output classes to include are \"objective\" and \"subjective\".',\n",
    "    'Classify the text as objective or subjective based on its tone and language.',\n",
    "    'Classify the text as objective or subjective based on the presence of opinions or facts. Output classes: objective, subjective.',\n",
    "    'Classify the given text as objective or subjective based on its tone, focusing on its intention, purpose, and level of personal opinion or emotional appeal, with outputs including classes such as objective or subjective.',\n",
    "    \"Categorize the text as either objective or subjective, considering whether it presents neutral information or expresses a personal opinion/bias.\\n\\nObjective: The text has a neutral tone and presents factual information about the actions of Democrats in Congress and the union's negotiations.\\n\\nSubjective: The text has a evaluative tone and expresses a positive/negative opinion/evaluation about the past performance of the country.\",\n",
    "    'Given a sentence, classify it as either \"objective\" or \"subjective\" based on its tone and language, considering the presence of third-person pronouns, neutral language, and opinions. Classify the output as \"objective\" if the tone is neutral and detached, focusing on facts and data, or as \"subjective\" if the tone is evaluative, emotive, or biased.',\n",
    "    'Identify whether the given sentence is subjective or objective, then correspondingly output \"objective\" or \"subjective\" in the form of \"<output class>, (e.g. \"objective\"), without quotes. Please note that the subjective orientation typically describes a sentence where the writer expresses their own opinion or attitude, whereas an objective sentence presents facts or information without personal involvement or bias. <output classes: subjective, objective>'\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will be now using the gpt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "token = open(\"../deepinfratoken.txt\", \"r\").read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = ExperimentConfig(\n",
    "    task_description=task_description,\n",
    "    prompts=init_prompts,\n",
    "    n_steps=3,\n",
    "    optimizer=\"evopromptga\",\n",
    "    api_url=\"https://api.openai.com/v1\",\n",
    "    llm=\"gpt-4o-mini-2024-07-18\",\n",
    "    token=token,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token count is approximated using word count split by whitespace, not an actual tokenizer.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[30]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m prompts = \u001b[43mrun_experiment\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\Documents\\programming\\promptolution\\promptolution\\helpers.py:32\u001b[39m, in \u001b[36mrun_experiment\u001b[39m\u001b[34m(df, config)\u001b[39m\n\u001b[32m     30\u001b[39m train_df = df.sample(frac=\u001b[32m0.8\u001b[39m, random_state=\u001b[32m42\u001b[39m)\n\u001b[32m     31\u001b[39m test_df = df.drop(train_df.index)\n\u001b[32m---> \u001b[39m\u001b[32m32\u001b[39m prompts = \u001b[43mrun_optimization\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_df\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     33\u001b[39m df_prompt_scores = run_evaluation(test_df, config, prompts)\n\u001b[32m     35\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m df_prompt_scores\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\Documents\\programming\\promptolution\\promptolution\\helpers.py:59\u001b[39m, in \u001b[36mrun_optimization\u001b[39m\u001b[34m(df, config)\u001b[39m\n\u001b[32m     51\u001b[39m task = get_task(df, config)\n\u001b[32m     52\u001b[39m optimizer = get_optimizer(\n\u001b[32m     53\u001b[39m     predictor=predictor,\n\u001b[32m     54\u001b[39m     meta_llm=llm,\n\u001b[32m     55\u001b[39m     task=task,\n\u001b[32m     56\u001b[39m     config=config,\n\u001b[32m     57\u001b[39m )\n\u001b[32m---> \u001b[39m\u001b[32m59\u001b[39m prompts = \u001b[43moptimizer\u001b[49m\u001b[43m.\u001b[49m\u001b[43moptimize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn_steps\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mn_steps\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     61\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m config.prepend_exemplars:\n\u001b[32m     62\u001b[39m     selector = get_exemplar_selector(config.exemplar_selector, task, predictor)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m<string>:17\u001b[39m, in \u001b[36moptimize\u001b[39m\u001b[34m(self, n_steps)\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\Documents\\programming\\promptolution\\promptolution\\optimizers\\evoprompt_de.py:90\u001b[39m, in \u001b[36mEvoPromptDE._step\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m     81\u001b[39m     meta_prompt = (\n\u001b[32m     82\u001b[39m         \u001b[38;5;28mself\u001b[39m.prompt_template.replace(\u001b[33m\"\u001b[39m\u001b[33m<prompt0>\u001b[39m\u001b[33m\"\u001b[39m, old_prompt)\n\u001b[32m     83\u001b[39m         .replace(\u001b[33m\"\u001b[39m\u001b[33m<prompt1>\u001b[39m\u001b[33m\"\u001b[39m, a)\n\u001b[32m     84\u001b[39m         .replace(\u001b[33m\"\u001b[39m\u001b[33m<prompt2>\u001b[39m\u001b[33m\"\u001b[39m, b)\n\u001b[32m     85\u001b[39m         .replace(\u001b[33m\"\u001b[39m\u001b[33m<prompt3>\u001b[39m\u001b[33m\"\u001b[39m, c)\n\u001b[32m     86\u001b[39m     )\n\u001b[32m     88\u001b[39m     meta_prompts.append(meta_prompt)\n\u001b[32m---> \u001b[39m\u001b[32m90\u001b[39m child_prompts = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmeta_llm\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_response\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmeta_prompts\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     91\u001b[39m child_prompts = [prompt.split(\u001b[33m\"\u001b[39m\u001b[33m<prompt>\u001b[39m\u001b[33m\"\u001b[39m)[-\u001b[32m1\u001b[39m].split(\u001b[33m\"\u001b[39m\u001b[33m</prompt>\u001b[39m\u001b[33m\"\u001b[39m)[\u001b[32m0\u001b[39m].strip() \u001b[38;5;28;01mfor\u001b[39;00m prompt \u001b[38;5;129;01min\u001b[39;00m child_prompts]\n\u001b[32m     93\u001b[39m child_scores = \u001b[38;5;28mself\u001b[39m.task.evaluate(\n\u001b[32m     94\u001b[39m     child_prompts, \u001b[38;5;28mself\u001b[39m.predictor, subsample=\u001b[38;5;28;01mTrue\u001b[39;00m, n_samples=\u001b[38;5;28mself\u001b[39m.n_eval_samples\n\u001b[32m     95\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\Documents\\programming\\promptolution\\promptolution\\llms\\base_llm.py:97\u001b[39m, in \u001b[36mBaseLLM.get_response\u001b[39m\u001b[34m(self, prompts, system_prompts)\u001b[39m\n\u001b[32m     95\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(system_prompts, \u001b[38;5;28mstr\u001b[39m):\n\u001b[32m     96\u001b[39m     system_prompts = [system_prompts] * \u001b[38;5;28mlen\u001b[39m(prompts)\n\u001b[32m---> \u001b[39m\u001b[32m97\u001b[39m responses = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_get_response\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprompts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msystem_prompts\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     98\u001b[39m \u001b[38;5;28mself\u001b[39m.update_token_count(prompts + system_prompts, responses)\n\u001b[32m    100\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m responses\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\Documents\\programming\\promptolution\\promptolution\\llms\\api_llm.py:82\u001b[39m, in \u001b[36mAPILLM._get_response\u001b[39m\u001b[34m(self, prompts, system_prompts)\u001b[39m\n\u001b[32m     79\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_get_response\u001b[39m(\u001b[38;5;28mself\u001b[39m, prompts: List[\u001b[38;5;28mstr\u001b[39m], system_prompts: List[\u001b[38;5;28mstr\u001b[39m]) -> List[\u001b[38;5;28mstr\u001b[39m]:\n\u001b[32m     80\u001b[39m     \u001b[38;5;66;03m# Setup for async execution in sync context\u001b[39;00m\n\u001b[32m     81\u001b[39m     loop = asyncio.get_event_loop()\n\u001b[32m---> \u001b[39m\u001b[32m82\u001b[39m     responses = \u001b[43mloop\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun_until_complete\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_get_response_async\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprompts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msystem_prompts\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     83\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m responses\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\tzehl\\Documents\\programming\\promptolution\\.venv\\Lib\\site-packages\\nest_asyncio.py:92\u001b[39m, in \u001b[36m_patch_loop.<locals>.run_until_complete\u001b[39m\u001b[34m(self, future)\u001b[39m\n\u001b[32m     90\u001b[39m     f._log_destroy_pending = \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[32m     91\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m f.done():\n\u001b[32m---> \u001b[39m\u001b[32m92\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_run_once\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     93\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._stopping:\n\u001b[32m     94\u001b[39m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\tzehl\\Documents\\programming\\promptolution\\.venv\\Lib\\site-packages\\nest_asyncio.py:115\u001b[39m, in \u001b[36m_patch_loop.<locals>._run_once\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    108\u001b[39m     heappop(scheduled)\n\u001b[32m    110\u001b[39m timeout = (\n\u001b[32m    111\u001b[39m     \u001b[32m0\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m ready \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._stopping\n\u001b[32m    112\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mmin\u001b[39m(\u001b[38;5;28mmax\u001b[39m(\n\u001b[32m    113\u001b[39m         scheduled[\u001b[32m0\u001b[39m]._when - \u001b[38;5;28mself\u001b[39m.time(), \u001b[32m0\u001b[39m), \u001b[32m86400\u001b[39m) \u001b[38;5;28;01mif\u001b[39;00m scheduled\n\u001b[32m    114\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[32m--> \u001b[39m\u001b[32m115\u001b[39m event_list = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_selector\u001b[49m\u001b[43m.\u001b[49m\u001b[43mselect\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    116\u001b[39m \u001b[38;5;28mself\u001b[39m._process_events(event_list)\n\u001b[32m    118\u001b[39m end_time = \u001b[38;5;28mself\u001b[39m.time() + \u001b[38;5;28mself\u001b[39m._clock_resolution\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\selectors.py:323\u001b[39m, in \u001b[36mSelectSelector.select\u001b[39m\u001b[34m(self, timeout)\u001b[39m\n\u001b[32m    321\u001b[39m ready = []\n\u001b[32m    322\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m323\u001b[39m     r, w, _ = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_select\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_readers\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_writers\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    324\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mInterruptedError\u001b[39;00m:\n\u001b[32m    325\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m ready\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\selectors.py:314\u001b[39m, in \u001b[36mSelectSelector._select\u001b[39m\u001b[34m(self, r, w, _, timeout)\u001b[39m\n\u001b[32m    313\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_select\u001b[39m(\u001b[38;5;28mself\u001b[39m, r, w, _, timeout=\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[32m--> \u001b[39m\u001b[32m314\u001b[39m     r, w, x = \u001b[43mselect\u001b[49m\u001b[43m.\u001b[49m\u001b[43mselect\u001b[49m\u001b[43m(\u001b[49m\u001b[43mr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mw\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mw\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    315\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m r, w + x, []\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "prompts = run_experiment(df, config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from promptolution import get_llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "OpenAIError",
     "evalue": "The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mOpenAIError\u001b[39m                               Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[12]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43mget_llm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\Documents\\programming\\promptolution\\promptolution\\llms\\__init__.py:41\u001b[39m, in \u001b[36mget_llm\u001b[39m\u001b[34m(model_id, config)\u001b[39m\n\u001b[32m     38\u001b[39m     model_id = \u001b[33m\"\u001b[39m\u001b[33m-\u001b[39m\u001b[33m\"\u001b[39m.join(model_id.split(\u001b[33m\"\u001b[39m\u001b[33m-\u001b[39m\u001b[33m\"\u001b[39m)[\u001b[32m1\u001b[39m:])\n\u001b[32m     39\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m VLLM(model_id, config=config)\n\u001b[32m---> \u001b[39m\u001b[32m41\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mAPILLM\u001b[49m\u001b[43m(\u001b[49m\u001b[43mllm\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmodel_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\Documents\\programming\\promptolution\\promptolution\\llms\\api_llm.py:72\u001b[39m, in \u001b[36mAPILLM.__init__\u001b[39m\u001b[34m(self, api_url, llm, token, max_concurrent_calls, max_tokens, config)\u001b[39m\n\u001b[32m     69\u001b[39m \u001b[38;5;28mself\u001b[39m.max_tokens = max_tokens\n\u001b[32m     71\u001b[39m \u001b[38;5;28msuper\u001b[39m().\u001b[34m__init__\u001b[39m(config=config)\n\u001b[32m---> \u001b[39m\u001b[32m72\u001b[39m \u001b[38;5;28mself\u001b[39m.client = \u001b[43mAsyncOpenAI\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbase_url\u001b[49m\u001b[43m=\u001b[49m\u001b[43mapi_url\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mapi_key\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     73\u001b[39m \u001b[38;5;28mself\u001b[39m.semaphore = asyncio.Semaphore(max_concurrent_calls)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\tzehl\\Documents\\programming\\promptolution\\.venv\\Lib\\site-packages\\openai\\_client.py:349\u001b[39m, in \u001b[36mAsyncOpenAI.__init__\u001b[39m\u001b[34m(self, api_key, organization, project, base_url, websocket_base_url, timeout, max_retries, default_headers, default_query, http_client, _strict_response_validation)\u001b[39m\n\u001b[32m    347\u001b[39m     api_key = os.environ.get(\u001b[33m\"\u001b[39m\u001b[33mOPENAI_API_KEY\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    348\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m api_key \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m349\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m OpenAIError(\n\u001b[32m    350\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mThe api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    351\u001b[39m     )\n\u001b[32m    352\u001b[39m \u001b[38;5;28mself\u001b[39m.api_key = api_key\n\u001b[32m    354\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m organization \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[31mOpenAIError\u001b[39m: The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable"
     ]
    }
   ],
   "source": [
    "get_llm(config=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompts"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
