{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Getting Started with Promptolution\n",
    "\n",
    "## Welcome to Promptolution! \n",
    "\n",
    "Discover a powerful tool for evolving and optimizing your LLM prompts. This notebook provides a friendly introduction to Promptolution's core functionality.\n",
    "\n",
    "We're excited to have you try Promptolution - let's get started!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Installation\n",
    "Install Promptolution with a single command"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: promptolution in c:\\users\\tzehl\\documents\\programming\\promptolution\\.venv\\lib\\site-packages (1.3.2)\n",
      "Requirement already satisfied: fastparquet<2025.0.0,>=2024.11.0 in c:\\users\\tzehl\\documents\\programming\\promptolution\\.venv\\lib\\site-packages (from promptolution) (2024.11.0)\n",
      "Requirement already satisfied: numpy<2.0.0,>=1.26.0 in c:\\users\\tzehl\\documents\\programming\\promptolution\\.venv\\lib\\site-packages (from promptolution) (1.26.4)\n",
      "Requirement already satisfied: pandas<3.0.0,>=2.2.2 in c:\\users\\tzehl\\documents\\programming\\promptolution\\.venv\\lib\\site-packages (from promptolution) (2.2.3)\n",
      "Requirement already satisfied: scikit-learn<2.0.0,>=1.5.2 in c:\\users\\tzehl\\documents\\programming\\promptolution\\.venv\\lib\\site-packages (from promptolution) (1.6.1)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.66.5 in c:\\users\\tzehl\\documents\\programming\\promptolution\\.venv\\lib\\site-packages (from promptolution) (4.67.1)\n",
      "Requirement already satisfied: cramjam>=2.3 in c:\\users\\tzehl\\documents\\programming\\promptolution\\.venv\\lib\\site-packages (from fastparquet<2025.0.0,>=2024.11.0->promptolution) (2.10.0)\n",
      "Requirement already satisfied: fsspec in c:\\users\\tzehl\\documents\\programming\\promptolution\\.venv\\lib\\site-packages (from fastparquet<2025.0.0,>=2024.11.0->promptolution) (2025.3.2)\n",
      "Requirement already satisfied: packaging in c:\\users\\tzehl\\documents\\programming\\promptolution\\.venv\\lib\\site-packages (from fastparquet<2025.0.0,>=2024.11.0->promptolution) (24.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\tzehl\\documents\\programming\\promptolution\\.venv\\lib\\site-packages (from pandas<3.0.0,>=2.2.2->promptolution) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\tzehl\\documents\\programming\\promptolution\\.venv\\lib\\site-packages (from pandas<3.0.0,>=2.2.2->promptolution) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\tzehl\\documents\\programming\\promptolution\\.venv\\lib\\site-packages (from pandas<3.0.0,>=2.2.2->promptolution) (2025.2)\n",
      "Requirement already satisfied: scipy>=1.6.0 in c:\\users\\tzehl\\documents\\programming\\promptolution\\.venv\\lib\\site-packages (from scikit-learn<2.0.0,>=1.5.2->promptolution) (1.15.2)\n",
      "Requirement already satisfied: joblib>=1.2.0 in c:\\users\\tzehl\\documents\\programming\\promptolution\\.venv\\lib\\site-packages (from scikit-learn<2.0.0,>=1.5.2->promptolution) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in c:\\users\\tzehl\\documents\\programming\\promptolution\\.venv\\lib\\site-packages (from scikit-learn<2.0.0,>=1.5.2->promptolution) (3.6.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\tzehl\\documents\\programming\\promptolution\\.venv\\lib\\site-packages (from tqdm<5.0.0,>=4.66.5->promptolution) (0.4.6)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\tzehl\\documents\\programming\\promptolution\\.venv\\lib\\site-packages (from python-dateutil>=2.8.2->pandas<3.0.0,>=2.2.2->promptolution) (1.17.0)\n"
     ]
    }
   ],
   "source": [
    "! pip install promptolution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from promptolution import ExperimentConfig, run_experiment\n",
    "import nest_asyncio\n",
    "nest_asyncio.apply() # Required for notebook environments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting Up Your Experiment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below, we're using a subsample of the subjectivity dataset from Hugging Face as an example. When using your own dataset, simply ensure you name the input column \"x\" and the target column \"y\", and provide a brief description of your task, that will parsed to the meta-llm during optimization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"hf://datasets/tasksource/subjectivity/train.csv\").sample(400)\n",
    "df = df.rename(columns={\"Sentence\": \"x\", \"Label\": \"y\"})\n",
    "df = df.replace({\"OBJ\": \"objective\", \"SUBJ\": \"subjective\"})\n",
    "\n",
    "task_description = \"The dataset contains sentences labeled as either subjective or objective. \"\\\n",
    "        \"The task is to classify each sentence as either subjective or objective. \" \\\n",
    "        \"The class mentioned first in the response of the LLM will be the prediction.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating Inital Prompts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We've defined some starter prompts below, but feel free to experiment! You might also want to explore create_prompts_from_samples to automatically generate initial prompts based on your data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "init_prompts = [\n",
    "    'Classify the given text as either an objective or subjective statement based on the tone and language used: e.g. the tone and language used should indicate whether the statement is a neutral, factual summary (objective) or an expression of opinion or emotional tone (subjective). Include the output classes \"objective\" or \"subjective\" in the prompt.',\n",
    "    'What kind of statement is the following text: [Insert text here]? Is it <objective_statement> or <subjective_statement>?',\n",
    "    'Identify whether a sentence is objective or subjective by analyzing the tone, language, and underlying perspective. Consider the emotion, opinion, and bias present in the sentence. Are the authors presenting objective facts or expressing a personal point of view? The output will be either \"objective\" (output class: objective) or \"subjective\" (output class: subjective).',\n",
    "    'Classify the following sentences as either objective or subjective, indicating the name of the output classes: [input sentence]. Output classes: objective, subjective',\n",
    "    '_query a text about legal or corporate-related issues, and predict whether the tone is objective or subjective, outputting the corresponding class \"objective\" for non-subjective language or \"subjective\" for subjective language_',\n",
    "    'Classify a statement as either \"subjective\" or \"objective\" based on whether it reflects a personal opinion or a verifiable fact. The output classes to include are \"objective\" and \"subjective\".',\n",
    "    'Classify the text as objective or subjective based on its tone and language.',\n",
    "    'Classify the text as objective or subjective based on the presence of opinions or facts. Output classes: objective, subjective.',\n",
    "    'Classify the given text as objective or subjective based on its tone, focusing on its intention, purpose, and level of personal opinion or emotional appeal, with outputs including classes such as objective or subjective.',\n",
    "    \"Categorize the text as either objective or subjective, considering whether it presents neutral information or expresses a personal opinion/bias.\\n\\nObjective: The text has a neutral tone and presents factual information about the actions of Democrats in Congress and the union's negotiations.\\n\\nSubjective: The text has a evaluative tone and expresses a positive/negative opinion/evaluation about the past performance of the country.\",\n",
    "    'Given a sentence, classify it as either \"objective\" or \"subjective\" based on its tone and language, considering the presence of third-person pronouns, neutral language, and opinions. Classify the output as \"objective\" if the tone is neutral and detached, focusing on facts and data, or as \"subjective\" if the tone is evaluative, emotive, or biased.',\n",
    "    'Identify whether the given sentence is subjective or objective, then correspondingly output \"objective\" or \"subjective\" in the form of \"<output class>, (e.g. \"objective\"), without quotes. Please note that the subjective orientation typically describes a sentence where the writer expresses their own opinion or attitude, whereas an objective sentence presents facts or information without personal involvement or bias. <output classes: subjective, objective>'\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configure Your LLM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Promptolution offers three flexible ways to access language models:\n",
    "\n",
    "1. Local LLMs (using the Transformers library)\n",
    "1. vLLM backend (for efficient serving of large language models)\n",
    "1. API-based LLMs (compatible with any provider following the OpenAI standard)\n",
    "\n",
    "For this demonstration, we'll use the DeepInfra API, but you can easily switch to other providers like Anthropic or OpenAI by simply changing the base_url and llm string in the configuration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "token = \"YOUR_API_KEY\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's an explanation of each configuration parameter in the ExperimentConfig:\n",
    "\n",
    "- `task_description`: A string describing the task you're optimizing prompts for. This is used to provide the meta-llm with context about your task.\n",
    "- `prompts`: A list of initial prompt strings that will be used as the starting point for optimization.\n",
    "- `n_steps`: The number of optimization steps to run. Higher values allow more exploration and refinement but require more API calls and computational resources.\n",
    "- `optimizer`: The algorithm used for prompt optimization. Currently we support \"evopromptga\", \"evopromptde\", and \"opro\".\n",
    "- `api_url`: The API endpoint URL used to access the language model. This example uses DeepInfra's API which follows the OpenAI standard.\n",
    "- `llm`: The LLM to use for the experiment, as both downstream and meta LLM.\n",
    "- `token`: Your API authentication token required to access the language model service."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = ExperimentConfig(\n",
    "    task_description=task_description,\n",
    "    prompts=init_prompts,\n",
    "    n_steps=10,\n",
    "    optimizer=\"evopromptga\",\n",
    "    api_url=\"https://api.deepinfra.com/v1/openai\",\n",
    "    llm=\"meta-llama/Meta-Llama-3-8B-Instruct\",\n",
    "    token=token,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run Your Experiment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With everything configured, you're ready to evolve your prompts:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompts = run_experiment(df, config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>prompt</th>\n",
       "      <th>score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Determine the textual authenticity of the given sentences by distinguishing objective, factual information from subjective opinions, biases, and personal perspectives. Provide the predicted classification as the primary class label extracted within &lt;final_answer&gt; and &lt;/final_answer&gt;, with a classification of either 'objective' or 'subjective'</td>\n",
       "      <td>0.6875</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Classify the sentences as objective (empirical evidence) or subjective (personal opinions), taking into account potential biases and factual information while considering the context, and return the classification as 'objective' or 'subjective' within &lt;final_answer&gt; and &lt;/final_answer&gt;.</td>\n",
       "      <td>0.6125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Determine the sentiment of the given sentences by categorizing them as either objective or subjective, considering the presence of personal opinions, biases, or factual information. Output the predicted classification (objective or subjective) as the first class label extracted within the markers &lt;final_answer&gt; and &lt;/final_answer&gt;.</td>\n",
       "      <td>0.6125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Determine the objective or subjective nature of the input sentences by examining the presence of opinion, bias, or factual information and classify them accordingly, outputting the predicted classification as either 'objective' or 'subjective' with the corresponding label extracted from the text and marked as &lt;final_answer&gt; and &lt;/final_answer&gt;, providing accurate and concise results.</td>\n",
       "      <td>0.5875</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Classify each sentence as either objective, conveying verifiable facts, or subjective, expressing personal opinions or perspectives, and output the predicted classification within the markers &lt;final_answer&gt; and &lt;/final_answer&gt;, ensuring accurate identification of factual and opinion-based statements.</td>\n",
       "      <td>0.5875</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Consider the input sentences' neutrality, fact-based content, and writer's perspective to accurately classify them as either 'objective' or 'subjective', providing the predicted classification within &lt;final_answer&gt; and &lt;/final_answer&gt;</td>\n",
       "      <td>0.5750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Accurately categorize the sentences as objective or subjective, accounting for biases, factual data, and linguistic features, and provide the predicted classification within &lt;final_answer&gt; and &lt;/final_answer&gt;, indicating 'objective' or 'subjective'</td>\n",
       "      <td>0.5625</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Classify each sentence as objective (facts) or subjective (opinions) and output the predicted classification within &lt;final_answer&gt; and &lt;/final_answer&gt;.</td>\n",
       "      <td>0.5375</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Determine the objective or subjective nature of the input sentences by examining the presence of opinion, bias, or factual information, and output the predicted classification with the extracted label between &lt;final_answer&gt; and &lt;/final_answer&gt;, labeling it as either 'objective' or 'subjective'</td>\n",
       "      <td>0.5125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Determine the sentiment of the provided sentence, categorizing it as objective (factual) or subjective (personal), and report your classification within &lt;final_answer&gt; and &lt;/final_answer&gt;, stating either 'objective' or 'subjective'.</td>\n",
       "      <td>0.4875</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Classify the provided sentences as either objective, containing verifiable facts, or subjective, featuring personal opinions or biases, and extract the classification as the first response within the markers &lt;final_answer&gt; &lt;/final_answer&gt;.</td>\n",
       "      <td>0.4500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Determine the type of each sentence as objective (facts) or subjective (opinions), considering potential biases and factual information, and provide your predicted classification within &lt;final_answer&gt; and &lt;/final_answer&gt;.</td>\n",
       "      <td>0.4250</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                                                                                                                                                                                                                                                                                prompt  \\\n",
       "2                                             Determine the textual authenticity of the given sentences by distinguishing objective, factual information from subjective opinions, biases, and personal perspectives. Provide the predicted classification as the primary class label extracted within <final_answer> and </final_answer>, with a classification of either 'objective' or 'subjective'   \n",
       "5                                                                                                      Classify the sentences as objective (empirical evidence) or subjective (personal opinions), taking into account potential biases and factual information while considering the context, and return the classification as 'objective' or 'subjective' within <final_answer> and </final_answer>.   \n",
       "10                                                       Determine the sentiment of the given sentences by categorizing them as either objective or subjective, considering the presence of personal opinions, biases, or factual information. Output the predicted classification (objective or subjective) as the first class label extracted within the markers <final_answer> and </final_answer>.   \n",
       "4   Determine the objective or subjective nature of the input sentences by examining the presence of opinion, bias, or factual information and classify them accordingly, outputting the predicted classification as either 'objective' or 'subjective' with the corresponding label extracted from the text and marked as <final_answer> and </final_answer>, providing accurate and concise results.   \n",
       "6                                                                                        Classify each sentence as either objective, conveying verifiable facts, or subjective, expressing personal opinions or perspectives, and output the predicted classification within the markers <final_answer> and </final_answer>, ensuring accurate identification of factual and opinion-based statements.   \n",
       "11                                                                                                                                                          Consider the input sentences' neutrality, fact-based content, and writer's perspective to accurately classify them as either 'objective' or 'subjective', providing the predicted classification within <final_answer> and </final_answer>   \n",
       "7                                                                                                                                             Accurately categorize the sentences as objective or subjective, accounting for biases, factual data, and linguistic features, and provide the predicted classification within <final_answer> and </final_answer>, indicating 'objective' or 'subjective'   \n",
       "1                                                                                                                                                                                                                                              Classify each sentence as objective (facts) or subjective (opinions) and output the predicted classification within <final_answer> and </final_answer>.   \n",
       "3                                                                                               Determine the objective or subjective nature of the input sentences by examining the presence of opinion, bias, or factual information, and output the predicted classification with the extracted label between <final_answer> and </final_answer>, labeling it as either 'objective' or 'subjective'   \n",
       "9                                                                                                                                                             Determine the sentiment of the provided sentence, categorizing it as objective (factual) or subjective (personal), and report your classification within <final_answer> and </final_answer>, stating either 'objective' or 'subjective'.   \n",
       "0                                                                                                                                                      Classify the provided sentences as either objective, containing verifiable facts, or subjective, featuring personal opinions or biases, and extract the classification as the first response within the markers <final_answer> </final_answer>.   \n",
       "8                                                                                                                                                                        Determine the type of each sentence as objective (facts) or subjective (opinions), considering potential biases and factual information, and provide your predicted classification within <final_answer> and </final_answer>.   \n",
       "\n",
       "     score  \n",
       "2   0.6875  \n",
       "5   0.6125  \n",
       "10  0.6125  \n",
       "4   0.5875  \n",
       "6   0.5875  \n",
       "11  0.5750  \n",
       "7   0.5625  \n",
       "1   0.5375  \n",
       "3   0.5125  \n",
       "9   0.4875  \n",
       "0   0.4500  \n",
       "8   0.4250  "
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, most optimized prompts are semantically very similar, however they often differ heavily in performance. This is exactly what we observed in our experiments across various LLMs and datasets. Running prompt optimization is an easy way to gain significant performance improvements on your task for free!\n",
    "\n",
    "If you run into any issues while using Promptolution, please feel free to contact us. We're also happy to receive support through pull requests and other contributions to the project.\n",
    "\n",
    "\n",
    "Happy prompt optimizing! ðŸš€âœ¨ We can't wait to see what you build with Promptolution! ðŸ¤–ðŸ’¡"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
