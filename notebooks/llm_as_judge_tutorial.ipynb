{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Getting Started with Promptolution\n",
    "\n",
    "## Welcome to Promptolution! \n",
    "\n",
    "Discover a powerful tool for evolving and optimizing your LLM prompts. This notebook provides a friendly introduction to Promptolution's core functionality.\n",
    "\n",
    "We're excited to have you try Promptolution - let's get started!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Installation\n",
    "Install Promptolution with a single command"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! pip install promptolution[api]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from promptolution.utils import ExperimentConfig\n",
    "from promptolution.helpers import run_experiment\n",
    "import nest_asyncio\n",
    "\n",
    "nest_asyncio.apply()  # Required for notebook environments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting Up Your Experiment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below, we're using a subsample of the subjectivity dataset from Hugging Face as an example. When using your own dataset, simply ensure you name the input column \"x\" and the target column \"y\", and provide a brief description of your task, that will parsed to the meta-llm during optimization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(\n",
    "    {\n",
    "        \"x\": [str(i) for i in range(30)],\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating Inital Prompts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We've defined some starter prompts below, but feel free to experiment! You might also want to explore create_prompts_from_samples to automatically generate initial prompts based on your data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "init_prompts = [\n",
    "    \"Tell the user a joke, that is novel and funny.\",\n",
    "    \"Create a humorous joke for the user.\",\n",
    "    \"Generate a joke that will make the user laugh.\",\n",
    "    \"Craft a joke that is both funny and original.\",\n",
    "    \"Invent a joke that is amusing and unique.\",\n",
    "    \"Tell a joke to the user, that is unexpected and humorous.\",\n",
    "    \"Devise a joke that is entertaining and fresh.\",\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configure Your LLM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Promptolution offers three flexible ways to access language models:\n",
    "\n",
    "1. Local LLMs (using the Transformers library)\n",
    "1. vLLM backend (for efficient serving of large language models)\n",
    "1. API-based LLMs (compatible with any provider following the OpenAI standard)\n",
    "\n",
    "For this demonstration, we'll use the DeepInfra API, but you can easily switch to other providers like Anthropic or OpenAI by simply changing the base_url and llm string in the configuration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "api_key = \"YOUR_API_KEY\"  # Replace with your Promptolution API key\n",
    "\n",
    "with open(\"../deepinfratoken.txt\", \"r\") as f:\n",
    "    api_key = f.read().strip()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's an explanation of each configuration parameter in the ExperimentConfig:\n",
    "- `optimizer`: The algorithm used for prompt optimization. Currently we support \"capo\", \"evopromptga\", \"evopromptde\", and \"opro\". For this example, we use \"capo\" as it is capable of leveraging few-shot examples.\n",
    "- `task_description`: A string describing the task you're optimizing prompts for. This is used to provide the meta-llm with context about your task.\n",
    "- `prompts`: A list of initial prompt strings that will be used as the starting point for optimization.\n",
    "- `n_steps`: The number of optimization steps to run. Higher values allow more exploration and refinement but require more API calls and computational resources.\n",
    "- `api_url`: The API endpoint URL used to access the language model. This example uses DeepInfra's API which follows the OpenAI standard.\n",
    "- `llm`: The LLM to use for the experiment, as both downstream and meta LLM.\n",
    "- `token`: Your API authentication token required to access the language model service."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = ExperimentConfig(\n",
    "    optimizer=\"evopromptga\",\n",
    "    task_description=\"Tell the user a joke!\",\n",
    "    prompts=init_prompts,\n",
    "    n_steps=3,\n",
    "    n_subsamples=3,\n",
    "    subsample_strategy=\"random_subsample\",\n",
    "    api_url=\"https://api.deepinfra.com/v1/openai\",\n",
    "    model_id=\"meta-llama/Meta-Llama-3-8B-Instruct\",\n",
    "    api_key=api_key,\n",
    "    task_type=\"judge\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run Your Experiment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With everything configured, you're ready to optimize your prompts! The `run_experiment` function will run the optimization and evaluate on a holdout set. You can expect this cell to take a few minutes to run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ðŸ”¥ Starting optimization...\n",
      "Failed to parse score from judge response: .getResponse:\n",
      "\n",
      "<final_answer>+5</final_answer>\n",
      "\n",
      "Reasoning:\n",
      "\n",
      "The response addresses the task correctly by providing a joke as requested. The joke is also relevant to the input, as it is a play on words related to technology (GPS). The response is comprehensive, including the setup, punchline, and explanation of the joke, which adds to its effectiveness and makes it more enjoyable for the user. Overall, the response is a perfect match for the task, hence the score of +5.. Using 0.0 as default.\n",
      "Failed to parse score from judge response: <Evaluation>\n",
      "\n",
      "The response is a joke, and it does attempt to tell the user a joke as per the task. The joke combines two famous scientific concepts, Pavlov's dogs and SchrÃ¶dinger's cat, in a humorous way. The response is creative and clever, and it does bring a smile to the face.\n",
      "\n",
      "However, the response does not explicitly state \"0\" as the input, which is a requirement according to the task. The joke is applicable to all users, not just those who input \"0\".\n",
      "\n",
      "Given these points, I would score the response as follows:\n",
      "\n",
      "<Efinal_score> +2 </final_score>\n",
      "\n",
      "The response partially addresses the task, but misses some important details.. Using 0.0 as default.\n",
      "Failed to parse score from judge response: <final_score>$4.5</final_score>\n",
      "\n",
      "The response successfully addresses the task by telling a joke to the user. The joke is relevant to the input, and the user is provided with a chance to ask for another one. However, the joke is not exactly \"extracted\" between \"<final_answer>\" and \"</final_answer>\" markers as required. Although, the format is not strictly adhered to, the overall task is indeed accomplished.\n",
      "\n",
      "The joke itself is interesting and might appeal to those with a background in physics. But its humor might not be to everyone's taste.\n",
      "\n",
      "The scoring is adjusted based on the close, but not exact, adherence to the task and the potential humor appeal of the joke.. Using 0.0 as default.\n",
      "Failed to parse score from judge response: <final_score>=3</final_score>\n",
      "\n",
      "The response is partially correct, but not fully. The task asks for a joke, and the response does provide one. However, the task also specifies that the class label (in this case, the punchline \"because it was looking for a neural connection... but it kept getting stuck in a feedback loop and couldn't get past the small talk!\") should be extracted between `<final_answer>` and `</final_answer>` markers, which are not present in the response. Additionally, the response includes extra information such as the words \"i'm glad you're ready for a laugh!\", \"here's a brand new joke i just concocted:\", and \"hope that brings a smile to your face!\", which are not strictly necessary for the task.\n",
      "\n",
      "Overall, while the response is somewhat correct, it does not fully address the task's requirements, and thus receives a score of +3.. Using 0.0 as default.\n",
      "Failed to parse score from judge response: </final_score> +3</final_score>\n",
      "\n",
      "The response attempts to address the task by providing a joke, which is relevant to the input. However, the response is not fully correct and appropriate for the following reasons:\n",
      "\n",
      "1. The input \"24\" is not related to the joke told, and a joke has not been tailored specifically for the input value. A good response should take into account the input and address it in some way.\n",
      "2. While the joke is a classic one, it is not explicitly marked with the requested class label (<final_answer> and </final_answer>). To fully address the task, the response should include the class label.\n",
      "\n",
      "Overall, the response partially addresses the task, but with some room for improvement.. Using 0.0 as default.\n",
      "Failed to parse score from judge response: (final_score: +5)\n",
      "\n",
      "This response is fully correct and appropriate for the task. The class label \"25\" is extracted from the input and used as the punchline for a joke, which is the format expected in this task. The joke itself is also clever and relevant to the input number. Additionally, the response is concise and well-structured. Overall, the response addresses the task fully and correctly.. Using 0.0 as default.\n",
      "Failed to parse score from judge response: <final_score>4+</final_score>\n",
      "\n",
      "The response is partially correct in that it provides a joke, which was the task requirement. However, the task specifically asked for a joke with a class label between the markers <final_answer> and </final_answer>. The response does not include these markers, which is a critical aspect of the task. Nevertheless, the joke itself is relevant and appropriate for the task, and the user will likely find it entertaining. If the class label were included, I would have given a score of +5, as the response would have fully addressed the task requirements.. Using 0.0 as default.\n",
      "Failed to parse score from judge response: <label>I think the response was fully correct and appropriate. It addressed the task by providing a joke as requested. The tone was lighthearted and inviting, which is suitable for a joke. The use of a wait for it and the invited feedback of how it was received adds to the playful and engaging nature of the response. The response does not have any significant errors or omissions. By the task of telling a joke and the given criteria, I would give a score of +5.&#x3C;final\\_score&#x3E;+5&#x3C;/final\\_score&#x3E;. Using 0.0 as default.\n",
      "Failed to parse score from judge response:  FINAL REST\n",
      "\n",
      "The response effectively addresses the input task of telling the user a joke. The respondent provides a lighthearted and entertaining joke with a clear setup and punchline, making it easily understandable and enjoyable. The silly pun on \"two-tired\" is clever and cleverly.. Using 0.0 as default.\n",
      "Failed to parse score from judge response: <script> newsletter-mailing.push({name: ' Evaluator', email: 'evaluator@system'}});. Using 0.0 as default.\n",
      "Failed to parse score from judge response:  fungi/. Using 0.0 as default.\n",
      "Failed to parse score from judge response: (final_score>+5</final_score>)\n",
      "\n",
      "The response is not only correct and relevant to the task, but it also exceeds expectations by including a humorous element, a personal touch, and an attempt to engage the user. The joke is delivered as requested, with the class label (\"joke!\") implied through the tone and content of the response. Overall, the response is fully correct and appropriate, making it deserving of a score of +5.. Using 0.0 as default.\n",
      "Failed to parse score from judge response: I'd like to evaluate the provided response.. Using 0.0 as default.\n",
      "Failed to parse score from judge response: <fial_score>+5</final_score>\n",
      "\n",
      "The response is a joke, which is exactly what the task asks for. The joke is original and cleverly worded, and the user is given a chance to anticipate the punchline before it is delivered. The response is also well-written, with a friendly tone and a pleasure at sharing the joke. The use of the phrase \"you're sure to bring a grin\" sets the right tone for the joke.\n",
      "\n",
      "The only thing that could make the response more perfect is adding a way for the user to know their answer was correct or not. Since the task asks for a joke, an evaluation component is not necessary but it would polish the interaction.\n",
      "\n",
      "Overall, the response is fully correct and appropriate, and scores a +5.. Using 0.0 as default.\n",
      "Failed to parse score from judge response: </final_answer>+5</final_answer>\n",
      "\n",
      "The response fully addresses the task by providing a joke as requested. It also includes a clear setup and punchline, and ends with a lighthearted and playful tone. The joke is also relevant and applicable to the input \"2\" (the number of the class), as it is a classic example of a clever and humorous joke.Overall, the response is fully correct and appropriate, earning a score of +5.. Using 0.0 as default.\n",
      "Failed to parse score from judge response: (final_score)= +5\n",
      "\n",
      "The response is entirely correct and appropriate. It tells a joke to the user in response to the input of \"8\", which is a numerical cue to share a joke. The joke is clever, well-crafted, and relates well to the input. There is no lack of effort or consideration shown in this response, and it effectively addresses the task as requested.. Using 0.0 as default.\n",
      "Failed to parse score from judge response: <fimal_score>+5</final_score>\n",
      "\n",
      "The response is a perfect match for the task. The input is \"28\", which is seemingly unrelated to the joke, but the response is a joke, which is exactly what the task asked for. The joke is well-crafted, and the user is even asked to wait for it, which adds to the humor. The attempted setup and punchline are clever, and the joke is complete. There's no evidence of incorrect or irrelevant information, which makes this response fully correct and appropriate.. Using 0.0 as default.\n",
      "Failed to parse score from judge response: <Evaluated Score: +5>\n",
      "\n",
      "The response clearly addresses the input task of telling the user a joke. The joke is presented in a humorous and engaging manner, with a clear setup and punchline. The joke is also relatable and accessible, making it suitable for a wide range of audiences.\n",
      "\n",
      "The class label extracted between the markers <final_answer> and </final_answer> is \"joke\", which is the correct class label for the response.\n",
      "\n",
      "The response is fully correct and appropriate, hence the score of +5.. Using 0.0 as default.\n",
      "Ã°Å¸â€œÅ  Starting evaluation...\n",
      "Failed to parse score from judge response: <final_score>=+5</final_score>\n",
      "\n",
      "The response provides an accurate and relevant joke, exactly addressing the task of telling the user a joke. The use of the humorously crafted pun \"outstanding in his field\" (typical phrase for an award) and its unexpected twist as an accountant's skill is a classic and enjoyable joke. The response is concise and well-structured, providing a buildup and delivery of the punchline. Overall, the response is fully correct, relevant, and complete, achieving a perfect score.. Using 0.0 as default.\n",
      "Failed to parse score from judge response: <fimal_score> +5 </final_score>\n",
      "\n",
      "The response directly addresses the task of telling the user a joke! The joke is a creative and humorous response that is relevant to the input (14, which could be a joke number, but no specific context is provided). The response is also complete, providing a clear setup, punchline, and even a funny add-on with the \"intergalactic acting awards\".\n",
      "\n",
      "The extracted class label is \"<final_answer> because he needed space... to rehearse his free fall of love monologue for the intergalactic acting awards! </final_answer>\", which is accurate and relevant.\n",
      "\n",
      "Overall, the response is exactly what was asked for, making it a +5 score.. Using 0.0 as default.\n"
     ]
    }
   ],
   "source": [
    "prompts = run_experiment(df, config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>prompt</th>\n",
       "      <th>score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Generate a joke that will make the user laugh.</td>\n",
       "      <td>5.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Conceive a fresh, humorous joke that will have a positive impact on the user's mood.</td>\n",
       "      <td>4.666667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Tell an unorthodox yet hilarious joke that will surprise and entertain the user.</td>\n",
       "      <td>4.666667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Create an unexpected yet comedic joke that will bring a smile to the user's face.</td>\n",
       "      <td>4.666667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Craft a witty and unexpected joke to amuse the audience.</td>\n",
       "      <td>4.166667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Surprise the user with a witty, side-splitting joke that will leave them in stitches!</td>\n",
       "      <td>4.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Tell a joke to the user, that is unexpected and humorous.</td>\n",
       "      <td>3.833333</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                  prompt  \\\n",
       "0                                         Generate a joke that will make the user laugh.   \n",
       "1   Conceive a fresh, humorous joke that will have a positive impact on the user's mood.   \n",
       "2       Tell an unorthodox yet hilarious joke that will surprise and entertain the user.   \n",
       "3      Create an unexpected yet comedic joke that will bring a smile to the user's face.   \n",
       "4                               Craft a witty and unexpected joke to amuse the audience.   \n",
       "5  Surprise the user with a witty, side-splitting joke that will leave them in stitches!   \n",
       "6                              Tell a joke to the user, that is unexpected and humorous.   \n",
       "\n",
       "      score  \n",
       "0  5.000000  \n",
       "1  4.666667  \n",
       "2  4.666667  \n",
       "3  4.666667  \n",
       "4  4.166667  \n",
       "5  4.000000  \n",
       "6  3.833333  "
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.set_option(\"display.max_colwidth\", None)  # Show full text in DataFrame\n",
    "prompts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, most optimized prompts are semantically very similar, however they often differ heavily in performance. This is exactly what we observed in our experiments across various LLMs and datasets. Running prompt optimization is an easy way to gain significant performance improvements on your task for free!\n",
    "\n",
    "If you run into any issues while using Promptolution, please feel free to contact us. We're also happy to receive support through pull requests and other contributions to the project.\n",
    "\n",
    "\n",
    "Happy prompt optimizing! ðŸš€âœ¨ We can't wait to see what you build with Promptolution! ðŸ¤–ðŸ’¡"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "promptolution-py3.12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
