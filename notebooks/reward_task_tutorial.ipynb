{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Getting Started with Promptolution\n",
    "\n",
    "## Welcome to Promptolution! \n",
    "\n",
    "Discover a powerful tool for evolving and optimizing your LLM prompts. This notebook provides a friendly introduction to Promptolution's core functionality.\n",
    "\n",
    "We're excited to have you try Promptolution - let's get started!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Installation\n",
    "Install Promptolution with a single command"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! pip install promptolution[api]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from promptolution.utils import ExperimentConfig\n",
    "from promptolution.helpers import run_optimization\n",
    "import nest_asyncio\n",
    "\n",
    "nest_asyncio.apply()  # Required for notebook environments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting Up Your Experiment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below, we're using a subsample of the subjectivity dataset from Hugging Face as an example. When using your own dataset, simply ensure you name the input column \"x\" and the target column \"y\", and provide a brief description of your task, that will parsed to the meta-llm during optimization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame({\"x\": [\"\"]})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating Inital Prompts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We've defined some starter prompts below, but feel free to experiment! You might also want to explore create_prompts_from_samples to automatically generate initial prompts based on your data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "init_prompts = [\"Tell the user a joke, that is novel and funny.\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configure Your LLM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Promptolution offers three flexible ways to access language models:\n",
    "\n",
    "1. Local LLMs (using the Transformers library)\n",
    "1. vLLM backend (for efficient serving of large language models)\n",
    "1. API-based LLMs (compatible with any provider following the OpenAI standard)\n",
    "\n",
    "For this demonstration, we'll use the DeepInfra API, but you can easily switch to other providers like Anthropic or OpenAI by simply changing the base_url and llm string in the configuration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "api_key = \"YOUR_API_KEY\"  # Replace with your Promptolution API key\n",
    "\n",
    "with open(\"../deepinfratoken.txt\", \"r\") as f:\n",
    "    api_key = f.read().strip()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's an explanation of each configuration parameter in the ExperimentConfig:\n",
    "- `optimizer`: The algorithm used for prompt optimization. Currently we support \"capo\", \"evopromptga\", \"evopromptde\", and \"opro\". For this example, we use \"capo\" as it is capable of leveraging few-shot examples.\n",
    "- `task_description`: A string describing the task you're optimizing prompts for. This is used to provide the meta-llm with context about your task.\n",
    "- `prompts`: A list of initial prompt strings that will be used as the starting point for optimization.\n",
    "- `n_steps`: The number of optimization steps to run. Higher values allow more exploration and refinement but require more API calls and computational resources.\n",
    "- `api_url`: The API endpoint URL used to access the language model. This example uses DeepInfra's API which follows the OpenAI standard.\n",
    "- `llm`: The LLM to use for the experiment, as both downstream and meta LLM.\n",
    "- `token`: Your API authentication token required to access the language model service."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reward_function(prediction: str) -> float:\n",
    "    score = input(prediction)\n",
    "    return float(score) / 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = ExperimentConfig(\n",
    "    optimizer=\"opro\",\n",
    "    task_description=\"Tell the user a joke!\",\n",
    "    prompts=init_prompts,\n",
    "    n_steps=5,\n",
    "    num_instructions_per_step=1,\n",
    "    api_url=\"https://api.deepinfra.com/v1/openai\",\n",
    "    model_id=\"meta-llama/Meta-Llama-3-8B-Instruct\",\n",
    "    api_key=api_key,\n",
    "    n_subsamples=3,\n",
    "    task_type=\"reward\",\n",
    "    reward_function=reward_function,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run Your Experiment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With everything configured, you're ready to optimize your prompts! The `run_experiment` function will run the optimization and evaluate on a holdout set. You can expect this cell to take a few minutes to run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "üî• Starting optimization...\n",
      "‚ö†Ô∏è Unused configuration attributes: {'reward_function'}\n"
     ]
    }
   ],
   "source": [
    "prompts = run_optimization(df, config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Tell the user a joke, that is novel and funny.',\n",
       " 'Craft a joke that weaves together two unrelated concepts in a clever and whimsical way, using a hint of absurdity to create a delightful surprise for the user. The joke should be witty, yet easy to comprehend, and have a high chance of eliciting an amused response.',\n",
       " 'Explain a clever and playful joke that leverages wordplay, using a seemingly unrelated topic or unexpected twist to create a delightful surprise for the user, while maintaining clarity and avoiding obscure references.',\n",
       " 'Share a surprising and lighthearted joke that is likely to elicit a chuckle from the user, considering their interests and previous conversations.',\n",
       " 'Capture the essence of a novel and compelling joke by crafting a narrative that cleverly juxtaposes two unexpected concepts, leveraging wordplay, clever misdirection, and a dash of creativity to create an amusing and psychologically satisfying payoff, carefully balancing surprise, clarity, and emotional resonance to elicit a delightful response from the user.',\n",
       " 'Tell the user a joke that subverts their expectations, playing on a common phrase or stereotype in a surprising and humorous way, but still makes sense and is easy to understand.']"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, most optimized prompts are semantically very similar, however they often differ heavily in performance. This is exactly what we observed in our experiments across various LLMs and datasets. Running prompt optimization is an easy way to gain significant performance improvements on your task for free!\n",
    "\n",
    "If you run into any issues while using Promptolution, please feel free to contact us. We're also happy to receive support through pull requests and other contributions to the project.\n",
    "\n",
    "\n",
    "Happy prompt optimizing! üöÄ‚ú® We can't wait to see what you build with Promptolution! ü§ñüí°"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "promptolution-py3.12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
