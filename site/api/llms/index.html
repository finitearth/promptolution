<!DOCTYPE html>
<html lang="en" data-bs-theme="light">
    <head>
        <meta charset="utf-8">
        <meta http-equiv="X-UA-Compatible" content="IE=edge">
        <meta name="viewport" content="width=device-width, initial-scale=1.0">
        
        
        
        <link rel="shortcut icon" href="../../img/favicon.ico">
        <title>LLMs - Promptolution Documentation</title>
        <link href="../../css/bootstrap.min.css" rel="stylesheet">
        <link href="../../css/fontawesome.min.css" rel="stylesheet">
        <link href="../../css/brands.min.css" rel="stylesheet">
        <link href="../../css/solid.min.css" rel="stylesheet">
        <link href="../../css/v4-font-face.min.css" rel="stylesheet">
        <link href="../../css/base.css" rel="stylesheet">
        <link id="hljs-light" rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.8.0/styles/github.min.css" >
        <link id="hljs-dark" rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.8.0/styles/github-dark.min.css" disabled>
        <link href="../../assets/_mkdocstrings.css" rel="stylesheet">
        <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.8.0/highlight.min.js"></script>
        <script>hljs.highlightAll();</script> 
    </head>

    <body>
        <div class="navbar fixed-top navbar-expand-lg navbar-dark bg-primary">
            <div class="container">
                <a class="navbar-brand" href="../..">Promptolution Documentation</a>
                <!-- Expander button -->
                <button type="button" class="navbar-toggler" data-bs-toggle="collapse" data-bs-target="#navbar-collapse" aria-controls="navbar-collapse" aria-expanded="false" aria-label="Toggle navigation">
                    <span class="navbar-toggler-icon"></span>
                </button>

                <!-- Expanded navigation -->
                <div id="navbar-collapse" class="navbar-collapse collapse">
                        <!-- Main navigation -->
                        <ul class="nav navbar-nav">
                            <li class="nav-item">
                                <a href="../.." class="nav-link">Home</a>
                            </li>
                            <li class="nav-item dropdown">
                                <a href="#" class="nav-link dropdown-toggle active" aria-current="page" role="button" data-bs-toggle="dropdown"  aria-expanded="false">API Reference</a>
                                <ul class="dropdown-menu">
                                    
<li>
    <a href="./" class="dropdown-item active" aria-current="page">LLMs</a>
</li>
                                    
<li>
    <a href="../optimizers/" class="dropdown-item">Optimizers</a>
</li>
                                    
<li>
    <a href="../predictors/" class="dropdown-item">Predictors</a>
</li>
                                    
<li>
    <a href="../tasks/" class="dropdown-item">Tasks</a>
</li>
                                    
<li>
    <a href="../callbacks/" class="dropdown-item">Callbacks</a>
</li>
                                    
<li>
    <a href="../config/" class="dropdown-item">Config</a>
</li>
                                </ul>
                            </li>
                        </ul>

                    <ul class="nav navbar-nav ms-md-auto">
                        <li class="nav-item">
                            <a href="#" class="nav-link" data-bs-toggle="modal" data-bs-target="#mkdocs_search_modal">
                                <i class="fa fa-search"></i> Search
                            </a>
                        </li>
                            <li class="nav-item">
                                <a rel="prev" href="../.." class="nav-link">
                                    <i class="fa fa-arrow-left"></i> Previous
                                </a>
                            </li>
                            <li class="nav-item">
                                <a rel="next" href="../optimizers/" class="nav-link">
                                    Next <i class="fa fa-arrow-right"></i>
                                </a>
                            </li>
                    </ul>
                </div>
            </div>
        </div>

        <div class="container">
            <div class="row">
                    <div class="col-md-3"><div class="navbar-expand-md bs-sidebar hidden-print affix" role="complementary">
    <div class="navbar-header">
        <button type="button" class="navbar-toggler collapsed" data-bs-toggle="collapse" data-bs-target="#toc-collapse" title="Table of Contents">
            <span class="fa fa-angle-down"></span>
        </button>
    </div>

    
    <div id="toc-collapse" class="navbar-collapse collapse card bg-body-tertiary">
        <ul class="nav flex-column">
            
            <li class="nav-item" data-bs-level="1"><a href="#llms" class="nav-link">LLMs</a>
              <ul class="nav flex-column">
            <li class="nav-item" data-bs-level="2"><a href="#promptolution.llms" class="nav-link">llms</a>
              <ul class="nav flex-column">
              </ul>
            </li>
            <li class="nav-item" data-bs-level="2"><a href="#promptolution.llms.get_llm" class="nav-link">get_llm</a>
              <ul class="nav flex-column">
              </ul>
            </li>
            <li class="nav-item" data-bs-level="2"><a href="#promptolution.llms.api_llm" class="nav-link">api_llm</a>
              <ul class="nav flex-column">
              </ul>
            </li>
            <li class="nav-item" data-bs-level="2"><a href="#promptolution.llms.base_llm" class="nav-link">base_llm</a>
              <ul class="nav flex-column">
              </ul>
            </li>
            <li class="nav-item" data-bs-level="2"><a href="#promptolution.llms.deepinfra" class="nav-link">deepinfra</a>
              <ul class="nav flex-column">
              </ul>
            </li>
            <li class="nav-item" data-bs-level="2"><a href="#promptolution.llms.local_llm" class="nav-link">local_llm</a>
              <ul class="nav flex-column">
              </ul>
            </li>
            <li class="nav-item" data-bs-level="2"><a href="#api-llm" class="nav-link">API LLM</a>
              <ul class="nav flex-column">
              </ul>
            </li>
            <li class="nav-item" data-bs-level="2"><a href="#promptolution.llms.api_llm" class="nav-link">api_llm</a>
              <ul class="nav flex-column">
              </ul>
            </li>
            <li class="nav-item" data-bs-level="2"><a href="#promptolution.llms.api_llm.APILLM" class="nav-link">APILLM</a>
              <ul class="nav flex-column">
              </ul>
            </li>
            <li class="nav-item" data-bs-level="2"><a href="#promptolution.llms.api_llm.invoke_model" class="nav-link">invoke_model</a>
              <ul class="nav flex-column">
              </ul>
            </li>
            <li class="nav-item" data-bs-level="2"><a href="#base-llm" class="nav-link">Base LLM</a>
              <ul class="nav flex-column">
              </ul>
            </li>
            <li class="nav-item" data-bs-level="2"><a href="#promptolution.llms.base_llm" class="nav-link">base_llm</a>
              <ul class="nav flex-column">
              </ul>
            </li>
            <li class="nav-item" data-bs-level="2"><a href="#promptolution.llms.base_llm.BaseLLM" class="nav-link">BaseLLM</a>
              <ul class="nav flex-column">
              </ul>
            </li>
            <li class="nav-item" data-bs-level="2"><a href="#promptolution.llms.base_llm.DummyLLM" class="nav-link">DummyLLM</a>
              <ul class="nav flex-column">
              </ul>
            </li>
            <li class="nav-item" data-bs-level="2"><a href="#deepinfra-llm" class="nav-link">DeepInfra LLM</a>
              <ul class="nav flex-column">
              </ul>
            </li>
            <li class="nav-item" data-bs-level="2"><a href="#promptolution.llms.deepinfra" class="nav-link">deepinfra</a>
              <ul class="nav flex-column">
              </ul>
            </li>
            <li class="nav-item" data-bs-level="2"><a href="#promptolution.llms.deepinfra.ChatDeepInfra" class="nav-link">ChatDeepInfra</a>
              <ul class="nav flex-column">
              </ul>
            </li>
            <li class="nav-item" data-bs-level="2"><a href="#local-llm" class="nav-link">Local LLM</a>
              <ul class="nav flex-column">
              </ul>
            </li>
            <li class="nav-item" data-bs-level="2"><a href="#promptolution.llms.local_llm" class="nav-link">local_llm</a>
              <ul class="nav flex-column">
              </ul>
            </li>
            <li class="nav-item" data-bs-level="2"><a href="#promptolution.llms.local_llm.LocalLLM" class="nav-link">LocalLLM</a>
              <ul class="nav flex-column">
              </ul>
            </li>
              </ul>
            </li>
        </ul>
    </div>
</div></div>
                    <div class="col-md-9" role="main">

<h1 id="llms">LLMs</h1>
<p>This module contains the LLM (Large Language Model) implementations.</p>


<div class="doc doc-object doc-module">



<a id="promptolution.llms"></a>
    <div class="doc doc-contents first">



  <div class="doc doc-children">









<div class="doc doc-object doc-function">


<h2 id="promptolution.llms.get_llm" class="doc doc-heading">
            <code class="highlight language-python">get_llm(model_id, *args, **kwargs)</code>

</h2>


    <div class="doc doc-contents ">

        <p>Factory function to create and return a language model instance based on the provided model_id.</p>
<p>This function supports three types of language models:
1. DummyLLM: A mock LLM for testing purposes.
2. LocalLLM: For running models locally (identified by 'local' in the model_id).
3. APILLM: For API-based models (default if not matching other types).</p>


<p><span class="doc-section-title">Parameters:</span></p>
    <table>
      <thead>
        <tr>
          <th>Name</th>
          <th>Type</th>
          <th>Description</th>
          <th>Default</th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td><code>model_id</code></td>
            <td>
                  <code>str</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>Identifier for the model to use. Special cases:
            - "dummy" for DummyLLM
            - "local-{model_name}" for LocalLLM
            - Any other string for APILLM</p>
              </div>
            </td>
            <td>
                <em>required</em>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td><code>*args</code></td>
            <td>
            </td>
            <td>
              <div class="doc-md-description">
                <p>Variable length argument list passed to the LLM constructor.</p>
              </div>
            </td>
            <td>
                  <code>()</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td><code>**kwargs</code></td>
            <td>
            </td>
            <td>
              <div class="doc-md-description">
                <p>Arbitrary keyword arguments passed to the LLM constructor.</p>
              </div>
            </td>
            <td>
                  <code>{}</code>
            </td>
          </tr>
      </tbody>
    </table>


    <p><span class="doc-section-title">Returns:</span></p>
    <table>
      <thead>
        <tr>
          <th>Type</th>
          <th>Description</th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td>
            </td>
            <td>
              <div class="doc-md-description">
                <p>An instance of DummyLLM, LocalLLM, or APILLM based on the model_id.</p>
              </div>
            </td>
          </tr>
      </tbody>
    </table>

            <details class="quote">
              <summary>Source code in <code>promptolution\llms\__init__.py</code></summary>
              <pre class="highlight"><code class="language-python">def get_llm(model_id: str, *args, **kwargs):
    """
    Factory function to create and return a language model instance based on the provided model_id.

    This function supports three types of language models:
    1. DummyLLM: A mock LLM for testing purposes.
    2. LocalLLM: For running models locally (identified by 'local' in the model_id).
    3. APILLM: For API-based models (default if not matching other types).

    Args:
        model_id (str): Identifier for the model to use. Special cases:
                        - "dummy" for DummyLLM
                        - "local-{model_name}" for LocalLLM
                        - Any other string for APILLM
        *args: Variable length argument list passed to the LLM constructor.
        **kwargs: Arbitrary keyword arguments passed to the LLM constructor.

    Returns:
        An instance of DummyLLM, LocalLLM, or APILLM based on the model_id.
    """
    if model_id == "dummy":
        return DummyLLM(*args, **kwargs)
    if "local" in model_id:
        model_id = "-".join(model_id.split("-")[1:])
        return LocalLLM(model_id, *args, **kwargs)
    return APILLM(model_id, *args, **kwargs)</code></pre>
            </details>
    </div>

</div>


<div class="doc doc-object doc-module">



<h2 id="promptolution.llms.api_llm" class="doc doc-heading">
            <code>api_llm</code>


</h2>

    <div class="doc doc-contents ">



  <div class="doc doc-children">








<div class="doc doc-object doc-class">



<h3 id="promptolution.llms.api_llm.APILLM" class="doc doc-heading">
            <code>APILLM</code>


</h3>


    <div class="doc doc-contents ">


        <p>A class to interface with various language models through their respective APIs.</p>
<p>This class supports Claude (Anthropic), GPT (OpenAI), and LLaMA (DeepInfra) models.
It handles API key management, model initialization, and provides methods for
both synchronous and asynchronous inference.</p>


<p><span class="doc-section-title">Attributes:</span></p>
    <table>
      <thead>
        <tr>
          <th>Name</th>
          <th>Type</th>
          <th>Description</th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td><code><span title="promptolution.llms.api_llm.APILLM.model">model</span></code></td>
            <td>
            </td>
            <td>
              <div class="doc-md-description">
                <p>The initialized language model instance.</p>
              </div>
            </td>
          </tr>
      </tbody>
    </table>


<p><span class="doc-section-title">Methods:</span></p>
    <table>
      <thead>
        <tr>
          <th>Name</th>
          <th>Description</th>
        </tr>
      </thead>
      <tbody>
            <tr class="doc-section-item">
              <td><code><a class="autorefs autorefs-internal" title="promptolution.llms.api_llm.APILLM.get_response" href="#promptolution.llms.api_llm.APILLM.get_response">get_response</a></code></td>
              <td>
                <div class="doc-md-description">
                  <p>Synchronously get responses for a list of prompts.</p>
                </div>
              </td>
            </tr>
            <tr class="doc-section-item">
              <td><code><span title="promptolution.llms.api_llm.APILLM._get_response">_get_response</span></code></td>
              <td>
                <div class="doc-md-description">
                  <p>Asynchronously get responses for a list of prompts.</p>
                </div>
              </td>
            </tr>
      </tbody>
    </table>

              <details class="quote">
                <summary>Source code in <code>promptolution\llms\api_llm.py</code></summary>
                <pre class="highlight"><code class="language-python">class APILLM:
    """
    A class to interface with various language models through their respective APIs.

    This class supports Claude (Anthropic), GPT (OpenAI), and LLaMA (DeepInfra) models.
    It handles API key management, model initialization, and provides methods for
    both synchronous and asynchronous inference.

    Attributes:
        model: The initialized language model instance.

    Methods:
        get_response: Synchronously get responses for a list of prompts.
        _get_response: Asynchronously get responses for a list of prompts.
    """
    def __init__(self, model_id: str):
        """
        Initialize the APILLM with a specific model.

        Args:
            model_id (str): Identifier for the model to use.

        Raises:
            ValueError: If an unknown model identifier is provided.
        """
        if "claude" in model_id:
            ANTHROPIC_API_KEY = open("anthropictoken.txt", "r").read()
            self.model = ChatAnthropic(model=model_id, api_key=ANTHROPIC_API_KEY)
        elif "gpt" in model_id:
            OPENAI_API_KEY = open("openaitoken.txt", "r").read()
            self.model = ChatOpenAI(model=model_id, api_key=OPENAI_API_KEY)
        elif "llama" in model_id:
            DEEPINFRA_API_KEY = open("deepinfratoken.txt", "r").read()
            self.model = ChatDeepInfra(model_name=model_id, deepinfra_api_token=DEEPINFRA_API_KEY)
        else:
            raise ValueError(f"Unknown model: {model_id}")

    def get_response(self, prompts: List[str]) -&gt; List[str]:
        """
        Synchronously get responses for a list of prompts.

        This method includes retry logic for handling connection errors and rate limits.

        Args:
            prompts (list[str]): List of input prompts.

        Returns:
            list[str]: List of model responses.

        Raises:
            requests.exceptions.ConnectionError: If max retries are exceeded.
        """
        max_retries = 100
        delay = 3
        attempts = 0

        while attempts &lt; max_retries:
            try:
                responses = asyncio.run(self._get_response(prompts))
                return responses
            except requests.exceptions.ConnectionError as e:
                attempts += 1
                logger.critical(
                    f"Connection error: {e}. Attempt {attempts}/{max_retries}. Retrying in {delay} seconds..."
                )
                time.sleep(delay)
            except openai.RateLimitError as e:
                attempts += 1
                logger.critical(
                    f"Rate limit error: {e}. Attempt {attempts}/{max_retries}. Retrying in {delay} seconds..."
                )
                time.sleep(delay)

        # If the loop exits, it means max retries were reached
        raise requests.exceptions.ConnectionError("Max retries exceeded. Connection could not be established.")

    async def _get_response(
        self, prompts: list[str], max_concurrent_calls=200
    ) -&gt; list[str]:  
        """
        Asynchronously get responses for a list of prompts.

        This method uses a semaphore to limit the number of concurrent API calls.

        Args:
            prompts (list[str]): List of input prompts.
            max_concurrent_calls (int): Maximum number of concurrent API calls allowed.

        Returns:
            list[str]: List of model responses.
        """
        semaphore = asyncio.Semaphore(max_concurrent_calls)  # Limit the number of concurrent calls
        tasks = []

        for prompt in prompts:
            tasks.append(invoke_model(prompt, self.model, semaphore))

        responses = await asyncio.gather(*tasks)
        return responses</code></pre>
              </details>



  <div class="doc doc-children">









<div class="doc doc-object doc-function">


<h4 id="promptolution.llms.api_llm.APILLM.__init__" class="doc doc-heading">
            <code class="highlight language-python">__init__(model_id)</code>

</h4>


    <div class="doc doc-contents ">

        <p>Initialize the APILLM with a specific model.</p>


<p><span class="doc-section-title">Parameters:</span></p>
    <table>
      <thead>
        <tr>
          <th>Name</th>
          <th>Type</th>
          <th>Description</th>
          <th>Default</th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td><code>model_id</code></td>
            <td>
                  <code>str</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>Identifier for the model to use.</p>
              </div>
            </td>
            <td>
                <em>required</em>
            </td>
          </tr>
      </tbody>
    </table>


<p><span class="doc-section-title">Raises:</span></p>
    <table>
      <thead>
        <tr>
          <th>Type</th>
          <th>Description</th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td>
                  <code>ValueError</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>If an unknown model identifier is provided.</p>
              </div>
            </td>
          </tr>
      </tbody>
    </table>

            <details class="quote">
              <summary>Source code in <code>promptolution\llms\api_llm.py</code></summary>
              <pre class="highlight"><code class="language-python">def __init__(self, model_id: str):
    """
    Initialize the APILLM with a specific model.

    Args:
        model_id (str): Identifier for the model to use.

    Raises:
        ValueError: If an unknown model identifier is provided.
    """
    if "claude" in model_id:
        ANTHROPIC_API_KEY = open("anthropictoken.txt", "r").read()
        self.model = ChatAnthropic(model=model_id, api_key=ANTHROPIC_API_KEY)
    elif "gpt" in model_id:
        OPENAI_API_KEY = open("openaitoken.txt", "r").read()
        self.model = ChatOpenAI(model=model_id, api_key=OPENAI_API_KEY)
    elif "llama" in model_id:
        DEEPINFRA_API_KEY = open("deepinfratoken.txt", "r").read()
        self.model = ChatDeepInfra(model_name=model_id, deepinfra_api_token=DEEPINFRA_API_KEY)
    else:
        raise ValueError(f"Unknown model: {model_id}")</code></pre>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h4 id="promptolution.llms.api_llm.APILLM.get_response" class="doc doc-heading">
            <code class="highlight language-python">get_response(prompts)</code>

</h4>


    <div class="doc doc-contents ">

        <p>Synchronously get responses for a list of prompts.</p>
<p>This method includes retry logic for handling connection errors and rate limits.</p>


<p><span class="doc-section-title">Parameters:</span></p>
    <table>
      <thead>
        <tr>
          <th>Name</th>
          <th>Type</th>
          <th>Description</th>
          <th>Default</th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td><code>prompts</code></td>
            <td>
                  <code>list[str]</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>List of input prompts.</p>
              </div>
            </td>
            <td>
                <em>required</em>
            </td>
          </tr>
      </tbody>
    </table>


    <p><span class="doc-section-title">Returns:</span></p>
    <table>
      <thead>
        <tr>
          <th>Type</th>
          <th>Description</th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td>
                  <code><span title="typing.List">List</span>[str]</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>list[str]: List of model responses.</p>
              </div>
            </td>
          </tr>
      </tbody>
    </table>


<p><span class="doc-section-title">Raises:</span></p>
    <table>
      <thead>
        <tr>
          <th>Type</th>
          <th>Description</th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td>
                  <code><span title="requests.exceptions.ConnectionError">ConnectionError</span></code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>If max retries are exceeded.</p>
              </div>
            </td>
          </tr>
      </tbody>
    </table>

            <details class="quote">
              <summary>Source code in <code>promptolution\llms\api_llm.py</code></summary>
              <pre class="highlight"><code class="language-python">def get_response(self, prompts: List[str]) -&gt; List[str]:
    """
    Synchronously get responses for a list of prompts.

    This method includes retry logic for handling connection errors and rate limits.

    Args:
        prompts (list[str]): List of input prompts.

    Returns:
        list[str]: List of model responses.

    Raises:
        requests.exceptions.ConnectionError: If max retries are exceeded.
    """
    max_retries = 100
    delay = 3
    attempts = 0

    while attempts &lt; max_retries:
        try:
            responses = asyncio.run(self._get_response(prompts))
            return responses
        except requests.exceptions.ConnectionError as e:
            attempts += 1
            logger.critical(
                f"Connection error: {e}. Attempt {attempts}/{max_retries}. Retrying in {delay} seconds..."
            )
            time.sleep(delay)
        except openai.RateLimitError as e:
            attempts += 1
            logger.critical(
                f"Rate limit error: {e}. Attempt {attempts}/{max_retries}. Retrying in {delay} seconds..."
            )
            time.sleep(delay)

    # If the loop exits, it means max retries were reached
    raise requests.exceptions.ConnectionError("Max retries exceeded. Connection could not be established.")</code></pre>
            </details>
    </div>

</div>



  </div>

    </div>

</div>


<div class="doc doc-object doc-function">


<h3 id="promptolution.llms.api_llm.invoke_model" class="doc doc-heading">
            <code class="highlight language-python">invoke_model(prompt, model, semaphore)</code>

  <span class="doc doc-labels">
      <small class="doc doc-label doc-label-async"><code>async</code></small>
  </span>

</h3>


    <div class="doc doc-contents ">

        <p>Asynchronously invoke a language model with retry logic.</p>


<p><span class="doc-section-title">Parameters:</span></p>
    <table>
      <thead>
        <tr>
          <th>Name</th>
          <th>Type</th>
          <th>Description</th>
          <th>Default</th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td><code>prompt</code></td>
            <td>
                  <code>str</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>The input prompt for the model.</p>
              </div>
            </td>
            <td>
                <em>required</em>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td><code>model</code></td>
            <td>
            </td>
            <td>
              <div class="doc-md-description">
                <p>The language model to invoke.</p>
              </div>
            </td>
            <td>
                <em>required</em>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td><code>semaphore</code></td>
            <td>
                  <code><span title="asyncio.Semaphore">Semaphore</span></code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>Semaphore to limit concurrent calls.</p>
              </div>
            </td>
            <td>
                <em>required</em>
            </td>
          </tr>
      </tbody>
    </table>


    <p><span class="doc-section-title">Returns:</span></p>
    <table>
      <thead>
        <tr>
<th>Name</th>          <th>Type</th>
          <th>Description</th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
<td><code>str</code></td>            <td>
            </td>
            <td>
              <div class="doc-md-description">
                <p>The model's response content.</p>
              </div>
            </td>
          </tr>
      </tbody>
    </table>


<p><span class="doc-section-title">Raises:</span></p>
    <table>
      <thead>
        <tr>
          <th>Type</th>
          <th>Description</th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td>
                  <code><span title="langchain_community.chat_models.deepinfra.ChatDeepInfraException">ChatDeepInfraException</span></code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>If all retry attempts fail.</p>
              </div>
            </td>
          </tr>
      </tbody>
    </table>

            <details class="quote">
              <summary>Source code in <code>promptolution\llms\api_llm.py</code></summary>
              <pre class="highlight"><code class="language-python">async def invoke_model(prompt, model, semaphore):
    """
    Asynchronously invoke a language model with retry logic.

    Args:
        prompt (str): The input prompt for the model.
        model: The language model to invoke.
        semaphore (asyncio.Semaphore): Semaphore to limit concurrent calls.

    Returns:
        str: The model's response content.

    Raises:
        ChatDeepInfraException: If all retry attempts fail.
    """
    async with semaphore:
        max_retries = 100
        delay = 3
        attempts = 0

        while attempts &lt; max_retries:
            try:
                response = await asyncio.to_thread(model.invoke, [HumanMessage(content=prompt)])
                return response.content
            except ChatDeepInfraException as e:
                print(f"DeepInfra error: {e}. Attempt {attempts}/{max_retries}. Retrying in {delay} seconds...")
                attempts += 1
                time.sleep(delay)</code></pre>
            </details>
    </div>

</div>



  </div>

    </div>

</div>

<div class="doc doc-object doc-module">



<h2 id="promptolution.llms.base_llm" class="doc doc-heading">
            <code>base_llm</code>


</h2>

    <div class="doc doc-contents ">



  <div class="doc doc-children">








<div class="doc doc-object doc-class">



<h3 id="promptolution.llms.base_llm.BaseLLM" class="doc doc-heading">
            <code>BaseLLM</code>


</h3>


    <div class="doc doc-contents ">
            <p class="doc doc-class-bases">
              Bases: <code><span title="abc.ABC">ABC</span></code></p>


        <p>Abstract base class for Language Models in the promptolution library.</p>
<p>This class defines the interface that all concrete LLM implementations should follow.</p>


<p><span class="doc-section-title">Methods:</span></p>
    <table>
      <thead>
        <tr>
          <th>Name</th>
          <th>Description</th>
        </tr>
      </thead>
      <tbody>
            <tr class="doc-section-item">
              <td><code><a class="autorefs autorefs-internal" title="promptolution.llms.base_llm.BaseLLM.get_response" href="#promptolution.llms.base_llm.BaseLLM.get_response">get_response</a></code></td>
              <td>
                <div class="doc-md-description">
                  <p>An abstract method that should be implemented by subclasses
          to generate responses for given prompts.</p>
                </div>
              </td>
            </tr>
      </tbody>
    </table>

              <details class="quote">
                <summary>Source code in <code>promptolution\llms\base_llm.py</code></summary>
                <pre class="highlight"><code class="language-python">class BaseLLM(ABC):
    """
    Abstract base class for Language Models in the promptolution library.

    This class defines the interface that all concrete LLM implementations should follow.

    Methods:
        get_response: An abstract method that should be implemented by subclasses
                      to generate responses for given prompts.
    """
    def __init__(self, *args, **kwargs):
        pass

    @abstractmethod
    def get_response(self, prompts: List[str]) -&gt; List[str]:
        """
        Generate responses for the given prompts.

        This method should be implemented by subclasses to define how
        the LLM generates responses.

        Args:
            prompts (List[str]): A list of input prompts.

        Returns:
            List[str]: A list of generated responses corresponding to the input prompts.
        """
        pass</code></pre>
              </details>



  <div class="doc doc-children">









<div class="doc doc-object doc-function">


<h4 id="promptolution.llms.base_llm.BaseLLM.get_response" class="doc doc-heading">
            <code class="highlight language-python">get_response(prompts)</code>

  <span class="doc doc-labels">
      <small class="doc doc-label doc-label-abstractmethod"><code>abstractmethod</code></small>
  </span>

</h4>


    <div class="doc doc-contents ">

        <p>Generate responses for the given prompts.</p>
<p>This method should be implemented by subclasses to define how
the LLM generates responses.</p>


<p><span class="doc-section-title">Parameters:</span></p>
    <table>
      <thead>
        <tr>
          <th>Name</th>
          <th>Type</th>
          <th>Description</th>
          <th>Default</th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td><code>prompts</code></td>
            <td>
                  <code><span title="typing.List">List</span>[str]</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>A list of input prompts.</p>
              </div>
            </td>
            <td>
                <em>required</em>
            </td>
          </tr>
      </tbody>
    </table>


    <p><span class="doc-section-title">Returns:</span></p>
    <table>
      <thead>
        <tr>
          <th>Type</th>
          <th>Description</th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td>
                  <code><span title="typing.List">List</span>[str]</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>List[str]: A list of generated responses corresponding to the input prompts.</p>
              </div>
            </td>
          </tr>
      </tbody>
    </table>

            <details class="quote">
              <summary>Source code in <code>promptolution\llms\base_llm.py</code></summary>
              <pre class="highlight"><code class="language-python">@abstractmethod
def get_response(self, prompts: List[str]) -&gt; List[str]:
    """
    Generate responses for the given prompts.

    This method should be implemented by subclasses to define how
    the LLM generates responses.

    Args:
        prompts (List[str]): A list of input prompts.

    Returns:
        List[str]: A list of generated responses corresponding to the input prompts.
    """
    pass</code></pre>
            </details>
    </div>

</div>



  </div>

    </div>

</div>

<div class="doc doc-object doc-class">



<h3 id="promptolution.llms.base_llm.DummyLLM" class="doc doc-heading">
            <code>DummyLLM</code>


</h3>


    <div class="doc doc-contents ">
            <p class="doc doc-class-bases">
              Bases: <code><a class="autorefs autorefs-internal" title="promptolution.llms.base_llm.BaseLLM" href="#promptolution.llms.base_llm.BaseLLM">BaseLLM</a></code></p>


        <p>A dummy implementation of the BaseLLM for testing purposes.</p>
<p>This class generates random responses for given prompts, simulating
the behavior of a language model without actually performing any
complex natural language processing.</p>

              <details class="quote">
                <summary>Source code in <code>promptolution\llms\base_llm.py</code></summary>
                <pre class="highlight"><code class="language-python">class DummyLLM(BaseLLM):
    """
    A dummy implementation of the BaseLLM for testing purposes.

    This class generates random responses for given prompts, simulating
    the behavior of a language model without actually performing any
    complex natural language processing.
    """
    def __init__(self, *args, **kwargs):
        pass

    def get_response(self, prompts: str) -&gt; str:
        """
        Generate random responses for the given prompts.

        This method creates silly, random responses enclosed in &lt;prompt&gt; tags.
        It's designed for testing and demonstration purposes.

        Args:
            prompts (str or List[str]): Input prompt(s). If a single string is provided,
                                        it's converted to a list containing that string.

        Returns:
            List[str]: A list of randomly generated responses, one for each input prompt.
        """
        if isinstance(prompts, str):
            prompts = [prompts]
        results = []
        for _ in prompts:
            r = np.random.rand()
            if r &lt; 0.3:
                results += [f"Joooo wazzuppp &lt;prompt&gt;hier gehts los {r} &lt;/prompt&gt;"]
            if 0.3 &lt;= r &lt; 0.6:
                results += [f"was das hier? &lt;prompt&gt;peter lustig{r}&lt;/prompt&gt;"]
            else:
                results += [f"hier ist ein &lt;prompt&gt;test{r}&lt;/prompt&gt;"]

        return results</code></pre>
              </details>



  <div class="doc doc-children">









<div class="doc doc-object doc-function">


<h4 id="promptolution.llms.base_llm.DummyLLM.get_response" class="doc doc-heading">
            <code class="highlight language-python">get_response(prompts)</code>

</h4>


    <div class="doc doc-contents ">

        <p>Generate random responses for the given prompts.</p>
<p>This method creates silly, random responses enclosed in <prompt> tags.
It's designed for testing and demonstration purposes.</p>


<p><span class="doc-section-title">Parameters:</span></p>
    <table>
      <thead>
        <tr>
          <th>Name</th>
          <th>Type</th>
          <th>Description</th>
          <th>Default</th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td><code>prompts</code></td>
            <td>
                  <code>str or <span title="typing.List">List</span>[str]</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>Input prompt(s). If a single string is provided,
                        it's converted to a list containing that string.</p>
              </div>
            </td>
            <td>
                <em>required</em>
            </td>
          </tr>
      </tbody>
    </table>


    <p><span class="doc-section-title">Returns:</span></p>
    <table>
      <thead>
        <tr>
          <th>Type</th>
          <th>Description</th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td>
                  <code>str</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>List[str]: A list of randomly generated responses, one for each input prompt.</p>
              </div>
            </td>
          </tr>
      </tbody>
    </table>

            <details class="quote">
              <summary>Source code in <code>promptolution\llms\base_llm.py</code></summary>
              <pre class="highlight"><code class="language-python">def get_response(self, prompts: str) -&gt; str:
    """
    Generate random responses for the given prompts.

    This method creates silly, random responses enclosed in &lt;prompt&gt; tags.
    It's designed for testing and demonstration purposes.

    Args:
        prompts (str or List[str]): Input prompt(s). If a single string is provided,
                                    it's converted to a list containing that string.

    Returns:
        List[str]: A list of randomly generated responses, one for each input prompt.
    """
    if isinstance(prompts, str):
        prompts = [prompts]
    results = []
    for _ in prompts:
        r = np.random.rand()
        if r &lt; 0.3:
            results += [f"Joooo wazzuppp &lt;prompt&gt;hier gehts los {r} &lt;/prompt&gt;"]
        if 0.3 &lt;= r &lt; 0.6:
            results += [f"was das hier? &lt;prompt&gt;peter lustig{r}&lt;/prompt&gt;"]
        else:
            results += [f"hier ist ein &lt;prompt&gt;test{r}&lt;/prompt&gt;"]

    return results</code></pre>
            </details>
    </div>

</div>



  </div>

    </div>

</div>




  </div>

    </div>

</div>

<div class="doc doc-object doc-module">



<h2 id="promptolution.llms.deepinfra" class="doc doc-heading">
            <code>deepinfra</code>


</h2>

    <div class="doc doc-contents ">



  <div class="doc doc-children">








<div class="doc doc-object doc-class">



<h3 id="promptolution.llms.deepinfra.ChatDeepInfra" class="doc doc-heading">
            <code>ChatDeepInfra</code>


</h3>


    <div class="doc doc-contents ">
            <p class="doc doc-class-bases">
              Bases: <code><span title="langchain_core.language_models.chat_models.BaseChatModel">BaseChatModel</span></code></p>


        <p>A chat model that uses the DeepInfra API.</p>

              <details class="quote">
                <summary>Source code in <code>promptolution\llms\deepinfra.py</code></summary>
                <pre class="highlight"><code class="language-python">class ChatDeepInfra(BaseChatModel):
    """A chat model that uses the DeepInfra API."""

    # client: Any  #: :meta private:
    model_name: str = Field(alias="model")
    """The model name to use for the chat model."""
    deepinfra_api_token: Optional[str] = None
    request_timeout: Optional[float] = Field(default=None, alias="timeout")
    temperature: Optional[float] = 1
    model_kwargs: Dict[str, Any] = Field(default_factory=dict)
    """Run inference with this temperature. Must be in the closed
       interval [0.0, 1.0]."""
    top_p: Optional[float] = None
    """Decode using nucleus sampling: consider the smallest set of tokens whose
       probability sum is at least top_p. Must be in the closed interval [0.0, 1.0]."""
    top_k: Optional[int] = None
    """Decode using top-k sampling: consider the set of top_k most probable tokens.
       Must be positive."""
    n: int = 1
    """Number of chat completions to generate for each prompt. Note that the API may
       not return the full n completions if duplicates are generated."""
    max_tokens: int = 256
    streaming: bool = False
    max_retries: int = 1

    def __init__(self, model_name: str, **kwargs: Any):
        super().__init__(model=model_name, **kwargs)

    @property
    def _default_params(self) -&gt; Dict[str, Any]:
        """Get the default parameters for calling OpenAI API."""
        return {
            "model": self.model_name,
            "max_tokens": self.max_tokens,
            "stream": self.streaming,
            "n": self.n,
            "temperature": self.temperature,
            "request_timeout": self.request_timeout,
            **self.model_kwargs,
        }

    @property
    def _client_params(self) -&gt; Dict[str, Any]:
        """Get the parameters used for the openai client."""
        return {**self._default_params}

    def completion_with_retry(self, run_manager: Optional[CallbackManagerForLLMRun] = None, **kwargs: Any) -&gt; Any:
        """Use tenacity to retry the completion call."""
        retry_decorator = _create_retry_decorator(self, run_manager=run_manager)

        @retry_decorator
        def _completion_with_retry(**kwargs: Any) -&gt; Any:
            try:
                request_timeout = kwargs.pop("request_timeout")
                request = Requests(headers=self._headers())
                response = request.post(url=self._url(), data=self._body(kwargs), timeout=request_timeout)
                self._handle_status(response.status_code, response.text)
                return response
            except Exception as e:
                # import pdb; pdb.set_trace()
                print("EX", e)  # noqa: T201
                raise

        return _completion_with_retry(**kwargs)

    async def acompletion_with_retry(
        self,
        run_manager: Optional[AsyncCallbackManagerForLLMRun] = None,
        **kwargs: Any,
    ) -&gt; Any:
        """Use tenacity to retry the async completion call."""
        retry_decorator = _create_retry_decorator(self, run_manager=run_manager)

        @retry_decorator
        async def _completion_with_retry(**kwargs: Any) -&gt; Any:
            try:
                request_timeout = kwargs.pop("request_timeout")
                request = Requests(headers=self._headers())
                async with request.apost(url=self._url(), data=self._body(kwargs), timeout=request_timeout) as response:
                    self._handle_status(response.status, response.text)
                    return await response.json()
            except Exception as e:
                print("EX", e)  # noqa: T201
                raise

        return await _completion_with_retry(**kwargs)

    @root_validator(pre=True)
    def init_defaults(cls, values: Dict) -&gt; Dict:
        """Validate api key, python package exists, temperature, top_p, and top_k."""
        # For compatibility with LiteLLM
        api_key = get_from_dict_or_env(
            values,
            "deepinfra_api_key",
            "DEEPINFRA_API_KEY",
            default="",
        )
        values["deepinfra_api_token"] = get_from_dict_or_env(
            values,
            "deepinfra_api_token",
            "DEEPINFRA_API_TOKEN",
            default=api_key,
        )
        # set model id
        # values["model_name"] = get_from_dict_or_env(
        #     values,
        #     "model_name",
        #     "DEEPINFRA_MODEL_NAME",
        #     default="",
        # )
        return values

    @root_validator(pre=False, skip_on_failure=True)
    def validate_environment(cls, values: Dict) -&gt; Dict:
        if values["temperature"] is not None and not 0 &lt;= values["temperature"] &lt;= 1:
            raise ValueError("temperature must be in the range [0.0, 1.0]")

        if values["top_p"] is not None and not 0 &lt;= values["top_p"] &lt;= 1:
            raise ValueError("top_p must be in the range [0.0, 1.0]")

        if values["top_k"] is not None and values["top_k"] &lt;= 0:
            raise ValueError("top_k must be positive")

        return values

    def _generate(
        self,
        messages: List[BaseMessage],
        stop: Optional[List[str]] = None,
        run_manager: Optional[CallbackManagerForLLMRun] = None,
        stream: Optional[bool] = None,
        **kwargs: Any,
    ) -&gt; ChatResult:
        should_stream = stream if stream is not None else self.streaming
        if should_stream:
            stream_iter = self._stream(messages, stop=stop, run_manager=run_manager, **kwargs)
            return generate_from_stream(stream_iter)

        message_dicts, params = self._create_message_dicts(messages, stop)
        params = {**params, **kwargs}
        response = self.completion_with_retry(messages=message_dicts, run_manager=run_manager, **params)
        return self._create_chat_result(response.json())

    def _create_chat_result(self, response: Mapping[str, Any]) -&gt; ChatResult:
        generations = []
        for res in response["choices"]:
            message = _convert_dict_to_message(res["message"])
            gen = ChatGeneration(
                message=message,
                generation_info=dict(finish_reason=res.get("finish_reason")),
            )
            generations.append(gen)
        token_usage = response.get("usage", {})
        llm_output = {"token_usage": token_usage, "model": self.model_name}
        res = ChatResult(generations=generations, llm_output=llm_output)
        return res

    def _create_message_dicts(
        self, messages: List[BaseMessage], stop: Optional[List[str]]
    ) -&gt; Tuple[List[Dict[str, Any]], Dict[str, Any]]:
        params = self._client_params
        if stop is not None:
            if "stop" in params:
                raise ValueError("`stop` found in both the input and default params.")
            params["stop"] = stop
        message_dicts = [_convert_message_to_dict(m) for m in messages]
        return message_dicts, params

    def _stream(
        self,
        messages: List[BaseMessage],
        stop: Optional[List[str]] = None,
        run_manager: Optional[CallbackManagerForLLMRun] = None,
        **kwargs: Any,
    ) -&gt; Iterator[ChatGenerationChunk]:
        message_dicts, params = self._create_message_dicts(messages, stop)
        params = {**params, **kwargs, "stream": True}

        response = self.completion_with_retry(messages=message_dicts, run_manager=run_manager, **params)
        for line in _parse_stream(response.iter_lines()):
            chunk = _handle_sse_line(line)
            if chunk:
                cg_chunk = ChatGenerationChunk(message=chunk, generation_info=None)
                if run_manager:
                    run_manager.on_llm_new_token(str(chunk.content), chunk=cg_chunk)
                yield cg_chunk

    async def _astream(
        self,
        messages: List[BaseMessage],
        stop: Optional[List[str]] = None,
        run_manager: Optional[AsyncCallbackManagerForLLMRun] = None,
        **kwargs: Any,
    ) -&gt; AsyncIterator[ChatGenerationChunk]:
        message_dicts, params = self._create_message_dicts(messages, stop)
        params = {"messages": message_dicts, "stream": True, **params, **kwargs}

        request_timeout = params.pop("request_timeout")
        request = Requests(headers=self._headers())
        async with request.apost(url=self._url(), data=self._body(params), timeout=request_timeout) as response:
            async for line in _parse_stream_async(response.content):
                chunk = _handle_sse_line(line)
                if chunk:
                    cg_chunk = ChatGenerationChunk(message=chunk, generation_info=None)
                    if run_manager:
                        await run_manager.on_llm_new_token(str(chunk.content), chunk=cg_chunk)
                    yield cg_chunk

    async def _agenerate(
        self,
        messages: List[BaseMessage],
        stop: Optional[List[str]] = None,
        run_manager: Optional[AsyncCallbackManagerForLLMRun] = None,
        stream: Optional[bool] = None,
        **kwargs: Any,
    ) -&gt; ChatResult:
        should_stream = stream if stream is not None else self.streaming
        if should_stream:
            stream_iter = self._astream(messages, stop=stop, run_manager=run_manager, **kwargs)
            return await agenerate_from_stream(stream_iter)

        message_dicts, params = self._create_message_dicts(messages, stop)
        params = {"messages": message_dicts, **params, **kwargs}

        res = await self.acompletion_with_retry(run_manager=run_manager, **params)
        return self._create_chat_result(res)

    @property
    def _identifying_params(self) -&gt; Dict[str, Any]:
        """Get the identifying parameters."""
        return {
            "model": self.model_name,
            "temperature": self.temperature,
            "top_p": self.top_p,
            "top_k": self.top_k,
            "n": self.n,
        }

    @property
    def _llm_type(self) -&gt; str:
        return "deepinfra-chat"

    def _handle_status(self, code: int, text: Any) -&gt; None:
        if code &gt;= 500:
            raise ChatDeepInfraException(f"DeepInfra Server: Error {code}")
        elif code &gt;= 400:
            raise ValueError(f"DeepInfra received an invalid payload: {text}")
        elif code != 200:
            raise Exception(f"DeepInfra returned an unexpected response with status " f"{code}: {text}")

    def _url(self) -&gt; str:
        return "https://stage.api.deepinfra.com/v1/openai/chat/completions"

    def _headers(self) -&gt; Dict:
        return {
            "Authorization": f"bearer {self.deepinfra_api_token}",
            "Content-Type": "application/json",
        }

    def _body(self, kwargs: Any) -&gt; Dict:
        return kwargs

    def bind_tools(
        self,
        tools: Sequence[Union[Dict[str, Any], Type[BaseModel], Callable, BaseTool]],
        **kwargs: Any,
    ) -&gt; Runnable[LanguageModelInput, BaseMessage]:
        """Bind tool-like objects to this chat model.

        Assumes model is compatible with OpenAI tool-calling API.

        Args:
            tools: A list of tool definitions to bind to this chat model.
                Can be  a dictionary, pydantic model, callable, or BaseTool. Pydantic
                models, callables, and BaseTools will be automatically converted to
                their schema dictionary representation.
            **kwargs: Any additional parameters to pass to the
                :class:`~langchain.runnable.Runnable` constructor.
        """

        formatted_tools = [convert_to_openai_tool(tool) for tool in tools]
        return super().bind(tools=formatted_tools, **kwargs)</code></pre>
              </details>



  <div class="doc doc-children">







<div class="doc doc-object doc-attribute">



<h4 id="promptolution.llms.deepinfra.ChatDeepInfra.model_kwargs" class="doc doc-heading">
            <code class="highlight language-python">model_kwargs: Dict[str, Any] = Field(default_factory=dict)</code>

  <span class="doc doc-labels">
      <small class="doc doc-label doc-label-class-attribute"><code>class-attribute</code></small>
      <small class="doc doc-label doc-label-instance-attribute"><code>instance-attribute</code></small>
  </span>

</h4>


    <div class="doc doc-contents ">

        <p>Run inference with this temperature. Must be in the closed
interval [0.0, 1.0].</p>
    </div>

</div>

<div class="doc doc-object doc-attribute">



<h4 id="promptolution.llms.deepinfra.ChatDeepInfra.model_name" class="doc doc-heading">
            <code class="highlight language-python">model_name: str = Field(alias='model')</code>

  <span class="doc doc-labels">
      <small class="doc doc-label doc-label-class-attribute"><code>class-attribute</code></small>
      <small class="doc doc-label doc-label-instance-attribute"><code>instance-attribute</code></small>
  </span>

</h4>


    <div class="doc doc-contents ">

        <p>The model name to use for the chat model.</p>
    </div>

</div>

<div class="doc doc-object doc-attribute">



<h4 id="promptolution.llms.deepinfra.ChatDeepInfra.n" class="doc doc-heading">
            <code class="highlight language-python">n: int = 1</code>

  <span class="doc doc-labels">
      <small class="doc doc-label doc-label-class-attribute"><code>class-attribute</code></small>
      <small class="doc doc-label doc-label-instance-attribute"><code>instance-attribute</code></small>
  </span>

</h4>


    <div class="doc doc-contents ">

        <p>Number of chat completions to generate for each prompt. Note that the API may
not return the full n completions if duplicates are generated.</p>
    </div>

</div>

<div class="doc doc-object doc-attribute">



<h4 id="promptolution.llms.deepinfra.ChatDeepInfra.top_k" class="doc doc-heading">
            <code class="highlight language-python">top_k: Optional[int] = None</code>

  <span class="doc doc-labels">
      <small class="doc doc-label doc-label-class-attribute"><code>class-attribute</code></small>
      <small class="doc doc-label doc-label-instance-attribute"><code>instance-attribute</code></small>
  </span>

</h4>


    <div class="doc doc-contents ">

        <p>Decode using top-k sampling: consider the set of top_k most probable tokens.
Must be positive.</p>
    </div>

</div>

<div class="doc doc-object doc-attribute">



<h4 id="promptolution.llms.deepinfra.ChatDeepInfra.top_p" class="doc doc-heading">
            <code class="highlight language-python">top_p: Optional[float] = None</code>

  <span class="doc doc-labels">
      <small class="doc doc-label doc-label-class-attribute"><code>class-attribute</code></small>
      <small class="doc doc-label doc-label-instance-attribute"><code>instance-attribute</code></small>
  </span>

</h4>


    <div class="doc doc-contents ">

        <p>Decode using nucleus sampling: consider the smallest set of tokens whose
probability sum is at least top_p. Must be in the closed interval [0.0, 1.0].</p>
    </div>

</div>



<div class="doc doc-object doc-function">


<h4 id="promptolution.llms.deepinfra.ChatDeepInfra.acompletion_with_retry" class="doc doc-heading">
            <code class="highlight language-python">acompletion_with_retry(run_manager=None, **kwargs)</code>

  <span class="doc doc-labels">
      <small class="doc doc-label doc-label-async"><code>async</code></small>
  </span>

</h4>


    <div class="doc doc-contents ">

        <p>Use tenacity to retry the async completion call.</p>

            <details class="quote">
              <summary>Source code in <code>promptolution\llms\deepinfra.py</code></summary>
              <pre class="highlight"><code class="language-python">async def acompletion_with_retry(
    self,
    run_manager: Optional[AsyncCallbackManagerForLLMRun] = None,
    **kwargs: Any,
) -&gt; Any:
    """Use tenacity to retry the async completion call."""
    retry_decorator = _create_retry_decorator(self, run_manager=run_manager)

    @retry_decorator
    async def _completion_with_retry(**kwargs: Any) -&gt; Any:
        try:
            request_timeout = kwargs.pop("request_timeout")
            request = Requests(headers=self._headers())
            async with request.apost(url=self._url(), data=self._body(kwargs), timeout=request_timeout) as response:
                self._handle_status(response.status, response.text)
                return await response.json()
        except Exception as e:
            print("EX", e)  # noqa: T201
            raise

    return await _completion_with_retry(**kwargs)</code></pre>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h4 id="promptolution.llms.deepinfra.ChatDeepInfra.bind_tools" class="doc doc-heading">
            <code class="highlight language-python">bind_tools(tools, **kwargs)</code>

</h4>


    <div class="doc doc-contents ">

        <p>Bind tool-like objects to this chat model.</p>
<p>Assumes model is compatible with OpenAI tool-calling API.</p>


<p><span class="doc-section-title">Parameters:</span></p>
    <table>
      <thead>
        <tr>
          <th>Name</th>
          <th>Type</th>
          <th>Description</th>
          <th>Default</th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td><code>tools</code></td>
            <td>
                  <code><span title="typing.Sequence">Sequence</span>[<span title="typing.Union">Union</span>[<span title="typing.Dict">Dict</span>[str, <span title="typing.Any">Any</span>], <span title="typing.Type">Type</span>[<span title="langchain_core.pydantic_v1.BaseModel">BaseModel</span>], <span title="typing.Callable">Callable</span>, <span title="langchain_core.tools.BaseTool">BaseTool</span>]]</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>A list of tool definitions to bind to this chat model.
Can be  a dictionary, pydantic model, callable, or BaseTool. Pydantic
models, callables, and BaseTools will be automatically converted to
their schema dictionary representation.</p>
              </div>
            </td>
            <td>
                <em>required</em>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td><code>**kwargs</code></td>
            <td>
                  <code><span title="typing.Any">Any</span></code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>Any additional parameters to pass to the
:class:<code>~langchain.runnable.Runnable</code> constructor.</p>
              </div>
            </td>
            <td>
                  <code>{}</code>
            </td>
          </tr>
      </tbody>
    </table>

            <details class="quote">
              <summary>Source code in <code>promptolution\llms\deepinfra.py</code></summary>
              <pre class="highlight"><code class="language-python">def bind_tools(
    self,
    tools: Sequence[Union[Dict[str, Any], Type[BaseModel], Callable, BaseTool]],
    **kwargs: Any,
) -&gt; Runnable[LanguageModelInput, BaseMessage]:
    """Bind tool-like objects to this chat model.

    Assumes model is compatible with OpenAI tool-calling API.

    Args:
        tools: A list of tool definitions to bind to this chat model.
            Can be  a dictionary, pydantic model, callable, or BaseTool. Pydantic
            models, callables, and BaseTools will be automatically converted to
            their schema dictionary representation.
        **kwargs: Any additional parameters to pass to the
            :class:`~langchain.runnable.Runnable` constructor.
    """

    formatted_tools = [convert_to_openai_tool(tool) for tool in tools]
    return super().bind(tools=formatted_tools, **kwargs)</code></pre>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h4 id="promptolution.llms.deepinfra.ChatDeepInfra.completion_with_retry" class="doc doc-heading">
            <code class="highlight language-python">completion_with_retry(run_manager=None, **kwargs)</code>

</h4>


    <div class="doc doc-contents ">

        <p>Use tenacity to retry the completion call.</p>

            <details class="quote">
              <summary>Source code in <code>promptolution\llms\deepinfra.py</code></summary>
              <pre class="highlight"><code class="language-python">def completion_with_retry(self, run_manager: Optional[CallbackManagerForLLMRun] = None, **kwargs: Any) -&gt; Any:
    """Use tenacity to retry the completion call."""
    retry_decorator = _create_retry_decorator(self, run_manager=run_manager)

    @retry_decorator
    def _completion_with_retry(**kwargs: Any) -&gt; Any:
        try:
            request_timeout = kwargs.pop("request_timeout")
            request = Requests(headers=self._headers())
            response = request.post(url=self._url(), data=self._body(kwargs), timeout=request_timeout)
            self._handle_status(response.status_code, response.text)
            return response
        except Exception as e:
            # import pdb; pdb.set_trace()
            print("EX", e)  # noqa: T201
            raise

    return _completion_with_retry(**kwargs)</code></pre>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h4 id="promptolution.llms.deepinfra.ChatDeepInfra.init_defaults" class="doc doc-heading">
            <code class="highlight language-python">init_defaults(values)</code>

</h4>


    <div class="doc doc-contents ">

        <p>Validate api key, python package exists, temperature, top_p, and top_k.</p>

            <details class="quote">
              <summary>Source code in <code>promptolution\llms\deepinfra.py</code></summary>
              <pre class="highlight"><code class="language-python">@root_validator(pre=True)
def init_defaults(cls, values: Dict) -&gt; Dict:
    """Validate api key, python package exists, temperature, top_p, and top_k."""
    # For compatibility with LiteLLM
    api_key = get_from_dict_or_env(
        values,
        "deepinfra_api_key",
        "DEEPINFRA_API_KEY",
        default="",
    )
    values["deepinfra_api_token"] = get_from_dict_or_env(
        values,
        "deepinfra_api_token",
        "DEEPINFRA_API_TOKEN",
        default=api_key,
    )
    # set model id
    # values["model_name"] = get_from_dict_or_env(
    #     values,
    #     "model_name",
    #     "DEEPINFRA_MODEL_NAME",
    #     default="",
    # )
    return values</code></pre>
            </details>
    </div>

</div>



  </div>

    </div>

</div>




  </div>

    </div>

</div>

<div class="doc doc-object doc-module">



<h2 id="promptolution.llms.local_llm" class="doc doc-heading">
            <code>local_llm</code>


</h2>

    <div class="doc doc-contents ">



  <div class="doc doc-children">








<div class="doc doc-object doc-class">



<h3 id="promptolution.llms.local_llm.LocalLLM" class="doc doc-heading">
            <code>LocalLLM</code>


</h3>


    <div class="doc doc-contents ">


        <p>A class for running language models locally using the Hugging Face Transformers library.</p>
<p>This class sets up a text generation pipeline with specified model parameters
and provides a method to generate responses for given prompts.</p>


<p><span class="doc-section-title">Attributes:</span></p>
    <table>
      <thead>
        <tr>
          <th>Name</th>
          <th>Type</th>
          <th>Description</th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td><code><span title="promptolution.llms.local_llm.LocalLLM.pipeline">pipeline</span></code></td>
            <td>
                  <code><span title="transformers.Pipeline">Pipeline</span></code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>The text generation pipeline.</p>
              </div>
            </td>
          </tr>
      </tbody>
    </table>


<p><span class="doc-section-title">Methods:</span></p>
    <table>
      <thead>
        <tr>
          <th>Name</th>
          <th>Description</th>
        </tr>
      </thead>
      <tbody>
            <tr class="doc-section-item">
              <td><code><a class="autorefs autorefs-internal" title="promptolution.llms.local_llm.LocalLLM.get_response" href="#promptolution.llms.local_llm.LocalLLM.get_response">get_response</a></code></td>
              <td>
                <div class="doc-md-description">
                  <p>Generate responses for a list of prompts.</p>
                </div>
              </td>
            </tr>
      </tbody>
    </table>

              <details class="quote">
                <summary>Source code in <code>promptolution\llms\local_llm.py</code></summary>
                <pre class="highlight"><code class="language-python">class LocalLLM:
    """
    A class for running language models locally using the Hugging Face Transformers library.

    This class sets up a text generation pipeline with specified model parameters
    and provides a method to generate responses for given prompts.

    Attributes:
        pipeline (transformers.Pipeline): The text generation pipeline.

    Methods:
        get_response: Generate responses for a list of prompts.
    """
    def __init__(self, model_id: str, batch_size=8):
        """
        Initialize the LocalLLM with a specific model.

        Args:
            model_id (str): The identifier of the model to use (e.g., "gpt2", "facebook/opt-1.3b").
            batch_size (int, optional): The batch size for text generation. Defaults to 8.

        Note:
            This method sets up a text generation pipeline with bfloat16 precision,
            automatic device mapping, and specific generation parameters.
        """
        self.pipeline = transformers.pipeline(
            "text-generation",
            model=model_id,
            model_kwargs={"torch_dtype": torch.bfloat16},
            device_map="auto",
            max_new_tokens=256,
            batch_size=batch_size,
            num_return_sequences=1,
            return_full_text=False,
        )
        self.pipeline.tokenizer.pad_token_id = self.pipeline.tokenizer.eos_token_id
        self.pipeline.tokenizer.padding_side = "left"

    def get_response(self, prompts: list[str]):
        """
        Generate responses for a list of prompts using the local language model.

        Args:
            prompts (list[str]): A list of input prompts.

        Returns:
            list[str]: A list of generated responses corresponding to the input prompts.

        Note:
            This method uses torch.no_grad() for inference to reduce memory usage.
            It handles both single and batch inputs, ensuring consistent output format.
        """
        with torch.no_grad():
            response = self.pipeline(prompts, pad_token_id=self.pipeline.tokenizer.eos_token_id)

        if len(response) != 1:
            response = [r[0] if isinstance(r, list) else r for r in response]

        response = [r["generated_text"] for r in response]
        return response

    def __del__(self):
        try:
            del self.pipeline
            torch.cuda.empty_cache()
        except Exception as e:
            logger.warning(f"Error during LocalLLM cleanup: {e}")</code></pre>
              </details>



  <div class="doc doc-children">









<div class="doc doc-object doc-function">


<h4 id="promptolution.llms.local_llm.LocalLLM.__init__" class="doc doc-heading">
            <code class="highlight language-python">__init__(model_id, batch_size=8)</code>

</h4>


    <div class="doc doc-contents ">

        <p>Initialize the LocalLLM with a specific model.</p>


<p><span class="doc-section-title">Parameters:</span></p>
    <table>
      <thead>
        <tr>
          <th>Name</th>
          <th>Type</th>
          <th>Description</th>
          <th>Default</th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td><code>model_id</code></td>
            <td>
                  <code>str</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>The identifier of the model to use (e.g., "gpt2", "facebook/opt-1.3b").</p>
              </div>
            </td>
            <td>
                <em>required</em>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td><code>batch_size</code></td>
            <td>
                  <code>int</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>The batch size for text generation. Defaults to 8.</p>
              </div>
            </td>
            <td>
                  <code>8</code>
            </td>
          </tr>
      </tbody>
    </table>


<details class="note" open>
  <summary>Note</summary>
  <p>This method sets up a text generation pipeline with bfloat16 precision,
automatic device mapping, and specific generation parameters.</p>
</details>
            <details class="quote">
              <summary>Source code in <code>promptolution\llms\local_llm.py</code></summary>
              <pre class="highlight"><code class="language-python">def __init__(self, model_id: str, batch_size=8):
    """
    Initialize the LocalLLM with a specific model.

    Args:
        model_id (str): The identifier of the model to use (e.g., "gpt2", "facebook/opt-1.3b").
        batch_size (int, optional): The batch size for text generation. Defaults to 8.

    Note:
        This method sets up a text generation pipeline with bfloat16 precision,
        automatic device mapping, and specific generation parameters.
    """
    self.pipeline = transformers.pipeline(
        "text-generation",
        model=model_id,
        model_kwargs={"torch_dtype": torch.bfloat16},
        device_map="auto",
        max_new_tokens=256,
        batch_size=batch_size,
        num_return_sequences=1,
        return_full_text=False,
    )
    self.pipeline.tokenizer.pad_token_id = self.pipeline.tokenizer.eos_token_id
    self.pipeline.tokenizer.padding_side = "left"</code></pre>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h4 id="promptolution.llms.local_llm.LocalLLM.get_response" class="doc doc-heading">
            <code class="highlight language-python">get_response(prompts)</code>

</h4>


    <div class="doc doc-contents ">

        <p>Generate responses for a list of prompts using the local language model.</p>


<p><span class="doc-section-title">Parameters:</span></p>
    <table>
      <thead>
        <tr>
          <th>Name</th>
          <th>Type</th>
          <th>Description</th>
          <th>Default</th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td><code>prompts</code></td>
            <td>
                  <code>list[str]</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>A list of input prompts.</p>
              </div>
            </td>
            <td>
                <em>required</em>
            </td>
          </tr>
      </tbody>
    </table>


    <p><span class="doc-section-title">Returns:</span></p>
    <table>
      <thead>
        <tr>
          <th>Type</th>
          <th>Description</th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td>
            </td>
            <td>
              <div class="doc-md-description">
                <p>list[str]: A list of generated responses corresponding to the input prompts.</p>
              </div>
            </td>
          </tr>
      </tbody>
    </table>


<details class="note" open>
  <summary>Note</summary>
  <p>This method uses torch.no_grad() for inference to reduce memory usage.
It handles both single and batch inputs, ensuring consistent output format.</p>
</details>
            <details class="quote">
              <summary>Source code in <code>promptolution\llms\local_llm.py</code></summary>
              <pre class="highlight"><code class="language-python">def get_response(self, prompts: list[str]):
    """
    Generate responses for a list of prompts using the local language model.

    Args:
        prompts (list[str]): A list of input prompts.

    Returns:
        list[str]: A list of generated responses corresponding to the input prompts.

    Note:
        This method uses torch.no_grad() for inference to reduce memory usage.
        It handles both single and batch inputs, ensuring consistent output format.
    """
    with torch.no_grad():
        response = self.pipeline(prompts, pad_token_id=self.pipeline.tokenizer.eos_token_id)

    if len(response) != 1:
        response = [r[0] if isinstance(r, list) else r for r in response]

    response = [r["generated_text"] for r in response]
    return response</code></pre>
            </details>
    </div>

</div>



  </div>

    </div>

</div>




  </div>

    </div>

</div>


  </div>

    </div>

</div><h2 id="api-llm">API LLM</h2>


<div class="doc doc-object doc-module">



<a id="promptolution.llms.api_llm"></a>
    <div class="doc doc-contents first">



  <div class="doc doc-children">








<div class="doc doc-object doc-class">



<h2 id="promptolution.llms.api_llm.APILLM" class="doc doc-heading">
            <code>APILLM</code>


</h2>


    <div class="doc doc-contents ">


        <p>A class to interface with various language models through their respective APIs.</p>
<p>This class supports Claude (Anthropic), GPT (OpenAI), and LLaMA (DeepInfra) models.
It handles API key management, model initialization, and provides methods for
both synchronous and asynchronous inference.</p>


<p><span class="doc-section-title">Attributes:</span></p>
    <table>
      <thead>
        <tr>
          <th>Name</th>
          <th>Type</th>
          <th>Description</th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td><code><span title="promptolution.llms.api_llm.APILLM.model">model</span></code></td>
            <td>
            </td>
            <td>
              <div class="doc-md-description">
                <p>The initialized language model instance.</p>
              </div>
            </td>
          </tr>
      </tbody>
    </table>


<p><span class="doc-section-title">Methods:</span></p>
    <table>
      <thead>
        <tr>
          <th>Name</th>
          <th>Description</th>
        </tr>
      </thead>
      <tbody>
            <tr class="doc-section-item">
              <td><code><a class="autorefs autorefs-internal" title="promptolution.llms.api_llm.APILLM.get_response" href="#promptolution.llms.api_llm.APILLM.get_response">get_response</a></code></td>
              <td>
                <div class="doc-md-description">
                  <p>Synchronously get responses for a list of prompts.</p>
                </div>
              </td>
            </tr>
            <tr class="doc-section-item">
              <td><code><span title="promptolution.llms.api_llm.APILLM._get_response">_get_response</span></code></td>
              <td>
                <div class="doc-md-description">
                  <p>Asynchronously get responses for a list of prompts.</p>
                </div>
              </td>
            </tr>
      </tbody>
    </table>

              <details class="quote">
                <summary>Source code in <code>promptolution\llms\api_llm.py</code></summary>
                <pre class="highlight"><code class="language-python">class APILLM:
    """
    A class to interface with various language models through their respective APIs.

    This class supports Claude (Anthropic), GPT (OpenAI), and LLaMA (DeepInfra) models.
    It handles API key management, model initialization, and provides methods for
    both synchronous and asynchronous inference.

    Attributes:
        model: The initialized language model instance.

    Methods:
        get_response: Synchronously get responses for a list of prompts.
        _get_response: Asynchronously get responses for a list of prompts.
    """
    def __init__(self, model_id: str):
        """
        Initialize the APILLM with a specific model.

        Args:
            model_id (str): Identifier for the model to use.

        Raises:
            ValueError: If an unknown model identifier is provided.
        """
        if "claude" in model_id:
            ANTHROPIC_API_KEY = open("anthropictoken.txt", "r").read()
            self.model = ChatAnthropic(model=model_id, api_key=ANTHROPIC_API_KEY)
        elif "gpt" in model_id:
            OPENAI_API_KEY = open("openaitoken.txt", "r").read()
            self.model = ChatOpenAI(model=model_id, api_key=OPENAI_API_KEY)
        elif "llama" in model_id:
            DEEPINFRA_API_KEY = open("deepinfratoken.txt", "r").read()
            self.model = ChatDeepInfra(model_name=model_id, deepinfra_api_token=DEEPINFRA_API_KEY)
        else:
            raise ValueError(f"Unknown model: {model_id}")

    def get_response(self, prompts: List[str]) -&gt; List[str]:
        """
        Synchronously get responses for a list of prompts.

        This method includes retry logic for handling connection errors and rate limits.

        Args:
            prompts (list[str]): List of input prompts.

        Returns:
            list[str]: List of model responses.

        Raises:
            requests.exceptions.ConnectionError: If max retries are exceeded.
        """
        max_retries = 100
        delay = 3
        attempts = 0

        while attempts &lt; max_retries:
            try:
                responses = asyncio.run(self._get_response(prompts))
                return responses
            except requests.exceptions.ConnectionError as e:
                attempts += 1
                logger.critical(
                    f"Connection error: {e}. Attempt {attempts}/{max_retries}. Retrying in {delay} seconds..."
                )
                time.sleep(delay)
            except openai.RateLimitError as e:
                attempts += 1
                logger.critical(
                    f"Rate limit error: {e}. Attempt {attempts}/{max_retries}. Retrying in {delay} seconds..."
                )
                time.sleep(delay)

        # If the loop exits, it means max retries were reached
        raise requests.exceptions.ConnectionError("Max retries exceeded. Connection could not be established.")

    async def _get_response(
        self, prompts: list[str], max_concurrent_calls=200
    ) -&gt; list[str]:  
        """
        Asynchronously get responses for a list of prompts.

        This method uses a semaphore to limit the number of concurrent API calls.

        Args:
            prompts (list[str]): List of input prompts.
            max_concurrent_calls (int): Maximum number of concurrent API calls allowed.

        Returns:
            list[str]: List of model responses.
        """
        semaphore = asyncio.Semaphore(max_concurrent_calls)  # Limit the number of concurrent calls
        tasks = []

        for prompt in prompts:
            tasks.append(invoke_model(prompt, self.model, semaphore))

        responses = await asyncio.gather(*tasks)
        return responses</code></pre>
              </details>



  <div class="doc doc-children">









<div class="doc doc-object doc-function">


<h3 id="promptolution.llms.api_llm.APILLM.__init__" class="doc doc-heading">
            <code class="highlight language-python">__init__(model_id)</code>

</h3>


    <div class="doc doc-contents ">

        <p>Initialize the APILLM with a specific model.</p>


<p><span class="doc-section-title">Parameters:</span></p>
    <table>
      <thead>
        <tr>
          <th>Name</th>
          <th>Type</th>
          <th>Description</th>
          <th>Default</th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td><code>model_id</code></td>
            <td>
                  <code>str</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>Identifier for the model to use.</p>
              </div>
            </td>
            <td>
                <em>required</em>
            </td>
          </tr>
      </tbody>
    </table>


<p><span class="doc-section-title">Raises:</span></p>
    <table>
      <thead>
        <tr>
          <th>Type</th>
          <th>Description</th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td>
                  <code>ValueError</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>If an unknown model identifier is provided.</p>
              </div>
            </td>
          </tr>
      </tbody>
    </table>

            <details class="quote">
              <summary>Source code in <code>promptolution\llms\api_llm.py</code></summary>
              <pre class="highlight"><code class="language-python">def __init__(self, model_id: str):
    """
    Initialize the APILLM with a specific model.

    Args:
        model_id (str): Identifier for the model to use.

    Raises:
        ValueError: If an unknown model identifier is provided.
    """
    if "claude" in model_id:
        ANTHROPIC_API_KEY = open("anthropictoken.txt", "r").read()
        self.model = ChatAnthropic(model=model_id, api_key=ANTHROPIC_API_KEY)
    elif "gpt" in model_id:
        OPENAI_API_KEY = open("openaitoken.txt", "r").read()
        self.model = ChatOpenAI(model=model_id, api_key=OPENAI_API_KEY)
    elif "llama" in model_id:
        DEEPINFRA_API_KEY = open("deepinfratoken.txt", "r").read()
        self.model = ChatDeepInfra(model_name=model_id, deepinfra_api_token=DEEPINFRA_API_KEY)
    else:
        raise ValueError(f"Unknown model: {model_id}")</code></pre>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h3 id="promptolution.llms.api_llm.APILLM.get_response" class="doc doc-heading">
            <code class="highlight language-python">get_response(prompts)</code>

</h3>


    <div class="doc doc-contents ">

        <p>Synchronously get responses for a list of prompts.</p>
<p>This method includes retry logic for handling connection errors and rate limits.</p>


<p><span class="doc-section-title">Parameters:</span></p>
    <table>
      <thead>
        <tr>
          <th>Name</th>
          <th>Type</th>
          <th>Description</th>
          <th>Default</th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td><code>prompts</code></td>
            <td>
                  <code>list[str]</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>List of input prompts.</p>
              </div>
            </td>
            <td>
                <em>required</em>
            </td>
          </tr>
      </tbody>
    </table>


    <p><span class="doc-section-title">Returns:</span></p>
    <table>
      <thead>
        <tr>
          <th>Type</th>
          <th>Description</th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td>
                  <code><span title="typing.List">List</span>[str]</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>list[str]: List of model responses.</p>
              </div>
            </td>
          </tr>
      </tbody>
    </table>


<p><span class="doc-section-title">Raises:</span></p>
    <table>
      <thead>
        <tr>
          <th>Type</th>
          <th>Description</th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td>
                  <code><span title="requests.exceptions.ConnectionError">ConnectionError</span></code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>If max retries are exceeded.</p>
              </div>
            </td>
          </tr>
      </tbody>
    </table>

            <details class="quote">
              <summary>Source code in <code>promptolution\llms\api_llm.py</code></summary>
              <pre class="highlight"><code class="language-python">def get_response(self, prompts: List[str]) -&gt; List[str]:
    """
    Synchronously get responses for a list of prompts.

    This method includes retry logic for handling connection errors and rate limits.

    Args:
        prompts (list[str]): List of input prompts.

    Returns:
        list[str]: List of model responses.

    Raises:
        requests.exceptions.ConnectionError: If max retries are exceeded.
    """
    max_retries = 100
    delay = 3
    attempts = 0

    while attempts &lt; max_retries:
        try:
            responses = asyncio.run(self._get_response(prompts))
            return responses
        except requests.exceptions.ConnectionError as e:
            attempts += 1
            logger.critical(
                f"Connection error: {e}. Attempt {attempts}/{max_retries}. Retrying in {delay} seconds..."
            )
            time.sleep(delay)
        except openai.RateLimitError as e:
            attempts += 1
            logger.critical(
                f"Rate limit error: {e}. Attempt {attempts}/{max_retries}. Retrying in {delay} seconds..."
            )
            time.sleep(delay)

    # If the loop exits, it means max retries were reached
    raise requests.exceptions.ConnectionError("Max retries exceeded. Connection could not be established.")</code></pre>
            </details>
    </div>

</div>



  </div>

    </div>

</div>


<div class="doc doc-object doc-function">


<h2 id="promptolution.llms.api_llm.invoke_model" class="doc doc-heading">
            <code class="highlight language-python">invoke_model(prompt, model, semaphore)</code>

  <span class="doc doc-labels">
      <small class="doc doc-label doc-label-async"><code>async</code></small>
  </span>

</h2>


    <div class="doc doc-contents ">

        <p>Asynchronously invoke a language model with retry logic.</p>


<p><span class="doc-section-title">Parameters:</span></p>
    <table>
      <thead>
        <tr>
          <th>Name</th>
          <th>Type</th>
          <th>Description</th>
          <th>Default</th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td><code>prompt</code></td>
            <td>
                  <code>str</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>The input prompt for the model.</p>
              </div>
            </td>
            <td>
                <em>required</em>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td><code>model</code></td>
            <td>
            </td>
            <td>
              <div class="doc-md-description">
                <p>The language model to invoke.</p>
              </div>
            </td>
            <td>
                <em>required</em>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td><code>semaphore</code></td>
            <td>
                  <code><span title="asyncio.Semaphore">Semaphore</span></code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>Semaphore to limit concurrent calls.</p>
              </div>
            </td>
            <td>
                <em>required</em>
            </td>
          </tr>
      </tbody>
    </table>


    <p><span class="doc-section-title">Returns:</span></p>
    <table>
      <thead>
        <tr>
<th>Name</th>          <th>Type</th>
          <th>Description</th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
<td><code>str</code></td>            <td>
            </td>
            <td>
              <div class="doc-md-description">
                <p>The model's response content.</p>
              </div>
            </td>
          </tr>
      </tbody>
    </table>


<p><span class="doc-section-title">Raises:</span></p>
    <table>
      <thead>
        <tr>
          <th>Type</th>
          <th>Description</th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td>
                  <code><span title="langchain_community.chat_models.deepinfra.ChatDeepInfraException">ChatDeepInfraException</span></code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>If all retry attempts fail.</p>
              </div>
            </td>
          </tr>
      </tbody>
    </table>

            <details class="quote">
              <summary>Source code in <code>promptolution\llms\api_llm.py</code></summary>
              <pre class="highlight"><code class="language-python">async def invoke_model(prompt, model, semaphore):
    """
    Asynchronously invoke a language model with retry logic.

    Args:
        prompt (str): The input prompt for the model.
        model: The language model to invoke.
        semaphore (asyncio.Semaphore): Semaphore to limit concurrent calls.

    Returns:
        str: The model's response content.

    Raises:
        ChatDeepInfraException: If all retry attempts fail.
    """
    async with semaphore:
        max_retries = 100
        delay = 3
        attempts = 0

        while attempts &lt; max_retries:
            try:
                response = await asyncio.to_thread(model.invoke, [HumanMessage(content=prompt)])
                return response.content
            except ChatDeepInfraException as e:
                print(f"DeepInfra error: {e}. Attempt {attempts}/{max_retries}. Retrying in {delay} seconds...")
                attempts += 1
                time.sleep(delay)</code></pre>
            </details>
    </div>

</div>



  </div>

    </div>

</div><h2 id="base-llm">Base LLM</h2>


<div class="doc doc-object doc-module">



<a id="promptolution.llms.base_llm"></a>
    <div class="doc doc-contents first">



  <div class="doc doc-children">








<div class="doc doc-object doc-class">



<h2 id="promptolution.llms.base_llm.BaseLLM" class="doc doc-heading">
            <code>BaseLLM</code>


</h2>


    <div class="doc doc-contents ">
            <p class="doc doc-class-bases">
              Bases: <code><span title="abc.ABC">ABC</span></code></p>


        <p>Abstract base class for Language Models in the promptolution library.</p>
<p>This class defines the interface that all concrete LLM implementations should follow.</p>


<p><span class="doc-section-title">Methods:</span></p>
    <table>
      <thead>
        <tr>
          <th>Name</th>
          <th>Description</th>
        </tr>
      </thead>
      <tbody>
            <tr class="doc-section-item">
              <td><code><a class="autorefs autorefs-internal" title="promptolution.llms.base_llm.BaseLLM.get_response" href="#promptolution.llms.base_llm.BaseLLM.get_response">get_response</a></code></td>
              <td>
                <div class="doc-md-description">
                  <p>An abstract method that should be implemented by subclasses
          to generate responses for given prompts.</p>
                </div>
              </td>
            </tr>
      </tbody>
    </table>

              <details class="quote">
                <summary>Source code in <code>promptolution\llms\base_llm.py</code></summary>
                <pre class="highlight"><code class="language-python">class BaseLLM(ABC):
    """
    Abstract base class for Language Models in the promptolution library.

    This class defines the interface that all concrete LLM implementations should follow.

    Methods:
        get_response: An abstract method that should be implemented by subclasses
                      to generate responses for given prompts.
    """
    def __init__(self, *args, **kwargs):
        pass

    @abstractmethod
    def get_response(self, prompts: List[str]) -&gt; List[str]:
        """
        Generate responses for the given prompts.

        This method should be implemented by subclasses to define how
        the LLM generates responses.

        Args:
            prompts (List[str]): A list of input prompts.

        Returns:
            List[str]: A list of generated responses corresponding to the input prompts.
        """
        pass</code></pre>
              </details>



  <div class="doc doc-children">









<div class="doc doc-object doc-function">


<h3 id="promptolution.llms.base_llm.BaseLLM.get_response" class="doc doc-heading">
            <code class="highlight language-python">get_response(prompts)</code>

  <span class="doc doc-labels">
      <small class="doc doc-label doc-label-abstractmethod"><code>abstractmethod</code></small>
  </span>

</h3>


    <div class="doc doc-contents ">

        <p>Generate responses for the given prompts.</p>
<p>This method should be implemented by subclasses to define how
the LLM generates responses.</p>


<p><span class="doc-section-title">Parameters:</span></p>
    <table>
      <thead>
        <tr>
          <th>Name</th>
          <th>Type</th>
          <th>Description</th>
          <th>Default</th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td><code>prompts</code></td>
            <td>
                  <code><span title="typing.List">List</span>[str]</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>A list of input prompts.</p>
              </div>
            </td>
            <td>
                <em>required</em>
            </td>
          </tr>
      </tbody>
    </table>


    <p><span class="doc-section-title">Returns:</span></p>
    <table>
      <thead>
        <tr>
          <th>Type</th>
          <th>Description</th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td>
                  <code><span title="typing.List">List</span>[str]</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>List[str]: A list of generated responses corresponding to the input prompts.</p>
              </div>
            </td>
          </tr>
      </tbody>
    </table>

            <details class="quote">
              <summary>Source code in <code>promptolution\llms\base_llm.py</code></summary>
              <pre class="highlight"><code class="language-python">@abstractmethod
def get_response(self, prompts: List[str]) -&gt; List[str]:
    """
    Generate responses for the given prompts.

    This method should be implemented by subclasses to define how
    the LLM generates responses.

    Args:
        prompts (List[str]): A list of input prompts.

    Returns:
        List[str]: A list of generated responses corresponding to the input prompts.
    """
    pass</code></pre>
            </details>
    </div>

</div>



  </div>

    </div>

</div>

<div class="doc doc-object doc-class">



<h2 id="promptolution.llms.base_llm.DummyLLM" class="doc doc-heading">
            <code>DummyLLM</code>


</h2>


    <div class="doc doc-contents ">
            <p class="doc doc-class-bases">
              Bases: <code><a class="autorefs autorefs-internal" title="promptolution.llms.base_llm.BaseLLM" href="#promptolution.llms.base_llm.BaseLLM">BaseLLM</a></code></p>


        <p>A dummy implementation of the BaseLLM for testing purposes.</p>
<p>This class generates random responses for given prompts, simulating
the behavior of a language model without actually performing any
complex natural language processing.</p>

              <details class="quote">
                <summary>Source code in <code>promptolution\llms\base_llm.py</code></summary>
                <pre class="highlight"><code class="language-python">class DummyLLM(BaseLLM):
    """
    A dummy implementation of the BaseLLM for testing purposes.

    This class generates random responses for given prompts, simulating
    the behavior of a language model without actually performing any
    complex natural language processing.
    """
    def __init__(self, *args, **kwargs):
        pass

    def get_response(self, prompts: str) -&gt; str:
        """
        Generate random responses for the given prompts.

        This method creates silly, random responses enclosed in &lt;prompt&gt; tags.
        It's designed for testing and demonstration purposes.

        Args:
            prompts (str or List[str]): Input prompt(s). If a single string is provided,
                                        it's converted to a list containing that string.

        Returns:
            List[str]: A list of randomly generated responses, one for each input prompt.
        """
        if isinstance(prompts, str):
            prompts = [prompts]
        results = []
        for _ in prompts:
            r = np.random.rand()
            if r &lt; 0.3:
                results += [f"Joooo wazzuppp &lt;prompt&gt;hier gehts los {r} &lt;/prompt&gt;"]
            if 0.3 &lt;= r &lt; 0.6:
                results += [f"was das hier? &lt;prompt&gt;peter lustig{r}&lt;/prompt&gt;"]
            else:
                results += [f"hier ist ein &lt;prompt&gt;test{r}&lt;/prompt&gt;"]

        return results</code></pre>
              </details>



  <div class="doc doc-children">









<div class="doc doc-object doc-function">


<h3 id="promptolution.llms.base_llm.DummyLLM.get_response" class="doc doc-heading">
            <code class="highlight language-python">get_response(prompts)</code>

</h3>


    <div class="doc doc-contents ">

        <p>Generate random responses for the given prompts.</p>
<p>This method creates silly, random responses enclosed in <prompt> tags.
It's designed for testing and demonstration purposes.</p>


<p><span class="doc-section-title">Parameters:</span></p>
    <table>
      <thead>
        <tr>
          <th>Name</th>
          <th>Type</th>
          <th>Description</th>
          <th>Default</th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td><code>prompts</code></td>
            <td>
                  <code>str or <span title="typing.List">List</span>[str]</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>Input prompt(s). If a single string is provided,
                        it's converted to a list containing that string.</p>
              </div>
            </td>
            <td>
                <em>required</em>
            </td>
          </tr>
      </tbody>
    </table>


    <p><span class="doc-section-title">Returns:</span></p>
    <table>
      <thead>
        <tr>
          <th>Type</th>
          <th>Description</th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td>
                  <code>str</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>List[str]: A list of randomly generated responses, one for each input prompt.</p>
              </div>
            </td>
          </tr>
      </tbody>
    </table>

            <details class="quote">
              <summary>Source code in <code>promptolution\llms\base_llm.py</code></summary>
              <pre class="highlight"><code class="language-python">def get_response(self, prompts: str) -&gt; str:
    """
    Generate random responses for the given prompts.

    This method creates silly, random responses enclosed in &lt;prompt&gt; tags.
    It's designed for testing and demonstration purposes.

    Args:
        prompts (str or List[str]): Input prompt(s). If a single string is provided,
                                    it's converted to a list containing that string.

    Returns:
        List[str]: A list of randomly generated responses, one for each input prompt.
    """
    if isinstance(prompts, str):
        prompts = [prompts]
    results = []
    for _ in prompts:
        r = np.random.rand()
        if r &lt; 0.3:
            results += [f"Joooo wazzuppp &lt;prompt&gt;hier gehts los {r} &lt;/prompt&gt;"]
        if 0.3 &lt;= r &lt; 0.6:
            results += [f"was das hier? &lt;prompt&gt;peter lustig{r}&lt;/prompt&gt;"]
        else:
            results += [f"hier ist ein &lt;prompt&gt;test{r}&lt;/prompt&gt;"]

    return results</code></pre>
            </details>
    </div>

</div>



  </div>

    </div>

</div>




  </div>

    </div>

</div><h2 id="deepinfra-llm">DeepInfra LLM</h2>


<div class="doc doc-object doc-module">



<a id="promptolution.llms.deepinfra"></a>
    <div class="doc doc-contents first">



  <div class="doc doc-children">








<div class="doc doc-object doc-class">



<h2 id="promptolution.llms.deepinfra.ChatDeepInfra" class="doc doc-heading">
            <code>ChatDeepInfra</code>


</h2>


    <div class="doc doc-contents ">
            <p class="doc doc-class-bases">
              Bases: <code><span title="langchain_core.language_models.chat_models.BaseChatModel">BaseChatModel</span></code></p>


        <p>A chat model that uses the DeepInfra API.</p>

              <details class="quote">
                <summary>Source code in <code>promptolution\llms\deepinfra.py</code></summary>
                <pre class="highlight"><code class="language-python">class ChatDeepInfra(BaseChatModel):
    """A chat model that uses the DeepInfra API."""

    # client: Any  #: :meta private:
    model_name: str = Field(alias="model")
    """The model name to use for the chat model."""
    deepinfra_api_token: Optional[str] = None
    request_timeout: Optional[float] = Field(default=None, alias="timeout")
    temperature: Optional[float] = 1
    model_kwargs: Dict[str, Any] = Field(default_factory=dict)
    """Run inference with this temperature. Must be in the closed
       interval [0.0, 1.0]."""
    top_p: Optional[float] = None
    """Decode using nucleus sampling: consider the smallest set of tokens whose
       probability sum is at least top_p. Must be in the closed interval [0.0, 1.0]."""
    top_k: Optional[int] = None
    """Decode using top-k sampling: consider the set of top_k most probable tokens.
       Must be positive."""
    n: int = 1
    """Number of chat completions to generate for each prompt. Note that the API may
       not return the full n completions if duplicates are generated."""
    max_tokens: int = 256
    streaming: bool = False
    max_retries: int = 1

    def __init__(self, model_name: str, **kwargs: Any):
        super().__init__(model=model_name, **kwargs)

    @property
    def _default_params(self) -&gt; Dict[str, Any]:
        """Get the default parameters for calling OpenAI API."""
        return {
            "model": self.model_name,
            "max_tokens": self.max_tokens,
            "stream": self.streaming,
            "n": self.n,
            "temperature": self.temperature,
            "request_timeout": self.request_timeout,
            **self.model_kwargs,
        }

    @property
    def _client_params(self) -&gt; Dict[str, Any]:
        """Get the parameters used for the openai client."""
        return {**self._default_params}

    def completion_with_retry(self, run_manager: Optional[CallbackManagerForLLMRun] = None, **kwargs: Any) -&gt; Any:
        """Use tenacity to retry the completion call."""
        retry_decorator = _create_retry_decorator(self, run_manager=run_manager)

        @retry_decorator
        def _completion_with_retry(**kwargs: Any) -&gt; Any:
            try:
                request_timeout = kwargs.pop("request_timeout")
                request = Requests(headers=self._headers())
                response = request.post(url=self._url(), data=self._body(kwargs), timeout=request_timeout)
                self._handle_status(response.status_code, response.text)
                return response
            except Exception as e:
                # import pdb; pdb.set_trace()
                print("EX", e)  # noqa: T201
                raise

        return _completion_with_retry(**kwargs)

    async def acompletion_with_retry(
        self,
        run_manager: Optional[AsyncCallbackManagerForLLMRun] = None,
        **kwargs: Any,
    ) -&gt; Any:
        """Use tenacity to retry the async completion call."""
        retry_decorator = _create_retry_decorator(self, run_manager=run_manager)

        @retry_decorator
        async def _completion_with_retry(**kwargs: Any) -&gt; Any:
            try:
                request_timeout = kwargs.pop("request_timeout")
                request = Requests(headers=self._headers())
                async with request.apost(url=self._url(), data=self._body(kwargs), timeout=request_timeout) as response:
                    self._handle_status(response.status, response.text)
                    return await response.json()
            except Exception as e:
                print("EX", e)  # noqa: T201
                raise

        return await _completion_with_retry(**kwargs)

    @root_validator(pre=True)
    def init_defaults(cls, values: Dict) -&gt; Dict:
        """Validate api key, python package exists, temperature, top_p, and top_k."""
        # For compatibility with LiteLLM
        api_key = get_from_dict_or_env(
            values,
            "deepinfra_api_key",
            "DEEPINFRA_API_KEY",
            default="",
        )
        values["deepinfra_api_token"] = get_from_dict_or_env(
            values,
            "deepinfra_api_token",
            "DEEPINFRA_API_TOKEN",
            default=api_key,
        )
        # set model id
        # values["model_name"] = get_from_dict_or_env(
        #     values,
        #     "model_name",
        #     "DEEPINFRA_MODEL_NAME",
        #     default="",
        # )
        return values

    @root_validator(pre=False, skip_on_failure=True)
    def validate_environment(cls, values: Dict) -&gt; Dict:
        if values["temperature"] is not None and not 0 &lt;= values["temperature"] &lt;= 1:
            raise ValueError("temperature must be in the range [0.0, 1.0]")

        if values["top_p"] is not None and not 0 &lt;= values["top_p"] &lt;= 1:
            raise ValueError("top_p must be in the range [0.0, 1.0]")

        if values["top_k"] is not None and values["top_k"] &lt;= 0:
            raise ValueError("top_k must be positive")

        return values

    def _generate(
        self,
        messages: List[BaseMessage],
        stop: Optional[List[str]] = None,
        run_manager: Optional[CallbackManagerForLLMRun] = None,
        stream: Optional[bool] = None,
        **kwargs: Any,
    ) -&gt; ChatResult:
        should_stream = stream if stream is not None else self.streaming
        if should_stream:
            stream_iter = self._stream(messages, stop=stop, run_manager=run_manager, **kwargs)
            return generate_from_stream(stream_iter)

        message_dicts, params = self._create_message_dicts(messages, stop)
        params = {**params, **kwargs}
        response = self.completion_with_retry(messages=message_dicts, run_manager=run_manager, **params)
        return self._create_chat_result(response.json())

    def _create_chat_result(self, response: Mapping[str, Any]) -&gt; ChatResult:
        generations = []
        for res in response["choices"]:
            message = _convert_dict_to_message(res["message"])
            gen = ChatGeneration(
                message=message,
                generation_info=dict(finish_reason=res.get("finish_reason")),
            )
            generations.append(gen)
        token_usage = response.get("usage", {})
        llm_output = {"token_usage": token_usage, "model": self.model_name}
        res = ChatResult(generations=generations, llm_output=llm_output)
        return res

    def _create_message_dicts(
        self, messages: List[BaseMessage], stop: Optional[List[str]]
    ) -&gt; Tuple[List[Dict[str, Any]], Dict[str, Any]]:
        params = self._client_params
        if stop is not None:
            if "stop" in params:
                raise ValueError("`stop` found in both the input and default params.")
            params["stop"] = stop
        message_dicts = [_convert_message_to_dict(m) for m in messages]
        return message_dicts, params

    def _stream(
        self,
        messages: List[BaseMessage],
        stop: Optional[List[str]] = None,
        run_manager: Optional[CallbackManagerForLLMRun] = None,
        **kwargs: Any,
    ) -&gt; Iterator[ChatGenerationChunk]:
        message_dicts, params = self._create_message_dicts(messages, stop)
        params = {**params, **kwargs, "stream": True}

        response = self.completion_with_retry(messages=message_dicts, run_manager=run_manager, **params)
        for line in _parse_stream(response.iter_lines()):
            chunk = _handle_sse_line(line)
            if chunk:
                cg_chunk = ChatGenerationChunk(message=chunk, generation_info=None)
                if run_manager:
                    run_manager.on_llm_new_token(str(chunk.content), chunk=cg_chunk)
                yield cg_chunk

    async def _astream(
        self,
        messages: List[BaseMessage],
        stop: Optional[List[str]] = None,
        run_manager: Optional[AsyncCallbackManagerForLLMRun] = None,
        **kwargs: Any,
    ) -&gt; AsyncIterator[ChatGenerationChunk]:
        message_dicts, params = self._create_message_dicts(messages, stop)
        params = {"messages": message_dicts, "stream": True, **params, **kwargs}

        request_timeout = params.pop("request_timeout")
        request = Requests(headers=self._headers())
        async with request.apost(url=self._url(), data=self._body(params), timeout=request_timeout) as response:
            async for line in _parse_stream_async(response.content):
                chunk = _handle_sse_line(line)
                if chunk:
                    cg_chunk = ChatGenerationChunk(message=chunk, generation_info=None)
                    if run_manager:
                        await run_manager.on_llm_new_token(str(chunk.content), chunk=cg_chunk)
                    yield cg_chunk

    async def _agenerate(
        self,
        messages: List[BaseMessage],
        stop: Optional[List[str]] = None,
        run_manager: Optional[AsyncCallbackManagerForLLMRun] = None,
        stream: Optional[bool] = None,
        **kwargs: Any,
    ) -&gt; ChatResult:
        should_stream = stream if stream is not None else self.streaming
        if should_stream:
            stream_iter = self._astream(messages, stop=stop, run_manager=run_manager, **kwargs)
            return await agenerate_from_stream(stream_iter)

        message_dicts, params = self._create_message_dicts(messages, stop)
        params = {"messages": message_dicts, **params, **kwargs}

        res = await self.acompletion_with_retry(run_manager=run_manager, **params)
        return self._create_chat_result(res)

    @property
    def _identifying_params(self) -&gt; Dict[str, Any]:
        """Get the identifying parameters."""
        return {
            "model": self.model_name,
            "temperature": self.temperature,
            "top_p": self.top_p,
            "top_k": self.top_k,
            "n": self.n,
        }

    @property
    def _llm_type(self) -&gt; str:
        return "deepinfra-chat"

    def _handle_status(self, code: int, text: Any) -&gt; None:
        if code &gt;= 500:
            raise ChatDeepInfraException(f"DeepInfra Server: Error {code}")
        elif code &gt;= 400:
            raise ValueError(f"DeepInfra received an invalid payload: {text}")
        elif code != 200:
            raise Exception(f"DeepInfra returned an unexpected response with status " f"{code}: {text}")

    def _url(self) -&gt; str:
        return "https://stage.api.deepinfra.com/v1/openai/chat/completions"

    def _headers(self) -&gt; Dict:
        return {
            "Authorization": f"bearer {self.deepinfra_api_token}",
            "Content-Type": "application/json",
        }

    def _body(self, kwargs: Any) -&gt; Dict:
        return kwargs

    def bind_tools(
        self,
        tools: Sequence[Union[Dict[str, Any], Type[BaseModel], Callable, BaseTool]],
        **kwargs: Any,
    ) -&gt; Runnable[LanguageModelInput, BaseMessage]:
        """Bind tool-like objects to this chat model.

        Assumes model is compatible with OpenAI tool-calling API.

        Args:
            tools: A list of tool definitions to bind to this chat model.
                Can be  a dictionary, pydantic model, callable, or BaseTool. Pydantic
                models, callables, and BaseTools will be automatically converted to
                their schema dictionary representation.
            **kwargs: Any additional parameters to pass to the
                :class:`~langchain.runnable.Runnable` constructor.
        """

        formatted_tools = [convert_to_openai_tool(tool) for tool in tools]
        return super().bind(tools=formatted_tools, **kwargs)</code></pre>
              </details>



  <div class="doc doc-children">







<div class="doc doc-object doc-attribute">



<h3 id="promptolution.llms.deepinfra.ChatDeepInfra.model_kwargs" class="doc doc-heading">
            <code class="highlight language-python">model_kwargs: Dict[str, Any] = Field(default_factory=dict)</code>

  <span class="doc doc-labels">
      <small class="doc doc-label doc-label-class-attribute"><code>class-attribute</code></small>
      <small class="doc doc-label doc-label-instance-attribute"><code>instance-attribute</code></small>
  </span>

</h3>


    <div class="doc doc-contents ">

        <p>Run inference with this temperature. Must be in the closed
interval [0.0, 1.0].</p>
    </div>

</div>

<div class="doc doc-object doc-attribute">



<h3 id="promptolution.llms.deepinfra.ChatDeepInfra.model_name" class="doc doc-heading">
            <code class="highlight language-python">model_name: str = Field(alias='model')</code>

  <span class="doc doc-labels">
      <small class="doc doc-label doc-label-class-attribute"><code>class-attribute</code></small>
      <small class="doc doc-label doc-label-instance-attribute"><code>instance-attribute</code></small>
  </span>

</h3>


    <div class="doc doc-contents ">

        <p>The model name to use for the chat model.</p>
    </div>

</div>

<div class="doc doc-object doc-attribute">



<h3 id="promptolution.llms.deepinfra.ChatDeepInfra.n" class="doc doc-heading">
            <code class="highlight language-python">n: int = 1</code>

  <span class="doc doc-labels">
      <small class="doc doc-label doc-label-class-attribute"><code>class-attribute</code></small>
      <small class="doc doc-label doc-label-instance-attribute"><code>instance-attribute</code></small>
  </span>

</h3>


    <div class="doc doc-contents ">

        <p>Number of chat completions to generate for each prompt. Note that the API may
not return the full n completions if duplicates are generated.</p>
    </div>

</div>

<div class="doc doc-object doc-attribute">



<h3 id="promptolution.llms.deepinfra.ChatDeepInfra.top_k" class="doc doc-heading">
            <code class="highlight language-python">top_k: Optional[int] = None</code>

  <span class="doc doc-labels">
      <small class="doc doc-label doc-label-class-attribute"><code>class-attribute</code></small>
      <small class="doc doc-label doc-label-instance-attribute"><code>instance-attribute</code></small>
  </span>

</h3>


    <div class="doc doc-contents ">

        <p>Decode using top-k sampling: consider the set of top_k most probable tokens.
Must be positive.</p>
    </div>

</div>

<div class="doc doc-object doc-attribute">



<h3 id="promptolution.llms.deepinfra.ChatDeepInfra.top_p" class="doc doc-heading">
            <code class="highlight language-python">top_p: Optional[float] = None</code>

  <span class="doc doc-labels">
      <small class="doc doc-label doc-label-class-attribute"><code>class-attribute</code></small>
      <small class="doc doc-label doc-label-instance-attribute"><code>instance-attribute</code></small>
  </span>

</h3>


    <div class="doc doc-contents ">

        <p>Decode using nucleus sampling: consider the smallest set of tokens whose
probability sum is at least top_p. Must be in the closed interval [0.0, 1.0].</p>
    </div>

</div>



<div class="doc doc-object doc-function">


<h3 id="promptolution.llms.deepinfra.ChatDeepInfra.acompletion_with_retry" class="doc doc-heading">
            <code class="highlight language-python">acompletion_with_retry(run_manager=None, **kwargs)</code>

  <span class="doc doc-labels">
      <small class="doc doc-label doc-label-async"><code>async</code></small>
  </span>

</h3>


    <div class="doc doc-contents ">

        <p>Use tenacity to retry the async completion call.</p>

            <details class="quote">
              <summary>Source code in <code>promptolution\llms\deepinfra.py</code></summary>
              <pre class="highlight"><code class="language-python">async def acompletion_with_retry(
    self,
    run_manager: Optional[AsyncCallbackManagerForLLMRun] = None,
    **kwargs: Any,
) -&gt; Any:
    """Use tenacity to retry the async completion call."""
    retry_decorator = _create_retry_decorator(self, run_manager=run_manager)

    @retry_decorator
    async def _completion_with_retry(**kwargs: Any) -&gt; Any:
        try:
            request_timeout = kwargs.pop("request_timeout")
            request = Requests(headers=self._headers())
            async with request.apost(url=self._url(), data=self._body(kwargs), timeout=request_timeout) as response:
                self._handle_status(response.status, response.text)
                return await response.json()
        except Exception as e:
            print("EX", e)  # noqa: T201
            raise

    return await _completion_with_retry(**kwargs)</code></pre>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h3 id="promptolution.llms.deepinfra.ChatDeepInfra.bind_tools" class="doc doc-heading">
            <code class="highlight language-python">bind_tools(tools, **kwargs)</code>

</h3>


    <div class="doc doc-contents ">

        <p>Bind tool-like objects to this chat model.</p>
<p>Assumes model is compatible with OpenAI tool-calling API.</p>


<p><span class="doc-section-title">Parameters:</span></p>
    <table>
      <thead>
        <tr>
          <th>Name</th>
          <th>Type</th>
          <th>Description</th>
          <th>Default</th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td><code>tools</code></td>
            <td>
                  <code><span title="typing.Sequence">Sequence</span>[<span title="typing.Union">Union</span>[<span title="typing.Dict">Dict</span>[str, <span title="typing.Any">Any</span>], <span title="typing.Type">Type</span>[<span title="langchain_core.pydantic_v1.BaseModel">BaseModel</span>], <span title="typing.Callable">Callable</span>, <span title="langchain_core.tools.BaseTool">BaseTool</span>]]</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>A list of tool definitions to bind to this chat model.
Can be  a dictionary, pydantic model, callable, or BaseTool. Pydantic
models, callables, and BaseTools will be automatically converted to
their schema dictionary representation.</p>
              </div>
            </td>
            <td>
                <em>required</em>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td><code>**kwargs</code></td>
            <td>
                  <code><span title="typing.Any">Any</span></code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>Any additional parameters to pass to the
:class:<code>~langchain.runnable.Runnable</code> constructor.</p>
              </div>
            </td>
            <td>
                  <code>{}</code>
            </td>
          </tr>
      </tbody>
    </table>

            <details class="quote">
              <summary>Source code in <code>promptolution\llms\deepinfra.py</code></summary>
              <pre class="highlight"><code class="language-python">def bind_tools(
    self,
    tools: Sequence[Union[Dict[str, Any], Type[BaseModel], Callable, BaseTool]],
    **kwargs: Any,
) -&gt; Runnable[LanguageModelInput, BaseMessage]:
    """Bind tool-like objects to this chat model.

    Assumes model is compatible with OpenAI tool-calling API.

    Args:
        tools: A list of tool definitions to bind to this chat model.
            Can be  a dictionary, pydantic model, callable, or BaseTool. Pydantic
            models, callables, and BaseTools will be automatically converted to
            their schema dictionary representation.
        **kwargs: Any additional parameters to pass to the
            :class:`~langchain.runnable.Runnable` constructor.
    """

    formatted_tools = [convert_to_openai_tool(tool) for tool in tools]
    return super().bind(tools=formatted_tools, **kwargs)</code></pre>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h3 id="promptolution.llms.deepinfra.ChatDeepInfra.completion_with_retry" class="doc doc-heading">
            <code class="highlight language-python">completion_with_retry(run_manager=None, **kwargs)</code>

</h3>


    <div class="doc doc-contents ">

        <p>Use tenacity to retry the completion call.</p>

            <details class="quote">
              <summary>Source code in <code>promptolution\llms\deepinfra.py</code></summary>
              <pre class="highlight"><code class="language-python">def completion_with_retry(self, run_manager: Optional[CallbackManagerForLLMRun] = None, **kwargs: Any) -&gt; Any:
    """Use tenacity to retry the completion call."""
    retry_decorator = _create_retry_decorator(self, run_manager=run_manager)

    @retry_decorator
    def _completion_with_retry(**kwargs: Any) -&gt; Any:
        try:
            request_timeout = kwargs.pop("request_timeout")
            request = Requests(headers=self._headers())
            response = request.post(url=self._url(), data=self._body(kwargs), timeout=request_timeout)
            self._handle_status(response.status_code, response.text)
            return response
        except Exception as e:
            # import pdb; pdb.set_trace()
            print("EX", e)  # noqa: T201
            raise

    return _completion_with_retry(**kwargs)</code></pre>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h3 id="promptolution.llms.deepinfra.ChatDeepInfra.init_defaults" class="doc doc-heading">
            <code class="highlight language-python">init_defaults(values)</code>

</h3>


    <div class="doc doc-contents ">

        <p>Validate api key, python package exists, temperature, top_p, and top_k.</p>

            <details class="quote">
              <summary>Source code in <code>promptolution\llms\deepinfra.py</code></summary>
              <pre class="highlight"><code class="language-python">@root_validator(pre=True)
def init_defaults(cls, values: Dict) -&gt; Dict:
    """Validate api key, python package exists, temperature, top_p, and top_k."""
    # For compatibility with LiteLLM
    api_key = get_from_dict_or_env(
        values,
        "deepinfra_api_key",
        "DEEPINFRA_API_KEY",
        default="",
    )
    values["deepinfra_api_token"] = get_from_dict_or_env(
        values,
        "deepinfra_api_token",
        "DEEPINFRA_API_TOKEN",
        default=api_key,
    )
    # set model id
    # values["model_name"] = get_from_dict_or_env(
    #     values,
    #     "model_name",
    #     "DEEPINFRA_MODEL_NAME",
    #     default="",
    # )
    return values</code></pre>
            </details>
    </div>

</div>



  </div>

    </div>

</div>




  </div>

    </div>

</div><h2 id="local-llm">Local LLM</h2>


<div class="doc doc-object doc-module">



<a id="promptolution.llms.local_llm"></a>
    <div class="doc doc-contents first">



  <div class="doc doc-children">








<div class="doc doc-object doc-class">



<h2 id="promptolution.llms.local_llm.LocalLLM" class="doc doc-heading">
            <code>LocalLLM</code>


</h2>


    <div class="doc doc-contents ">


        <p>A class for running language models locally using the Hugging Face Transformers library.</p>
<p>This class sets up a text generation pipeline with specified model parameters
and provides a method to generate responses for given prompts.</p>


<p><span class="doc-section-title">Attributes:</span></p>
    <table>
      <thead>
        <tr>
          <th>Name</th>
          <th>Type</th>
          <th>Description</th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td><code><span title="promptolution.llms.local_llm.LocalLLM.pipeline">pipeline</span></code></td>
            <td>
                  <code><span title="transformers.Pipeline">Pipeline</span></code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>The text generation pipeline.</p>
              </div>
            </td>
          </tr>
      </tbody>
    </table>


<p><span class="doc-section-title">Methods:</span></p>
    <table>
      <thead>
        <tr>
          <th>Name</th>
          <th>Description</th>
        </tr>
      </thead>
      <tbody>
            <tr class="doc-section-item">
              <td><code><a class="autorefs autorefs-internal" title="promptolution.llms.local_llm.LocalLLM.get_response" href="#promptolution.llms.local_llm.LocalLLM.get_response">get_response</a></code></td>
              <td>
                <div class="doc-md-description">
                  <p>Generate responses for a list of prompts.</p>
                </div>
              </td>
            </tr>
      </tbody>
    </table>

              <details class="quote">
                <summary>Source code in <code>promptolution\llms\local_llm.py</code></summary>
                <pre class="highlight"><code class="language-python">class LocalLLM:
    """
    A class for running language models locally using the Hugging Face Transformers library.

    This class sets up a text generation pipeline with specified model parameters
    and provides a method to generate responses for given prompts.

    Attributes:
        pipeline (transformers.Pipeline): The text generation pipeline.

    Methods:
        get_response: Generate responses for a list of prompts.
    """
    def __init__(self, model_id: str, batch_size=8):
        """
        Initialize the LocalLLM with a specific model.

        Args:
            model_id (str): The identifier of the model to use (e.g., "gpt2", "facebook/opt-1.3b").
            batch_size (int, optional): The batch size for text generation. Defaults to 8.

        Note:
            This method sets up a text generation pipeline with bfloat16 precision,
            automatic device mapping, and specific generation parameters.
        """
        self.pipeline = transformers.pipeline(
            "text-generation",
            model=model_id,
            model_kwargs={"torch_dtype": torch.bfloat16},
            device_map="auto",
            max_new_tokens=256,
            batch_size=batch_size,
            num_return_sequences=1,
            return_full_text=False,
        )
        self.pipeline.tokenizer.pad_token_id = self.pipeline.tokenizer.eos_token_id
        self.pipeline.tokenizer.padding_side = "left"

    def get_response(self, prompts: list[str]):
        """
        Generate responses for a list of prompts using the local language model.

        Args:
            prompts (list[str]): A list of input prompts.

        Returns:
            list[str]: A list of generated responses corresponding to the input prompts.

        Note:
            This method uses torch.no_grad() for inference to reduce memory usage.
            It handles both single and batch inputs, ensuring consistent output format.
        """
        with torch.no_grad():
            response = self.pipeline(prompts, pad_token_id=self.pipeline.tokenizer.eos_token_id)

        if len(response) != 1:
            response = [r[0] if isinstance(r, list) else r for r in response]

        response = [r["generated_text"] for r in response]
        return response

    def __del__(self):
        try:
            del self.pipeline
            torch.cuda.empty_cache()
        except Exception as e:
            logger.warning(f"Error during LocalLLM cleanup: {e}")</code></pre>
              </details>



  <div class="doc doc-children">









<div class="doc doc-object doc-function">


<h3 id="promptolution.llms.local_llm.LocalLLM.__init__" class="doc doc-heading">
            <code class="highlight language-python">__init__(model_id, batch_size=8)</code>

</h3>


    <div class="doc doc-contents ">

        <p>Initialize the LocalLLM with a specific model.</p>


<p><span class="doc-section-title">Parameters:</span></p>
    <table>
      <thead>
        <tr>
          <th>Name</th>
          <th>Type</th>
          <th>Description</th>
          <th>Default</th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td><code>model_id</code></td>
            <td>
                  <code>str</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>The identifier of the model to use (e.g., "gpt2", "facebook/opt-1.3b").</p>
              </div>
            </td>
            <td>
                <em>required</em>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td><code>batch_size</code></td>
            <td>
                  <code>int</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>The batch size for text generation. Defaults to 8.</p>
              </div>
            </td>
            <td>
                  <code>8</code>
            </td>
          </tr>
      </tbody>
    </table>


<details class="note" open>
  <summary>Note</summary>
  <p>This method sets up a text generation pipeline with bfloat16 precision,
automatic device mapping, and specific generation parameters.</p>
</details>
            <details class="quote">
              <summary>Source code in <code>promptolution\llms\local_llm.py</code></summary>
              <pre class="highlight"><code class="language-python">def __init__(self, model_id: str, batch_size=8):
    """
    Initialize the LocalLLM with a specific model.

    Args:
        model_id (str): The identifier of the model to use (e.g., "gpt2", "facebook/opt-1.3b").
        batch_size (int, optional): The batch size for text generation. Defaults to 8.

    Note:
        This method sets up a text generation pipeline with bfloat16 precision,
        automatic device mapping, and specific generation parameters.
    """
    self.pipeline = transformers.pipeline(
        "text-generation",
        model=model_id,
        model_kwargs={"torch_dtype": torch.bfloat16},
        device_map="auto",
        max_new_tokens=256,
        batch_size=batch_size,
        num_return_sequences=1,
        return_full_text=False,
    )
    self.pipeline.tokenizer.pad_token_id = self.pipeline.tokenizer.eos_token_id
    self.pipeline.tokenizer.padding_side = "left"</code></pre>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h3 id="promptolution.llms.local_llm.LocalLLM.get_response" class="doc doc-heading">
            <code class="highlight language-python">get_response(prompts)</code>

</h3>


    <div class="doc doc-contents ">

        <p>Generate responses for a list of prompts using the local language model.</p>


<p><span class="doc-section-title">Parameters:</span></p>
    <table>
      <thead>
        <tr>
          <th>Name</th>
          <th>Type</th>
          <th>Description</th>
          <th>Default</th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td><code>prompts</code></td>
            <td>
                  <code>list[str]</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>A list of input prompts.</p>
              </div>
            </td>
            <td>
                <em>required</em>
            </td>
          </tr>
      </tbody>
    </table>


    <p><span class="doc-section-title">Returns:</span></p>
    <table>
      <thead>
        <tr>
          <th>Type</th>
          <th>Description</th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td>
            </td>
            <td>
              <div class="doc-md-description">
                <p>list[str]: A list of generated responses corresponding to the input prompts.</p>
              </div>
            </td>
          </tr>
      </tbody>
    </table>


<details class="note" open>
  <summary>Note</summary>
  <p>This method uses torch.no_grad() for inference to reduce memory usage.
It handles both single and batch inputs, ensuring consistent output format.</p>
</details>
            <details class="quote">
              <summary>Source code in <code>promptolution\llms\local_llm.py</code></summary>
              <pre class="highlight"><code class="language-python">def get_response(self, prompts: list[str]):
    """
    Generate responses for a list of prompts using the local language model.

    Args:
        prompts (list[str]): A list of input prompts.

    Returns:
        list[str]: A list of generated responses corresponding to the input prompts.

    Note:
        This method uses torch.no_grad() for inference to reduce memory usage.
        It handles both single and batch inputs, ensuring consistent output format.
    """
    with torch.no_grad():
        response = self.pipeline(prompts, pad_token_id=self.pipeline.tokenizer.eos_token_id)

    if len(response) != 1:
        response = [r[0] if isinstance(r, list) else r for r in response]

    response = [r["generated_text"] for r in response]
    return response</code></pre>
            </details>
    </div>

</div>



  </div>

    </div>

</div>




  </div>

    </div>

</div></div>
            </div>
        </div>

        <footer class="col-md-12">
            <hr>
                <p>&copy; 2024 <a href="https://github.com/yourusername"  target="_blank" rel="noopener">Tom Zehle, Timo Heiß, Moritz Schlager</a></p>
            <p>Documentation built with <a href="https://www.mkdocs.org/">MkDocs</a>.</p>
        </footer>
        <script src="../../js/bootstrap.bundle.min.js"></script>
        <script>
            var base_url = "../..",
                shortcuts = {"help": 191, "next": 78, "previous": 80, "search": 83};
        </script>
        <script src="../../js/base.js"></script>
        <script src="../../search/main.js"></script>

        <div class="modal" id="mkdocs_search_modal" tabindex="-1" role="dialog" aria-labelledby="searchModalLabel" aria-hidden="true">
    <div class="modal-dialog modal-lg">
        <div class="modal-content">
            <div class="modal-header">
                <h4 class="modal-title" id="searchModalLabel">Search</h4>
                <button type="button" class="btn-close" data-bs-dismiss="modal" aria-label="Close"></button>
            </div>
            <div class="modal-body">
                <p>From here you can search these documents. Enter your search terms below.</p>
                <form>
                    <div class="form-group">
                        <input type="search" class="form-control" placeholder="Search..." id="mkdocs-search-query" title="Type search term here">
                    </div>
                </form>
                <div id="mkdocs-search-results" data-no-results-text="No results found"></div>
            </div>
            <div class="modal-footer">
            </div>
        </div>
    </div>
</div><div class="modal" id="mkdocs_keyboard_modal" tabindex="-1" role="dialog" aria-labelledby="keyboardModalLabel" aria-hidden="true">
    <div class="modal-dialog">
        <div class="modal-content">
            <div class="modal-header">
                <h4 class="modal-title" id="keyboardModalLabel">Keyboard Shortcuts</h4>
                <button type="button" class="btn-close" data-bs-dismiss="modal" aria-label="Close"></button>
            </div>
            <div class="modal-body">
              <table class="table">
                <thead>
                  <tr>
                    <th style="width: 20%;">Keys</th>
                    <th>Action</th>
                  </tr>
                </thead>
                <tbody>
                  <tr>
                    <td class="help shortcut"><kbd>?</kbd></td>
                    <td>Open this help</td>
                  </tr>
                  <tr>
                    <td class="next shortcut"><kbd>n</kbd></td>
                    <td>Next page</td>
                  </tr>
                  <tr>
                    <td class="prev shortcut"><kbd>p</kbd></td>
                    <td>Previous page</td>
                  </tr>
                  <tr>
                    <td class="search shortcut"><kbd>s</kbd></td>
                    <td>Search</td>
                  </tr>
                </tbody>
              </table>
            </div>
            <div class="modal-footer">
            </div>
        </div>
    </div>
</div>

    </body>
</html>
