{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Welcome to Promptolution Promptolution is a library for optimizing prompts for large language models. Features Various LLM implementations Optimization algorithms for prompt tuning Task-specific modules Flexible configuration options Getting Started To get started with Promptolution, check out our API reference for detailed information on each module. API Reference LLMs Optimizers Predictors Tasks Callbacks Config","title":"Home"},{"location":"#welcome-to-promptolution","text":"Promptolution is a library for optimizing prompts for large language models.","title":"Welcome to Promptolution"},{"location":"#features","text":"Various LLM implementations Optimization algorithms for prompt tuning Task-specific modules Flexible configuration options","title":"Features"},{"location":"#getting-started","text":"To get started with Promptolution, check out our API reference for detailed information on each module.","title":"Getting Started"},{"location":"#api-reference","text":"LLMs Optimizers Predictors Tasks Callbacks Config","title":"API Reference"},{"location":"api/callbacks/","text":"Callbacks This module contains callback implementations for various purposes. BestPromptCallback Bases: Callback Callback for tracking the best prompt during optimization. This callback keeps track of the prompt with the highest score. Attributes: Name Type Description best_prompt str The prompt with the highest score so far. best_score float The highest score achieved so far. Source code in promptolution\\callbacks.py class BestPromptCallback(Callback): \"\"\" Callback for tracking the best prompt during optimization. This callback keeps track of the prompt with the highest score. Attributes: best_prompt (str): The prompt with the highest score so far. best_score (float): The highest score achieved so far. \"\"\" def __init__(self): self.best_prompt = \"\" self.best_score = -99999 def on_step_end(self, optimizer): if optimizer.scores[0] > self.best_score: self.best_score = optimizer.scores[0] self.best_prompt = optimizer.prompts[0] def get_best_prompt(self): return self.best_prompt, self.best_score CSVCallback Bases: Callback Callback for saving optimization progress to a CSV file. This callback saves prompts and scores at each step to a CSV file. Attributes: Name Type Description path str The path to the CSV file. step int The current step number. Source code in promptolution\\callbacks.py class CSVCallback(Callback): \"\"\" Callback for saving optimization progress to a CSV file. This callback saves prompts and scores at each step to a CSV file. Attributes: path (str): The path to the CSV file. step (int): The current step number. \"\"\" def __init__(self, path): # if dir does not exist if not os.path.exists(os.path.dirname(path)): os.makedirs(os.path.dirname(path)) # create file in path with header: \"step,prompt,score\" with open(path, \"w\") as f: f.write(\"step,prompt,score\\n\") self.path = path self.step = 0 def on_step_end(self, optimizer): \"\"\" Save prompts and scores to csv \"\"\" self.step += 1 df = pd.DataFrame( {\"step\": [self.step] * len(optimizer.prompts), \"prompt\": optimizer.prompts, \"score\": optimizer.scores} ) df.to_csv(self.path, mode=\"a\", header=False, index=False) def on_train_end(self, logs=None): pass on_step_end(optimizer) Save prompts and scores to csv Source code in promptolution\\callbacks.py def on_step_end(self, optimizer): \"\"\" Save prompts and scores to csv \"\"\" self.step += 1 df = pd.DataFrame( {\"step\": [self.step] * len(optimizer.prompts), \"prompt\": optimizer.prompts, \"score\": optimizer.scores} ) df.to_csv(self.path, mode=\"a\", header=False, index=False) LoggerCallback Bases: Callback Callback for logging optimization progress. This callback logs information about each step, epoch, and the end of training. Attributes: Name Type Description logger The logger object to use for logging. step int The current step number. Source code in promptolution\\callbacks.py class LoggerCallback(Callback): \"\"\" Callback for logging optimization progress. This callback logs information about each step, epoch, and the end of training. Attributes: logger: The logger object to use for logging. step (int): The current step number. \"\"\" def __init__(self, logger): self.logger = logger self.step = 0 def on_step_end(self, optimizer): self.step += 1 self.logger.critical(f\"\u2728Step {self.step} ended\u2728\") for i, (prompt, score) in enumerate(zip(optimizer.prompts, optimizer.scores)): self.logger.critical(f\"*** Prompt {i}: Score: {score}\") self.logger.critical(f\"{prompt}\") def on_epoch_end(self, epoch, logs=None): self.logger.critical(f\"Epoch {epoch} - {logs}\") def on_train_end(self, logs=None): self.logger.critical(f\"Training ended - {logs}\") ProgressBarCallback Bases: Callback Callback for displaying a progress bar during optimization. This callback uses tqdm to display a progress bar that updates at each step. Attributes: Name Type Description pbar tqdm The tqdm progress bar object. Source code in promptolution\\callbacks.py class ProgressBarCallback(Callback): \"\"\" Callback for displaying a progress bar during optimization. This callback uses tqdm to display a progress bar that updates at each step. Attributes: pbar (tqdm): The tqdm progress bar object. \"\"\" def __init__(self, total_steps): self.pbar = tqdm(total=total_steps) def on_step_end(self, optimizer): self.pbar.update(1) def on_train_end(self, logs=None): self.pbar.close()","title":"Callbacks"},{"location":"api/callbacks/#callbacks","text":"This module contains callback implementations for various purposes.","title":"Callbacks"},{"location":"api/callbacks/#promptolution.callbacks.BestPromptCallback","text":"Bases: Callback Callback for tracking the best prompt during optimization. This callback keeps track of the prompt with the highest score. Attributes: Name Type Description best_prompt str The prompt with the highest score so far. best_score float The highest score achieved so far. Source code in promptolution\\callbacks.py class BestPromptCallback(Callback): \"\"\" Callback for tracking the best prompt during optimization. This callback keeps track of the prompt with the highest score. Attributes: best_prompt (str): The prompt with the highest score so far. best_score (float): The highest score achieved so far. \"\"\" def __init__(self): self.best_prompt = \"\" self.best_score = -99999 def on_step_end(self, optimizer): if optimizer.scores[0] > self.best_score: self.best_score = optimizer.scores[0] self.best_prompt = optimizer.prompts[0] def get_best_prompt(self): return self.best_prompt, self.best_score","title":"BestPromptCallback"},{"location":"api/callbacks/#promptolution.callbacks.CSVCallback","text":"Bases: Callback Callback for saving optimization progress to a CSV file. This callback saves prompts and scores at each step to a CSV file. Attributes: Name Type Description path str The path to the CSV file. step int The current step number. Source code in promptolution\\callbacks.py class CSVCallback(Callback): \"\"\" Callback for saving optimization progress to a CSV file. This callback saves prompts and scores at each step to a CSV file. Attributes: path (str): The path to the CSV file. step (int): The current step number. \"\"\" def __init__(self, path): # if dir does not exist if not os.path.exists(os.path.dirname(path)): os.makedirs(os.path.dirname(path)) # create file in path with header: \"step,prompt,score\" with open(path, \"w\") as f: f.write(\"step,prompt,score\\n\") self.path = path self.step = 0 def on_step_end(self, optimizer): \"\"\" Save prompts and scores to csv \"\"\" self.step += 1 df = pd.DataFrame( {\"step\": [self.step] * len(optimizer.prompts), \"prompt\": optimizer.prompts, \"score\": optimizer.scores} ) df.to_csv(self.path, mode=\"a\", header=False, index=False) def on_train_end(self, logs=None): pass","title":"CSVCallback"},{"location":"api/callbacks/#promptolution.callbacks.CSVCallback.on_step_end","text":"Save prompts and scores to csv Source code in promptolution\\callbacks.py def on_step_end(self, optimizer): \"\"\" Save prompts and scores to csv \"\"\" self.step += 1 df = pd.DataFrame( {\"step\": [self.step] * len(optimizer.prompts), \"prompt\": optimizer.prompts, \"score\": optimizer.scores} ) df.to_csv(self.path, mode=\"a\", header=False, index=False)","title":"on_step_end"},{"location":"api/callbacks/#promptolution.callbacks.LoggerCallback","text":"Bases: Callback Callback for logging optimization progress. This callback logs information about each step, epoch, and the end of training. Attributes: Name Type Description logger The logger object to use for logging. step int The current step number. Source code in promptolution\\callbacks.py class LoggerCallback(Callback): \"\"\" Callback for logging optimization progress. This callback logs information about each step, epoch, and the end of training. Attributes: logger: The logger object to use for logging. step (int): The current step number. \"\"\" def __init__(self, logger): self.logger = logger self.step = 0 def on_step_end(self, optimizer): self.step += 1 self.logger.critical(f\"\u2728Step {self.step} ended\u2728\") for i, (prompt, score) in enumerate(zip(optimizer.prompts, optimizer.scores)): self.logger.critical(f\"*** Prompt {i}: Score: {score}\") self.logger.critical(f\"{prompt}\") def on_epoch_end(self, epoch, logs=None): self.logger.critical(f\"Epoch {epoch} - {logs}\") def on_train_end(self, logs=None): self.logger.critical(f\"Training ended - {logs}\")","title":"LoggerCallback"},{"location":"api/callbacks/#promptolution.callbacks.ProgressBarCallback","text":"Bases: Callback Callback for displaying a progress bar during optimization. This callback uses tqdm to display a progress bar that updates at each step. Attributes: Name Type Description pbar tqdm The tqdm progress bar object. Source code in promptolution\\callbacks.py class ProgressBarCallback(Callback): \"\"\" Callback for displaying a progress bar during optimization. This callback uses tqdm to display a progress bar that updates at each step. Attributes: pbar (tqdm): The tqdm progress bar object. \"\"\" def __init__(self, total_steps): self.pbar = tqdm(total=total_steps) def on_step_end(self, optimizer): self.pbar.update(1) def on_train_end(self, logs=None): self.pbar.close()","title":"ProgressBarCallback"},{"location":"api/config/","text":"Config This module contains the configuration settings for the Promptolution library. Config dataclass Configuration class for the promptolution library. This class handles loading and parsing of configuration settings, either from a config file or from keyword arguments. Attributes: Name Type Description task_name str Name of the task. ds_path str Path to the dataset. n_steps int Number of optimization steps. optimizer str Name of the optimizer to use. meta_prompt_path str Path to the meta prompt file. meta_llms str Name of the meta language model. downstream_llm str Name of the downstream language model. evaluation_llm str Name of the evaluation language model. init_pop_size int Initial population size. Defaults to 10. logging_dir str Directory for logging. Defaults to \"logs/run.csv\". experiment_name str Name of the experiment. Defaults to \"experiment\". include_task_desc bool Whether to include task description. Defaults to False. random_seed int Random seed for reproducibility. Defaults to 42. Source code in promptolution\\config.py @dataclass class Config: \"\"\" Configuration class for the promptolution library. This class handles loading and parsing of configuration settings, either from a config file or from keyword arguments. Attributes: task_name (str): Name of the task. ds_path (str): Path to the dataset. n_steps (int): Number of optimization steps. optimizer (str): Name of the optimizer to use. meta_prompt_path (str): Path to the meta prompt file. meta_llms (str): Name of the meta language model. downstream_llm (str): Name of the downstream language model. evaluation_llm (str): Name of the evaluation language model. init_pop_size (int): Initial population size. Defaults to 10. logging_dir (str): Directory for logging. Defaults to \"logs/run.csv\". experiment_name (str): Name of the experiment. Defaults to \"experiment\". include_task_desc (bool): Whether to include task description. Defaults to False. random_seed (int): Random seed for reproducibility. Defaults to 42. \"\"\" task_name: str ds_path: str n_steps: int optimizer: str meta_prompt_path: str meta_llms: str downstream_llm: str evaluation_llm: str init_pop_size: int = 10 logging_dir: str = \"logs/run.csv\" experiment_name: str = \"experiment\" include_task_desc: bool = False random_seed: int = 42 def __init__(self, config_path: str = None, **kwargs): if config_path: self.config_path = config_path self.config = ConfigParser() self.config.read(config_path) self._parse_config() else: for key, value in kwargs.items(): setattr(self, key, value) def _parse_config(self): self.task_name = self.config[\"task\"][\"task_name\"] self.ds_path = self.config[\"task\"][\"ds_path\"] self.n_steps = int(self.config[\"task\"][\"steps\"]) self.random_seed = int(self.config[\"task\"][\"random_seed\"]) self.optimizer = self.config[\"optimizer\"][\"name\"] self.meta_prompt_path = self.config[\"optimizer\"][\"meta_prompt_path\"] self.meta_llm = self.config[\"meta_llm\"][\"name\"] self.downstream_llm = self.config[\"downstream_llm\"][\"name\"] self.evaluation_llm = self.config[\"evaluator_llm\"][\"name\"] self.init_pop_size = int(self.config[\"optimizer\"][\"init_pop_size\"]) self.logging_dir = self.config[\"logging\"][\"dir\"] self.experiment_name = self.config[\"experiment\"][\"name\"] if \"include_task_desc\" in self.config[\"task\"]: self.include_task_desc = self.config[\"task\"][\"include_task_desc\"] == \"True\" if self.optimizer == \"evopromptga\": self.selection_mode = self.config[\"optimizer\"][\"selection_mode\"] elif self.optimizer == \"evopromptde\": self.selection_mode = self.config[\"optimizer\"][\"donor_random\"] if \"local\" in self.meta_llm: self.meta_bs = int(self.config[\"meta_llm\"][\"batch_size\"]) if \"local\" in self.downstream_llm: self.downstream_bs = int(self.config[\"downstream_llm\"][\"batch_size\"]) config = ConfigParser() instance-attribute config_path = config_path instance-attribute downstream_llm: str instance-attribute ds_path: str instance-attribute evaluation_llm: str instance-attribute experiment_name: str = 'experiment' class-attribute instance-attribute include_task_desc: bool = False class-attribute instance-attribute init_pop_size: int = 10 class-attribute instance-attribute logging_dir: str = 'logs/run.csv' class-attribute instance-attribute meta_llms: str instance-attribute meta_prompt_path: str instance-attribute n_steps: int instance-attribute optimizer: str instance-attribute random_seed: int = 42 class-attribute instance-attribute task_name: str instance-attribute __init__(config_path=None, **kwargs) Source code in promptolution\\config.py def __init__(self, config_path: str = None, **kwargs): if config_path: self.config_path = config_path self.config = ConfigParser() self.config.read(config_path) self._parse_config() else: for key, value in kwargs.items(): setattr(self, key, value)","title":"Config"},{"location":"api/config/#config","text":"This module contains the configuration settings for the Promptolution library.","title":"Config"},{"location":"api/config/#promptolution.config.Config","text":"Configuration class for the promptolution library. This class handles loading and parsing of configuration settings, either from a config file or from keyword arguments. Attributes: Name Type Description task_name str Name of the task. ds_path str Path to the dataset. n_steps int Number of optimization steps. optimizer str Name of the optimizer to use. meta_prompt_path str Path to the meta prompt file. meta_llms str Name of the meta language model. downstream_llm str Name of the downstream language model. evaluation_llm str Name of the evaluation language model. init_pop_size int Initial population size. Defaults to 10. logging_dir str Directory for logging. Defaults to \"logs/run.csv\". experiment_name str Name of the experiment. Defaults to \"experiment\". include_task_desc bool Whether to include task description. Defaults to False. random_seed int Random seed for reproducibility. Defaults to 42. Source code in promptolution\\config.py @dataclass class Config: \"\"\" Configuration class for the promptolution library. This class handles loading and parsing of configuration settings, either from a config file or from keyword arguments. Attributes: task_name (str): Name of the task. ds_path (str): Path to the dataset. n_steps (int): Number of optimization steps. optimizer (str): Name of the optimizer to use. meta_prompt_path (str): Path to the meta prompt file. meta_llms (str): Name of the meta language model. downstream_llm (str): Name of the downstream language model. evaluation_llm (str): Name of the evaluation language model. init_pop_size (int): Initial population size. Defaults to 10. logging_dir (str): Directory for logging. Defaults to \"logs/run.csv\". experiment_name (str): Name of the experiment. Defaults to \"experiment\". include_task_desc (bool): Whether to include task description. Defaults to False. random_seed (int): Random seed for reproducibility. Defaults to 42. \"\"\" task_name: str ds_path: str n_steps: int optimizer: str meta_prompt_path: str meta_llms: str downstream_llm: str evaluation_llm: str init_pop_size: int = 10 logging_dir: str = \"logs/run.csv\" experiment_name: str = \"experiment\" include_task_desc: bool = False random_seed: int = 42 def __init__(self, config_path: str = None, **kwargs): if config_path: self.config_path = config_path self.config = ConfigParser() self.config.read(config_path) self._parse_config() else: for key, value in kwargs.items(): setattr(self, key, value) def _parse_config(self): self.task_name = self.config[\"task\"][\"task_name\"] self.ds_path = self.config[\"task\"][\"ds_path\"] self.n_steps = int(self.config[\"task\"][\"steps\"]) self.random_seed = int(self.config[\"task\"][\"random_seed\"]) self.optimizer = self.config[\"optimizer\"][\"name\"] self.meta_prompt_path = self.config[\"optimizer\"][\"meta_prompt_path\"] self.meta_llm = self.config[\"meta_llm\"][\"name\"] self.downstream_llm = self.config[\"downstream_llm\"][\"name\"] self.evaluation_llm = self.config[\"evaluator_llm\"][\"name\"] self.init_pop_size = int(self.config[\"optimizer\"][\"init_pop_size\"]) self.logging_dir = self.config[\"logging\"][\"dir\"] self.experiment_name = self.config[\"experiment\"][\"name\"] if \"include_task_desc\" in self.config[\"task\"]: self.include_task_desc = self.config[\"task\"][\"include_task_desc\"] == \"True\" if self.optimizer == \"evopromptga\": self.selection_mode = self.config[\"optimizer\"][\"selection_mode\"] elif self.optimizer == \"evopromptde\": self.selection_mode = self.config[\"optimizer\"][\"donor_random\"] if \"local\" in self.meta_llm: self.meta_bs = int(self.config[\"meta_llm\"][\"batch_size\"]) if \"local\" in self.downstream_llm: self.downstream_bs = int(self.config[\"downstream_llm\"][\"batch_size\"])","title":"Config"},{"location":"api/config/#promptolution.config.Config.config","text":"","title":"config"},{"location":"api/config/#promptolution.config.Config.config_path","text":"","title":"config_path"},{"location":"api/config/#promptolution.config.Config.downstream_llm","text":"","title":"downstream_llm"},{"location":"api/config/#promptolution.config.Config.ds_path","text":"","title":"ds_path"},{"location":"api/config/#promptolution.config.Config.evaluation_llm","text":"","title":"evaluation_llm"},{"location":"api/config/#promptolution.config.Config.experiment_name","text":"","title":"experiment_name"},{"location":"api/config/#promptolution.config.Config.include_task_desc","text":"","title":"include_task_desc"},{"location":"api/config/#promptolution.config.Config.init_pop_size","text":"","title":"init_pop_size"},{"location":"api/config/#promptolution.config.Config.logging_dir","text":"","title":"logging_dir"},{"location":"api/config/#promptolution.config.Config.meta_llms","text":"","title":"meta_llms"},{"location":"api/config/#promptolution.config.Config.meta_prompt_path","text":"","title":"meta_prompt_path"},{"location":"api/config/#promptolution.config.Config.n_steps","text":"","title":"n_steps"},{"location":"api/config/#promptolution.config.Config.optimizer","text":"","title":"optimizer"},{"location":"api/config/#promptolution.config.Config.random_seed","text":"","title":"random_seed"},{"location":"api/config/#promptolution.config.Config.task_name","text":"","title":"task_name"},{"location":"api/config/#promptolution.config.Config.__init__","text":"Source code in promptolution\\config.py def __init__(self, config_path: str = None, **kwargs): if config_path: self.config_path = config_path self.config = ConfigParser() self.config.read(config_path) self._parse_config() else: for key, value in kwargs.items(): setattr(self, key, value)","title":"__init__"},{"location":"api/llms/","text":"LLMs This module contains the LLM (Large Language Model) implementations. get_llm(model_id, *args, **kwargs) Factory function to create and return a language model instance based on the provided model_id. This function supports three types of language models: 1. DummyLLM: A mock LLM for testing purposes. 2. LocalLLM: For running models locally (identified by 'local' in the model_id). 3. APILLM: For API-based models (default if not matching other types). Parameters: Name Type Description Default model_id str Identifier for the model to use. Special cases: - \"dummy\" for DummyLLM - \"local-{model_name}\" for LocalLLM - Any other string for APILLM required *args Variable length argument list passed to the LLM constructor. () **kwargs Arbitrary keyword arguments passed to the LLM constructor. {} Returns: Type Description An instance of DummyLLM, LocalLLM, or APILLM based on the model_id. Source code in promptolution\\llms\\__init__.py def get_llm(model_id: str, *args, **kwargs): \"\"\" Factory function to create and return a language model instance based on the provided model_id. This function supports three types of language models: 1. DummyLLM: A mock LLM for testing purposes. 2. LocalLLM: For running models locally (identified by 'local' in the model_id). 3. APILLM: For API-based models (default if not matching other types). Args: model_id (str): Identifier for the model to use. Special cases: - \"dummy\" for DummyLLM - \"local-{model_name}\" for LocalLLM - Any other string for APILLM *args: Variable length argument list passed to the LLM constructor. **kwargs: Arbitrary keyword arguments passed to the LLM constructor. Returns: An instance of DummyLLM, LocalLLM, or APILLM based on the model_id. \"\"\" if model_id == \"dummy\": return DummyLLM(*args, **kwargs) if \"local\" in model_id: model_id = \"-\".join(model_id.split(\"-\")[1:]) return LocalLLM(model_id, *args, **kwargs) return APILLM(model_id, *args, **kwargs) api_llm APILLM A class to interface with various language models through their respective APIs. This class supports Claude (Anthropic), GPT (OpenAI), and LLaMA (DeepInfra) models. It handles API key management, model initialization, and provides methods for both synchronous and asynchronous inference. Attributes: Name Type Description model The initialized language model instance. Methods: Name Description get_response Synchronously get responses for a list of prompts. _get_response Asynchronously get responses for a list of prompts. Source code in promptolution\\llms\\api_llm.py class APILLM: \"\"\" A class to interface with various language models through their respective APIs. This class supports Claude (Anthropic), GPT (OpenAI), and LLaMA (DeepInfra) models. It handles API key management, model initialization, and provides methods for both synchronous and asynchronous inference. Attributes: model: The initialized language model instance. Methods: get_response: Synchronously get responses for a list of prompts. _get_response: Asynchronously get responses for a list of prompts. \"\"\" def __init__(self, model_id: str): \"\"\" Initialize the APILLM with a specific model. Args: model_id (str): Identifier for the model to use. Raises: ValueError: If an unknown model identifier is provided. \"\"\" if \"claude\" in model_id: ANTHROPIC_API_KEY = open(\"anthropictoken.txt\", \"r\").read() self.model = ChatAnthropic(model=model_id, api_key=ANTHROPIC_API_KEY) elif \"gpt\" in model_id: OPENAI_API_KEY = open(\"openaitoken.txt\", \"r\").read() self.model = ChatOpenAI(model=model_id, api_key=OPENAI_API_KEY) elif \"llama\" in model_id: DEEPINFRA_API_KEY = open(\"deepinfratoken.txt\", \"r\").read() self.model = ChatDeepInfra(model_name=model_id, deepinfra_api_token=DEEPINFRA_API_KEY) else: raise ValueError(f\"Unknown model: {model_id}\") def get_response(self, prompts: List[str]) -> List[str]: \"\"\" Synchronously get responses for a list of prompts. This method includes retry logic for handling connection errors and rate limits. Args: prompts (list[str]): List of input prompts. Returns: list[str]: List of model responses. Raises: requests.exceptions.ConnectionError: If max retries are exceeded. \"\"\" max_retries = 100 delay = 3 attempts = 0 while attempts < max_retries: try: responses = asyncio.run(self._get_response(prompts)) return responses except requests.exceptions.ConnectionError as e: attempts += 1 logger.critical( f\"Connection error: {e}. Attempt {attempts}/{max_retries}. Retrying in {delay} seconds...\" ) time.sleep(delay) except openai.RateLimitError as e: attempts += 1 logger.critical( f\"Rate limit error: {e}. Attempt {attempts}/{max_retries}. Retrying in {delay} seconds...\" ) time.sleep(delay) # If the loop exits, it means max retries were reached raise requests.exceptions.ConnectionError(\"Max retries exceeded. Connection could not be established.\") async def _get_response( self, prompts: list[str], max_concurrent_calls=200 ) -> list[str]: \"\"\" Asynchronously get responses for a list of prompts. This method uses a semaphore to limit the number of concurrent API calls. Args: prompts (list[str]): List of input prompts. max_concurrent_calls (int): Maximum number of concurrent API calls allowed. Returns: list[str]: List of model responses. \"\"\" semaphore = asyncio.Semaphore(max_concurrent_calls) # Limit the number of concurrent calls tasks = [] for prompt in prompts: tasks.append(invoke_model(prompt, self.model, semaphore)) responses = await asyncio.gather(*tasks) return responses __init__(model_id) Initialize the APILLM with a specific model. Parameters: Name Type Description Default model_id str Identifier for the model to use. required Raises: Type Description ValueError If an unknown model identifier is provided. Source code in promptolution\\llms\\api_llm.py def __init__(self, model_id: str): \"\"\" Initialize the APILLM with a specific model. Args: model_id (str): Identifier for the model to use. Raises: ValueError: If an unknown model identifier is provided. \"\"\" if \"claude\" in model_id: ANTHROPIC_API_KEY = open(\"anthropictoken.txt\", \"r\").read() self.model = ChatAnthropic(model=model_id, api_key=ANTHROPIC_API_KEY) elif \"gpt\" in model_id: OPENAI_API_KEY = open(\"openaitoken.txt\", \"r\").read() self.model = ChatOpenAI(model=model_id, api_key=OPENAI_API_KEY) elif \"llama\" in model_id: DEEPINFRA_API_KEY = open(\"deepinfratoken.txt\", \"r\").read() self.model = ChatDeepInfra(model_name=model_id, deepinfra_api_token=DEEPINFRA_API_KEY) else: raise ValueError(f\"Unknown model: {model_id}\") get_response(prompts) Synchronously get responses for a list of prompts. This method includes retry logic for handling connection errors and rate limits. Parameters: Name Type Description Default prompts list [ str ] List of input prompts. required Returns: Type Description List [ str ] list[str]: List of model responses. Raises: Type Description ConnectionError If max retries are exceeded. Source code in promptolution\\llms\\api_llm.py def get_response(self, prompts: List[str]) -> List[str]: \"\"\" Synchronously get responses for a list of prompts. This method includes retry logic for handling connection errors and rate limits. Args: prompts (list[str]): List of input prompts. Returns: list[str]: List of model responses. Raises: requests.exceptions.ConnectionError: If max retries are exceeded. \"\"\" max_retries = 100 delay = 3 attempts = 0 while attempts < max_retries: try: responses = asyncio.run(self._get_response(prompts)) return responses except requests.exceptions.ConnectionError as e: attempts += 1 logger.critical( f\"Connection error: {e}. Attempt {attempts}/{max_retries}. Retrying in {delay} seconds...\" ) time.sleep(delay) except openai.RateLimitError as e: attempts += 1 logger.critical( f\"Rate limit error: {e}. Attempt {attempts}/{max_retries}. Retrying in {delay} seconds...\" ) time.sleep(delay) # If the loop exits, it means max retries were reached raise requests.exceptions.ConnectionError(\"Max retries exceeded. Connection could not be established.\") invoke_model(prompt, model, semaphore) async Asynchronously invoke a language model with retry logic. Parameters: Name Type Description Default prompt str The input prompt for the model. required model The language model to invoke. required semaphore Semaphore Semaphore to limit concurrent calls. required Returns: Name Type Description str The model's response content. Raises: Type Description ChatDeepInfraException If all retry attempts fail. Source code in promptolution\\llms\\api_llm.py async def invoke_model(prompt, model, semaphore): \"\"\" Asynchronously invoke a language model with retry logic. Args: prompt (str): The input prompt for the model. model: The language model to invoke. semaphore (asyncio.Semaphore): Semaphore to limit concurrent calls. Returns: str: The model's response content. Raises: ChatDeepInfraException: If all retry attempts fail. \"\"\" async with semaphore: max_retries = 100 delay = 3 attempts = 0 while attempts < max_retries: try: response = await asyncio.to_thread(model.invoke, [HumanMessage(content=prompt)]) return response.content except ChatDeepInfraException as e: print(f\"DeepInfra error: {e}. Attempt {attempts}/{max_retries}. Retrying in {delay} seconds...\") attempts += 1 time.sleep(delay) base_llm BaseLLM Bases: ABC Abstract base class for Language Models in the promptolution library. This class defines the interface that all concrete LLM implementations should follow. Methods: Name Description get_response An abstract method that should be implemented by subclasses to generate responses for given prompts. Source code in promptolution\\llms\\base_llm.py class BaseLLM(ABC): \"\"\" Abstract base class for Language Models in the promptolution library. This class defines the interface that all concrete LLM implementations should follow. Methods: get_response: An abstract method that should be implemented by subclasses to generate responses for given prompts. \"\"\" def __init__(self, *args, **kwargs): pass @abstractmethod def get_response(self, prompts: List[str]) -> List[str]: \"\"\" Generate responses for the given prompts. This method should be implemented by subclasses to define how the LLM generates responses. Args: prompts (List[str]): A list of input prompts. Returns: List[str]: A list of generated responses corresponding to the input prompts. \"\"\" pass get_response(prompts) abstractmethod Generate responses for the given prompts. This method should be implemented by subclasses to define how the LLM generates responses. Parameters: Name Type Description Default prompts List [ str ] A list of input prompts. required Returns: Type Description List [ str ] List[str]: A list of generated responses corresponding to the input prompts. Source code in promptolution\\llms\\base_llm.py @abstractmethod def get_response(self, prompts: List[str]) -> List[str]: \"\"\" Generate responses for the given prompts. This method should be implemented by subclasses to define how the LLM generates responses. Args: prompts (List[str]): A list of input prompts. Returns: List[str]: A list of generated responses corresponding to the input prompts. \"\"\" pass DummyLLM Bases: BaseLLM A dummy implementation of the BaseLLM for testing purposes. This class generates random responses for given prompts, simulating the behavior of a language model without actually performing any complex natural language processing. Source code in promptolution\\llms\\base_llm.py class DummyLLM(BaseLLM): \"\"\" A dummy implementation of the BaseLLM for testing purposes. This class generates random responses for given prompts, simulating the behavior of a language model without actually performing any complex natural language processing. \"\"\" def __init__(self, *args, **kwargs): pass def get_response(self, prompts: str) -> str: \"\"\" Generate random responses for the given prompts. This method creates silly, random responses enclosed in <prompt> tags. It's designed for testing and demonstration purposes. Args: prompts (str or List[str]): Input prompt(s). If a single string is provided, it's converted to a list containing that string. Returns: List[str]: A list of randomly generated responses, one for each input prompt. \"\"\" if isinstance(prompts, str): prompts = [prompts] results = [] for _ in prompts: r = np.random.rand() if r < 0.3: results += [f\"Joooo wazzuppp <prompt>hier gehts los {r} </prompt>\"] if 0.3 <= r < 0.6: results += [f\"was das hier? <prompt>peter lustig{r}</prompt>\"] else: results += [f\"hier ist ein <prompt>test{r}</prompt>\"] return results get_response(prompts) Generate random responses for the given prompts. This method creates silly, random responses enclosed in tags. It's designed for testing and demonstration purposes. Parameters: Name Type Description Default prompts str or List [ str ] Input prompt(s). If a single string is provided, it's converted to a list containing that string. required Returns: Type Description str List[str]: A list of randomly generated responses, one for each input prompt. Source code in promptolution\\llms\\base_llm.py def get_response(self, prompts: str) -> str: \"\"\" Generate random responses for the given prompts. This method creates silly, random responses enclosed in <prompt> tags. It's designed for testing and demonstration purposes. Args: prompts (str or List[str]): Input prompt(s). If a single string is provided, it's converted to a list containing that string. Returns: List[str]: A list of randomly generated responses, one for each input prompt. \"\"\" if isinstance(prompts, str): prompts = [prompts] results = [] for _ in prompts: r = np.random.rand() if r < 0.3: results += [f\"Joooo wazzuppp <prompt>hier gehts los {r} </prompt>\"] if 0.3 <= r < 0.6: results += [f\"was das hier? <prompt>peter lustig{r}</prompt>\"] else: results += [f\"hier ist ein <prompt>test{r}</prompt>\"] return results deepinfra ChatDeepInfra Bases: BaseChatModel A chat model that uses the DeepInfra API. Source code in promptolution\\llms\\deepinfra.py class ChatDeepInfra(BaseChatModel): \"\"\"A chat model that uses the DeepInfra API.\"\"\" # client: Any #: :meta private: model_name: str = Field(alias=\"model\") \"\"\"The model name to use for the chat model.\"\"\" deepinfra_api_token: Optional[str] = None request_timeout: Optional[float] = Field(default=None, alias=\"timeout\") temperature: Optional[float] = 1 model_kwargs: Dict[str, Any] = Field(default_factory=dict) \"\"\"Run inference with this temperature. Must be in the closed interval [0.0, 1.0].\"\"\" top_p: Optional[float] = None \"\"\"Decode using nucleus sampling: consider the smallest set of tokens whose probability sum is at least top_p. Must be in the closed interval [0.0, 1.0].\"\"\" top_k: Optional[int] = None \"\"\"Decode using top-k sampling: consider the set of top_k most probable tokens. Must be positive.\"\"\" n: int = 1 \"\"\"Number of chat completions to generate for each prompt. Note that the API may not return the full n completions if duplicates are generated.\"\"\" max_tokens: int = 256 streaming: bool = False max_retries: int = 1 def __init__(self, model_name: str, **kwargs: Any): super().__init__(model=model_name, **kwargs) @property def _default_params(self) -> Dict[str, Any]: \"\"\"Get the default parameters for calling OpenAI API.\"\"\" return { \"model\": self.model_name, \"max_tokens\": self.max_tokens, \"stream\": self.streaming, \"n\": self.n, \"temperature\": self.temperature, \"request_timeout\": self.request_timeout, **self.model_kwargs, } @property def _client_params(self) -> Dict[str, Any]: \"\"\"Get the parameters used for the openai client.\"\"\" return {**self._default_params} def completion_with_retry(self, run_manager: Optional[CallbackManagerForLLMRun] = None, **kwargs: Any) -> Any: \"\"\"Use tenacity to retry the completion call.\"\"\" retry_decorator = _create_retry_decorator(self, run_manager=run_manager) @retry_decorator def _completion_with_retry(**kwargs: Any) -> Any: try: request_timeout = kwargs.pop(\"request_timeout\") request = Requests(headers=self._headers()) response = request.post(url=self._url(), data=self._body(kwargs), timeout=request_timeout) self._handle_status(response.status_code, response.text) return response except Exception as e: # import pdb; pdb.set_trace() print(\"EX\", e) # noqa: T201 raise return _completion_with_retry(**kwargs) async def acompletion_with_retry( self, run_manager: Optional[AsyncCallbackManagerForLLMRun] = None, **kwargs: Any, ) -> Any: \"\"\"Use tenacity to retry the async completion call.\"\"\" retry_decorator = _create_retry_decorator(self, run_manager=run_manager) @retry_decorator async def _completion_with_retry(**kwargs: Any) -> Any: try: request_timeout = kwargs.pop(\"request_timeout\") request = Requests(headers=self._headers()) async with request.apost(url=self._url(), data=self._body(kwargs), timeout=request_timeout) as response: self._handle_status(response.status, response.text) return await response.json() except Exception as e: print(\"EX\", e) # noqa: T201 raise return await _completion_with_retry(**kwargs) @root_validator(pre=True) def init_defaults(cls, values: Dict) -> Dict: \"\"\"Validate api key, python package exists, temperature, top_p, and top_k.\"\"\" # For compatibility with LiteLLM api_key = get_from_dict_or_env( values, \"deepinfra_api_key\", \"DEEPINFRA_API_KEY\", default=\"\", ) values[\"deepinfra_api_token\"] = get_from_dict_or_env( values, \"deepinfra_api_token\", \"DEEPINFRA_API_TOKEN\", default=api_key, ) # set model id # values[\"model_name\"] = get_from_dict_or_env( # values, # \"model_name\", # \"DEEPINFRA_MODEL_NAME\", # default=\"\", # ) return values @root_validator(pre=False, skip_on_failure=True) def validate_environment(cls, values: Dict) -> Dict: if values[\"temperature\"] is not None and not 0 <= values[\"temperature\"] <= 1: raise ValueError(\"temperature must be in the range [0.0, 1.0]\") if values[\"top_p\"] is not None and not 0 <= values[\"top_p\"] <= 1: raise ValueError(\"top_p must be in the range [0.0, 1.0]\") if values[\"top_k\"] is not None and values[\"top_k\"] <= 0: raise ValueError(\"top_k must be positive\") return values def _generate( self, messages: List[BaseMessage], stop: Optional[List[str]] = None, run_manager: Optional[CallbackManagerForLLMRun] = None, stream: Optional[bool] = None, **kwargs: Any, ) -> ChatResult: should_stream = stream if stream is not None else self.streaming if should_stream: stream_iter = self._stream(messages, stop=stop, run_manager=run_manager, **kwargs) return generate_from_stream(stream_iter) message_dicts, params = self._create_message_dicts(messages, stop) params = {**params, **kwargs} response = self.completion_with_retry(messages=message_dicts, run_manager=run_manager, **params) return self._create_chat_result(response.json()) def _create_chat_result(self, response: Mapping[str, Any]) -> ChatResult: generations = [] for res in response[\"choices\"]: message = _convert_dict_to_message(res[\"message\"]) gen = ChatGeneration( message=message, generation_info=dict(finish_reason=res.get(\"finish_reason\")), ) generations.append(gen) token_usage = response.get(\"usage\", {}) llm_output = {\"token_usage\": token_usage, \"model\": self.model_name} res = ChatResult(generations=generations, llm_output=llm_output) return res def _create_message_dicts( self, messages: List[BaseMessage], stop: Optional[List[str]] ) -> Tuple[List[Dict[str, Any]], Dict[str, Any]]: params = self._client_params if stop is not None: if \"stop\" in params: raise ValueError(\"`stop` found in both the input and default params.\") params[\"stop\"] = stop message_dicts = [_convert_message_to_dict(m) for m in messages] return message_dicts, params def _stream( self, messages: List[BaseMessage], stop: Optional[List[str]] = None, run_manager: Optional[CallbackManagerForLLMRun] = None, **kwargs: Any, ) -> Iterator[ChatGenerationChunk]: message_dicts, params = self._create_message_dicts(messages, stop) params = {**params, **kwargs, \"stream\": True} response = self.completion_with_retry(messages=message_dicts, run_manager=run_manager, **params) for line in _parse_stream(response.iter_lines()): chunk = _handle_sse_line(line) if chunk: cg_chunk = ChatGenerationChunk(message=chunk, generation_info=None) if run_manager: run_manager.on_llm_new_token(str(chunk.content), chunk=cg_chunk) yield cg_chunk async def _astream( self, messages: List[BaseMessage], stop: Optional[List[str]] = None, run_manager: Optional[AsyncCallbackManagerForLLMRun] = None, **kwargs: Any, ) -> AsyncIterator[ChatGenerationChunk]: message_dicts, params = self._create_message_dicts(messages, stop) params = {\"messages\": message_dicts, \"stream\": True, **params, **kwargs} request_timeout = params.pop(\"request_timeout\") request = Requests(headers=self._headers()) async with request.apost(url=self._url(), data=self._body(params), timeout=request_timeout) as response: async for line in _parse_stream_async(response.content): chunk = _handle_sse_line(line) if chunk: cg_chunk = ChatGenerationChunk(message=chunk, generation_info=None) if run_manager: await run_manager.on_llm_new_token(str(chunk.content), chunk=cg_chunk) yield cg_chunk async def _agenerate( self, messages: List[BaseMessage], stop: Optional[List[str]] = None, run_manager: Optional[AsyncCallbackManagerForLLMRun] = None, stream: Optional[bool] = None, **kwargs: Any, ) -> ChatResult: should_stream = stream if stream is not None else self.streaming if should_stream: stream_iter = self._astream(messages, stop=stop, run_manager=run_manager, **kwargs) return await agenerate_from_stream(stream_iter) message_dicts, params = self._create_message_dicts(messages, stop) params = {\"messages\": message_dicts, **params, **kwargs} res = await self.acompletion_with_retry(run_manager=run_manager, **params) return self._create_chat_result(res) @property def _identifying_params(self) -> Dict[str, Any]: \"\"\"Get the identifying parameters.\"\"\" return { \"model\": self.model_name, \"temperature\": self.temperature, \"top_p\": self.top_p, \"top_k\": self.top_k, \"n\": self.n, } @property def _llm_type(self) -> str: return \"deepinfra-chat\" def _handle_status(self, code: int, text: Any) -> None: if code >= 500: raise ChatDeepInfraException(f\"DeepInfra Server: Error {code}\") elif code >= 400: raise ValueError(f\"DeepInfra received an invalid payload: {text}\") elif code != 200: raise Exception(f\"DeepInfra returned an unexpected response with status \" f\"{code}: {text}\") def _url(self) -> str: return \"https://stage.api.deepinfra.com/v1/openai/chat/completions\" def _headers(self) -> Dict: return { \"Authorization\": f\"bearer {self.deepinfra_api_token}\", \"Content-Type\": \"application/json\", } def _body(self, kwargs: Any) -> Dict: return kwargs def bind_tools( self, tools: Sequence[Union[Dict[str, Any], Type[BaseModel], Callable, BaseTool]], **kwargs: Any, ) -> Runnable[LanguageModelInput, BaseMessage]: \"\"\"Bind tool-like objects to this chat model. Assumes model is compatible with OpenAI tool-calling API. Args: tools: A list of tool definitions to bind to this chat model. Can be a dictionary, pydantic model, callable, or BaseTool. Pydantic models, callables, and BaseTools will be automatically converted to their schema dictionary representation. **kwargs: Any additional parameters to pass to the :class:`~langchain.runnable.Runnable` constructor. \"\"\" formatted_tools = [convert_to_openai_tool(tool) for tool in tools] return super().bind(tools=formatted_tools, **kwargs) model_kwargs: Dict[str, Any] = Field(default_factory=dict) class-attribute instance-attribute Run inference with this temperature. Must be in the closed interval [0.0, 1.0]. model_name: str = Field(alias='model') class-attribute instance-attribute The model name to use for the chat model. n: int = 1 class-attribute instance-attribute Number of chat completions to generate for each prompt. Note that the API may not return the full n completions if duplicates are generated. top_k: Optional[int] = None class-attribute instance-attribute Decode using top-k sampling: consider the set of top_k most probable tokens. Must be positive. top_p: Optional[float] = None class-attribute instance-attribute Decode using nucleus sampling: consider the smallest set of tokens whose probability sum is at least top_p. Must be in the closed interval [0.0, 1.0]. acompletion_with_retry(run_manager=None, **kwargs) async Use tenacity to retry the async completion call. Source code in promptolution\\llms\\deepinfra.py async def acompletion_with_retry( self, run_manager: Optional[AsyncCallbackManagerForLLMRun] = None, **kwargs: Any, ) -> Any: \"\"\"Use tenacity to retry the async completion call.\"\"\" retry_decorator = _create_retry_decorator(self, run_manager=run_manager) @retry_decorator async def _completion_with_retry(**kwargs: Any) -> Any: try: request_timeout = kwargs.pop(\"request_timeout\") request = Requests(headers=self._headers()) async with request.apost(url=self._url(), data=self._body(kwargs), timeout=request_timeout) as response: self._handle_status(response.status, response.text) return await response.json() except Exception as e: print(\"EX\", e) # noqa: T201 raise return await _completion_with_retry(**kwargs) bind_tools(tools, **kwargs) Bind tool-like objects to this chat model. Assumes model is compatible with OpenAI tool-calling API. Parameters: Name Type Description Default tools Sequence [ Union [ Dict [ str , Any ], Type [ BaseModel ], Callable , BaseTool ]] A list of tool definitions to bind to this chat model. Can be a dictionary, pydantic model, callable, or BaseTool. Pydantic models, callables, and BaseTools will be automatically converted to their schema dictionary representation. required **kwargs Any Any additional parameters to pass to the :class: ~langchain.runnable.Runnable constructor. {} Source code in promptolution\\llms\\deepinfra.py def bind_tools( self, tools: Sequence[Union[Dict[str, Any], Type[BaseModel], Callable, BaseTool]], **kwargs: Any, ) -> Runnable[LanguageModelInput, BaseMessage]: \"\"\"Bind tool-like objects to this chat model. Assumes model is compatible with OpenAI tool-calling API. Args: tools: A list of tool definitions to bind to this chat model. Can be a dictionary, pydantic model, callable, or BaseTool. Pydantic models, callables, and BaseTools will be automatically converted to their schema dictionary representation. **kwargs: Any additional parameters to pass to the :class:`~langchain.runnable.Runnable` constructor. \"\"\" formatted_tools = [convert_to_openai_tool(tool) for tool in tools] return super().bind(tools=formatted_tools, **kwargs) completion_with_retry(run_manager=None, **kwargs) Use tenacity to retry the completion call. Source code in promptolution\\llms\\deepinfra.py def completion_with_retry(self, run_manager: Optional[CallbackManagerForLLMRun] = None, **kwargs: Any) -> Any: \"\"\"Use tenacity to retry the completion call.\"\"\" retry_decorator = _create_retry_decorator(self, run_manager=run_manager) @retry_decorator def _completion_with_retry(**kwargs: Any) -> Any: try: request_timeout = kwargs.pop(\"request_timeout\") request = Requests(headers=self._headers()) response = request.post(url=self._url(), data=self._body(kwargs), timeout=request_timeout) self._handle_status(response.status_code, response.text) return response except Exception as e: # import pdb; pdb.set_trace() print(\"EX\", e) # noqa: T201 raise return _completion_with_retry(**kwargs) init_defaults(values) Validate api key, python package exists, temperature, top_p, and top_k. Source code in promptolution\\llms\\deepinfra.py @root_validator(pre=True) def init_defaults(cls, values: Dict) -> Dict: \"\"\"Validate api key, python package exists, temperature, top_p, and top_k.\"\"\" # For compatibility with LiteLLM api_key = get_from_dict_or_env( values, \"deepinfra_api_key\", \"DEEPINFRA_API_KEY\", default=\"\", ) values[\"deepinfra_api_token\"] = get_from_dict_or_env( values, \"deepinfra_api_token\", \"DEEPINFRA_API_TOKEN\", default=api_key, ) # set model id # values[\"model_name\"] = get_from_dict_or_env( # values, # \"model_name\", # \"DEEPINFRA_MODEL_NAME\", # default=\"\", # ) return values local_llm LocalLLM A class for running language models locally using the Hugging Face Transformers library. This class sets up a text generation pipeline with specified model parameters and provides a method to generate responses for given prompts. Attributes: Name Type Description pipeline Pipeline The text generation pipeline. Methods: Name Description get_response Generate responses for a list of prompts. Source code in promptolution\\llms\\local_llm.py class LocalLLM: \"\"\" A class for running language models locally using the Hugging Face Transformers library. This class sets up a text generation pipeline with specified model parameters and provides a method to generate responses for given prompts. Attributes: pipeline (transformers.Pipeline): The text generation pipeline. Methods: get_response: Generate responses for a list of prompts. \"\"\" def __init__(self, model_id: str, batch_size=8): \"\"\" Initialize the LocalLLM with a specific model. Args: model_id (str): The identifier of the model to use (e.g., \"gpt2\", \"facebook/opt-1.3b\"). batch_size (int, optional): The batch size for text generation. Defaults to 8. Note: This method sets up a text generation pipeline with bfloat16 precision, automatic device mapping, and specific generation parameters. \"\"\" self.pipeline = transformers.pipeline( \"text-generation\", model=model_id, model_kwargs={\"torch_dtype\": torch.bfloat16}, device_map=\"auto\", max_new_tokens=256, batch_size=batch_size, num_return_sequences=1, return_full_text=False, ) self.pipeline.tokenizer.pad_token_id = self.pipeline.tokenizer.eos_token_id self.pipeline.tokenizer.padding_side = \"left\" def get_response(self, prompts: list[str]): \"\"\" Generate responses for a list of prompts using the local language model. Args: prompts (list[str]): A list of input prompts. Returns: list[str]: A list of generated responses corresponding to the input prompts. Note: This method uses torch.no_grad() for inference to reduce memory usage. It handles both single and batch inputs, ensuring consistent output format. \"\"\" with torch.no_grad(): response = self.pipeline(prompts, pad_token_id=self.pipeline.tokenizer.eos_token_id) if len(response) != 1: response = [r[0] if isinstance(r, list) else r for r in response] response = [r[\"generated_text\"] for r in response] return response def __del__(self): try: del self.pipeline torch.cuda.empty_cache() except Exception as e: logger.warning(f\"Error during LocalLLM cleanup: {e}\") __init__(model_id, batch_size=8) Initialize the LocalLLM with a specific model. Parameters: Name Type Description Default model_id str The identifier of the model to use (e.g., \"gpt2\", \"facebook/opt-1.3b\"). required batch_size int The batch size for text generation. Defaults to 8. 8 Note This method sets up a text generation pipeline with bfloat16 precision, automatic device mapping, and specific generation parameters. Source code in promptolution\\llms\\local_llm.py def __init__(self, model_id: str, batch_size=8): \"\"\" Initialize the LocalLLM with a specific model. Args: model_id (str): The identifier of the model to use (e.g., \"gpt2\", \"facebook/opt-1.3b\"). batch_size (int, optional): The batch size for text generation. Defaults to 8. Note: This method sets up a text generation pipeline with bfloat16 precision, automatic device mapping, and specific generation parameters. \"\"\" self.pipeline = transformers.pipeline( \"text-generation\", model=model_id, model_kwargs={\"torch_dtype\": torch.bfloat16}, device_map=\"auto\", max_new_tokens=256, batch_size=batch_size, num_return_sequences=1, return_full_text=False, ) self.pipeline.tokenizer.pad_token_id = self.pipeline.tokenizer.eos_token_id self.pipeline.tokenizer.padding_side = \"left\" get_response(prompts) Generate responses for a list of prompts using the local language model. Parameters: Name Type Description Default prompts list [ str ] A list of input prompts. required Returns: Type Description list[str]: A list of generated responses corresponding to the input prompts. Note This method uses torch.no_grad() for inference to reduce memory usage. It handles both single and batch inputs, ensuring consistent output format. Source code in promptolution\\llms\\local_llm.py def get_response(self, prompts: list[str]): \"\"\" Generate responses for a list of prompts using the local language model. Args: prompts (list[str]): A list of input prompts. Returns: list[str]: A list of generated responses corresponding to the input prompts. Note: This method uses torch.no_grad() for inference to reduce memory usage. It handles both single and batch inputs, ensuring consistent output format. \"\"\" with torch.no_grad(): response = self.pipeline(prompts, pad_token_id=self.pipeline.tokenizer.eos_token_id) if len(response) != 1: response = [r[0] if isinstance(r, list) else r for r in response] response = [r[\"generated_text\"] for r in response] return response API LLM APILLM A class to interface with various language models through their respective APIs. This class supports Claude (Anthropic), GPT (OpenAI), and LLaMA (DeepInfra) models. It handles API key management, model initialization, and provides methods for both synchronous and asynchronous inference. Attributes: Name Type Description model The initialized language model instance. Methods: Name Description get_response Synchronously get responses for a list of prompts. _get_response Asynchronously get responses for a list of prompts. Source code in promptolution\\llms\\api_llm.py class APILLM: \"\"\" A class to interface with various language models through their respective APIs. This class supports Claude (Anthropic), GPT (OpenAI), and LLaMA (DeepInfra) models. It handles API key management, model initialization, and provides methods for both synchronous and asynchronous inference. Attributes: model: The initialized language model instance. Methods: get_response: Synchronously get responses for a list of prompts. _get_response: Asynchronously get responses for a list of prompts. \"\"\" def __init__(self, model_id: str): \"\"\" Initialize the APILLM with a specific model. Args: model_id (str): Identifier for the model to use. Raises: ValueError: If an unknown model identifier is provided. \"\"\" if \"claude\" in model_id: ANTHROPIC_API_KEY = open(\"anthropictoken.txt\", \"r\").read() self.model = ChatAnthropic(model=model_id, api_key=ANTHROPIC_API_KEY) elif \"gpt\" in model_id: OPENAI_API_KEY = open(\"openaitoken.txt\", \"r\").read() self.model = ChatOpenAI(model=model_id, api_key=OPENAI_API_KEY) elif \"llama\" in model_id: DEEPINFRA_API_KEY = open(\"deepinfratoken.txt\", \"r\").read() self.model = ChatDeepInfra(model_name=model_id, deepinfra_api_token=DEEPINFRA_API_KEY) else: raise ValueError(f\"Unknown model: {model_id}\") def get_response(self, prompts: List[str]) -> List[str]: \"\"\" Synchronously get responses for a list of prompts. This method includes retry logic for handling connection errors and rate limits. Args: prompts (list[str]): List of input prompts. Returns: list[str]: List of model responses. Raises: requests.exceptions.ConnectionError: If max retries are exceeded. \"\"\" max_retries = 100 delay = 3 attempts = 0 while attempts < max_retries: try: responses = asyncio.run(self._get_response(prompts)) return responses except requests.exceptions.ConnectionError as e: attempts += 1 logger.critical( f\"Connection error: {e}. Attempt {attempts}/{max_retries}. Retrying in {delay} seconds...\" ) time.sleep(delay) except openai.RateLimitError as e: attempts += 1 logger.critical( f\"Rate limit error: {e}. Attempt {attempts}/{max_retries}. Retrying in {delay} seconds...\" ) time.sleep(delay) # If the loop exits, it means max retries were reached raise requests.exceptions.ConnectionError(\"Max retries exceeded. Connection could not be established.\") async def _get_response( self, prompts: list[str], max_concurrent_calls=200 ) -> list[str]: \"\"\" Asynchronously get responses for a list of prompts. This method uses a semaphore to limit the number of concurrent API calls. Args: prompts (list[str]): List of input prompts. max_concurrent_calls (int): Maximum number of concurrent API calls allowed. Returns: list[str]: List of model responses. \"\"\" semaphore = asyncio.Semaphore(max_concurrent_calls) # Limit the number of concurrent calls tasks = [] for prompt in prompts: tasks.append(invoke_model(prompt, self.model, semaphore)) responses = await asyncio.gather(*tasks) return responses __init__(model_id) Initialize the APILLM with a specific model. Parameters: Name Type Description Default model_id str Identifier for the model to use. required Raises: Type Description ValueError If an unknown model identifier is provided. Source code in promptolution\\llms\\api_llm.py def __init__(self, model_id: str): \"\"\" Initialize the APILLM with a specific model. Args: model_id (str): Identifier for the model to use. Raises: ValueError: If an unknown model identifier is provided. \"\"\" if \"claude\" in model_id: ANTHROPIC_API_KEY = open(\"anthropictoken.txt\", \"r\").read() self.model = ChatAnthropic(model=model_id, api_key=ANTHROPIC_API_KEY) elif \"gpt\" in model_id: OPENAI_API_KEY = open(\"openaitoken.txt\", \"r\").read() self.model = ChatOpenAI(model=model_id, api_key=OPENAI_API_KEY) elif \"llama\" in model_id: DEEPINFRA_API_KEY = open(\"deepinfratoken.txt\", \"r\").read() self.model = ChatDeepInfra(model_name=model_id, deepinfra_api_token=DEEPINFRA_API_KEY) else: raise ValueError(f\"Unknown model: {model_id}\") get_response(prompts) Synchronously get responses for a list of prompts. This method includes retry logic for handling connection errors and rate limits. Parameters: Name Type Description Default prompts list [ str ] List of input prompts. required Returns: Type Description List [ str ] list[str]: List of model responses. Raises: Type Description ConnectionError If max retries are exceeded. Source code in promptolution\\llms\\api_llm.py def get_response(self, prompts: List[str]) -> List[str]: \"\"\" Synchronously get responses for a list of prompts. This method includes retry logic for handling connection errors and rate limits. Args: prompts (list[str]): List of input prompts. Returns: list[str]: List of model responses. Raises: requests.exceptions.ConnectionError: If max retries are exceeded. \"\"\" max_retries = 100 delay = 3 attempts = 0 while attempts < max_retries: try: responses = asyncio.run(self._get_response(prompts)) return responses except requests.exceptions.ConnectionError as e: attempts += 1 logger.critical( f\"Connection error: {e}. Attempt {attempts}/{max_retries}. Retrying in {delay} seconds...\" ) time.sleep(delay) except openai.RateLimitError as e: attempts += 1 logger.critical( f\"Rate limit error: {e}. Attempt {attempts}/{max_retries}. Retrying in {delay} seconds...\" ) time.sleep(delay) # If the loop exits, it means max retries were reached raise requests.exceptions.ConnectionError(\"Max retries exceeded. Connection could not be established.\") invoke_model(prompt, model, semaphore) async Asynchronously invoke a language model with retry logic. Parameters: Name Type Description Default prompt str The input prompt for the model. required model The language model to invoke. required semaphore Semaphore Semaphore to limit concurrent calls. required Returns: Name Type Description str The model's response content. Raises: Type Description ChatDeepInfraException If all retry attempts fail. Source code in promptolution\\llms\\api_llm.py async def invoke_model(prompt, model, semaphore): \"\"\" Asynchronously invoke a language model with retry logic. Args: prompt (str): The input prompt for the model. model: The language model to invoke. semaphore (asyncio.Semaphore): Semaphore to limit concurrent calls. Returns: str: The model's response content. Raises: ChatDeepInfraException: If all retry attempts fail. \"\"\" async with semaphore: max_retries = 100 delay = 3 attempts = 0 while attempts < max_retries: try: response = await asyncio.to_thread(model.invoke, [HumanMessage(content=prompt)]) return response.content except ChatDeepInfraException as e: print(f\"DeepInfra error: {e}. Attempt {attempts}/{max_retries}. Retrying in {delay} seconds...\") attempts += 1 time.sleep(delay) Base LLM BaseLLM Bases: ABC Abstract base class for Language Models in the promptolution library. This class defines the interface that all concrete LLM implementations should follow. Methods: Name Description get_response An abstract method that should be implemented by subclasses to generate responses for given prompts. Source code in promptolution\\llms\\base_llm.py class BaseLLM(ABC): \"\"\" Abstract base class for Language Models in the promptolution library. This class defines the interface that all concrete LLM implementations should follow. Methods: get_response: An abstract method that should be implemented by subclasses to generate responses for given prompts. \"\"\" def __init__(self, *args, **kwargs): pass @abstractmethod def get_response(self, prompts: List[str]) -> List[str]: \"\"\" Generate responses for the given prompts. This method should be implemented by subclasses to define how the LLM generates responses. Args: prompts (List[str]): A list of input prompts. Returns: List[str]: A list of generated responses corresponding to the input prompts. \"\"\" pass get_response(prompts) abstractmethod Generate responses for the given prompts. This method should be implemented by subclasses to define how the LLM generates responses. Parameters: Name Type Description Default prompts List [ str ] A list of input prompts. required Returns: Type Description List [ str ] List[str]: A list of generated responses corresponding to the input prompts. Source code in promptolution\\llms\\base_llm.py @abstractmethod def get_response(self, prompts: List[str]) -> List[str]: \"\"\" Generate responses for the given prompts. This method should be implemented by subclasses to define how the LLM generates responses. Args: prompts (List[str]): A list of input prompts. Returns: List[str]: A list of generated responses corresponding to the input prompts. \"\"\" pass DummyLLM Bases: BaseLLM A dummy implementation of the BaseLLM for testing purposes. This class generates random responses for given prompts, simulating the behavior of a language model without actually performing any complex natural language processing. Source code in promptolution\\llms\\base_llm.py class DummyLLM(BaseLLM): \"\"\" A dummy implementation of the BaseLLM for testing purposes. This class generates random responses for given prompts, simulating the behavior of a language model without actually performing any complex natural language processing. \"\"\" def __init__(self, *args, **kwargs): pass def get_response(self, prompts: str) -> str: \"\"\" Generate random responses for the given prompts. This method creates silly, random responses enclosed in <prompt> tags. It's designed for testing and demonstration purposes. Args: prompts (str or List[str]): Input prompt(s). If a single string is provided, it's converted to a list containing that string. Returns: List[str]: A list of randomly generated responses, one for each input prompt. \"\"\" if isinstance(prompts, str): prompts = [prompts] results = [] for _ in prompts: r = np.random.rand() if r < 0.3: results += [f\"Joooo wazzuppp <prompt>hier gehts los {r} </prompt>\"] if 0.3 <= r < 0.6: results += [f\"was das hier? <prompt>peter lustig{r}</prompt>\"] else: results += [f\"hier ist ein <prompt>test{r}</prompt>\"] return results get_response(prompts) Generate random responses for the given prompts. This method creates silly, random responses enclosed in tags. It's designed for testing and demonstration purposes. Parameters: Name Type Description Default prompts str or List [ str ] Input prompt(s). If a single string is provided, it's converted to a list containing that string. required Returns: Type Description str List[str]: A list of randomly generated responses, one for each input prompt. Source code in promptolution\\llms\\base_llm.py def get_response(self, prompts: str) -> str: \"\"\" Generate random responses for the given prompts. This method creates silly, random responses enclosed in <prompt> tags. It's designed for testing and demonstration purposes. Args: prompts (str or List[str]): Input prompt(s). If a single string is provided, it's converted to a list containing that string. Returns: List[str]: A list of randomly generated responses, one for each input prompt. \"\"\" if isinstance(prompts, str): prompts = [prompts] results = [] for _ in prompts: r = np.random.rand() if r < 0.3: results += [f\"Joooo wazzuppp <prompt>hier gehts los {r} </prompt>\"] if 0.3 <= r < 0.6: results += [f\"was das hier? <prompt>peter lustig{r}</prompt>\"] else: results += [f\"hier ist ein <prompt>test{r}</prompt>\"] return results DeepInfra LLM ChatDeepInfra Bases: BaseChatModel A chat model that uses the DeepInfra API. Source code in promptolution\\llms\\deepinfra.py class ChatDeepInfra(BaseChatModel): \"\"\"A chat model that uses the DeepInfra API.\"\"\" # client: Any #: :meta private: model_name: str = Field(alias=\"model\") \"\"\"The model name to use for the chat model.\"\"\" deepinfra_api_token: Optional[str] = None request_timeout: Optional[float] = Field(default=None, alias=\"timeout\") temperature: Optional[float] = 1 model_kwargs: Dict[str, Any] = Field(default_factory=dict) \"\"\"Run inference with this temperature. Must be in the closed interval [0.0, 1.0].\"\"\" top_p: Optional[float] = None \"\"\"Decode using nucleus sampling: consider the smallest set of tokens whose probability sum is at least top_p. Must be in the closed interval [0.0, 1.0].\"\"\" top_k: Optional[int] = None \"\"\"Decode using top-k sampling: consider the set of top_k most probable tokens. Must be positive.\"\"\" n: int = 1 \"\"\"Number of chat completions to generate for each prompt. Note that the API may not return the full n completions if duplicates are generated.\"\"\" max_tokens: int = 256 streaming: bool = False max_retries: int = 1 def __init__(self, model_name: str, **kwargs: Any): super().__init__(model=model_name, **kwargs) @property def _default_params(self) -> Dict[str, Any]: \"\"\"Get the default parameters for calling OpenAI API.\"\"\" return { \"model\": self.model_name, \"max_tokens\": self.max_tokens, \"stream\": self.streaming, \"n\": self.n, \"temperature\": self.temperature, \"request_timeout\": self.request_timeout, **self.model_kwargs, } @property def _client_params(self) -> Dict[str, Any]: \"\"\"Get the parameters used for the openai client.\"\"\" return {**self._default_params} def completion_with_retry(self, run_manager: Optional[CallbackManagerForLLMRun] = None, **kwargs: Any) -> Any: \"\"\"Use tenacity to retry the completion call.\"\"\" retry_decorator = _create_retry_decorator(self, run_manager=run_manager) @retry_decorator def _completion_with_retry(**kwargs: Any) -> Any: try: request_timeout = kwargs.pop(\"request_timeout\") request = Requests(headers=self._headers()) response = request.post(url=self._url(), data=self._body(kwargs), timeout=request_timeout) self._handle_status(response.status_code, response.text) return response except Exception as e: # import pdb; pdb.set_trace() print(\"EX\", e) # noqa: T201 raise return _completion_with_retry(**kwargs) async def acompletion_with_retry( self, run_manager: Optional[AsyncCallbackManagerForLLMRun] = None, **kwargs: Any, ) -> Any: \"\"\"Use tenacity to retry the async completion call.\"\"\" retry_decorator = _create_retry_decorator(self, run_manager=run_manager) @retry_decorator async def _completion_with_retry(**kwargs: Any) -> Any: try: request_timeout = kwargs.pop(\"request_timeout\") request = Requests(headers=self._headers()) async with request.apost(url=self._url(), data=self._body(kwargs), timeout=request_timeout) as response: self._handle_status(response.status, response.text) return await response.json() except Exception as e: print(\"EX\", e) # noqa: T201 raise return await _completion_with_retry(**kwargs) @root_validator(pre=True) def init_defaults(cls, values: Dict) -> Dict: \"\"\"Validate api key, python package exists, temperature, top_p, and top_k.\"\"\" # For compatibility with LiteLLM api_key = get_from_dict_or_env( values, \"deepinfra_api_key\", \"DEEPINFRA_API_KEY\", default=\"\", ) values[\"deepinfra_api_token\"] = get_from_dict_or_env( values, \"deepinfra_api_token\", \"DEEPINFRA_API_TOKEN\", default=api_key, ) # set model id # values[\"model_name\"] = get_from_dict_or_env( # values, # \"model_name\", # \"DEEPINFRA_MODEL_NAME\", # default=\"\", # ) return values @root_validator(pre=False, skip_on_failure=True) def validate_environment(cls, values: Dict) -> Dict: if values[\"temperature\"] is not None and not 0 <= values[\"temperature\"] <= 1: raise ValueError(\"temperature must be in the range [0.0, 1.0]\") if values[\"top_p\"] is not None and not 0 <= values[\"top_p\"] <= 1: raise ValueError(\"top_p must be in the range [0.0, 1.0]\") if values[\"top_k\"] is not None and values[\"top_k\"] <= 0: raise ValueError(\"top_k must be positive\") return values def _generate( self, messages: List[BaseMessage], stop: Optional[List[str]] = None, run_manager: Optional[CallbackManagerForLLMRun] = None, stream: Optional[bool] = None, **kwargs: Any, ) -> ChatResult: should_stream = stream if stream is not None else self.streaming if should_stream: stream_iter = self._stream(messages, stop=stop, run_manager=run_manager, **kwargs) return generate_from_stream(stream_iter) message_dicts, params = self._create_message_dicts(messages, stop) params = {**params, **kwargs} response = self.completion_with_retry(messages=message_dicts, run_manager=run_manager, **params) return self._create_chat_result(response.json()) def _create_chat_result(self, response: Mapping[str, Any]) -> ChatResult: generations = [] for res in response[\"choices\"]: message = _convert_dict_to_message(res[\"message\"]) gen = ChatGeneration( message=message, generation_info=dict(finish_reason=res.get(\"finish_reason\")), ) generations.append(gen) token_usage = response.get(\"usage\", {}) llm_output = {\"token_usage\": token_usage, \"model\": self.model_name} res = ChatResult(generations=generations, llm_output=llm_output) return res def _create_message_dicts( self, messages: List[BaseMessage], stop: Optional[List[str]] ) -> Tuple[List[Dict[str, Any]], Dict[str, Any]]: params = self._client_params if stop is not None: if \"stop\" in params: raise ValueError(\"`stop` found in both the input and default params.\") params[\"stop\"] = stop message_dicts = [_convert_message_to_dict(m) for m in messages] return message_dicts, params def _stream( self, messages: List[BaseMessage], stop: Optional[List[str]] = None, run_manager: Optional[CallbackManagerForLLMRun] = None, **kwargs: Any, ) -> Iterator[ChatGenerationChunk]: message_dicts, params = self._create_message_dicts(messages, stop) params = {**params, **kwargs, \"stream\": True} response = self.completion_with_retry(messages=message_dicts, run_manager=run_manager, **params) for line in _parse_stream(response.iter_lines()): chunk = _handle_sse_line(line) if chunk: cg_chunk = ChatGenerationChunk(message=chunk, generation_info=None) if run_manager: run_manager.on_llm_new_token(str(chunk.content), chunk=cg_chunk) yield cg_chunk async def _astream( self, messages: List[BaseMessage], stop: Optional[List[str]] = None, run_manager: Optional[AsyncCallbackManagerForLLMRun] = None, **kwargs: Any, ) -> AsyncIterator[ChatGenerationChunk]: message_dicts, params = self._create_message_dicts(messages, stop) params = {\"messages\": message_dicts, \"stream\": True, **params, **kwargs} request_timeout = params.pop(\"request_timeout\") request = Requests(headers=self._headers()) async with request.apost(url=self._url(), data=self._body(params), timeout=request_timeout) as response: async for line in _parse_stream_async(response.content): chunk = _handle_sse_line(line) if chunk: cg_chunk = ChatGenerationChunk(message=chunk, generation_info=None) if run_manager: await run_manager.on_llm_new_token(str(chunk.content), chunk=cg_chunk) yield cg_chunk async def _agenerate( self, messages: List[BaseMessage], stop: Optional[List[str]] = None, run_manager: Optional[AsyncCallbackManagerForLLMRun] = None, stream: Optional[bool] = None, **kwargs: Any, ) -> ChatResult: should_stream = stream if stream is not None else self.streaming if should_stream: stream_iter = self._astream(messages, stop=stop, run_manager=run_manager, **kwargs) return await agenerate_from_stream(stream_iter) message_dicts, params = self._create_message_dicts(messages, stop) params = {\"messages\": message_dicts, **params, **kwargs} res = await self.acompletion_with_retry(run_manager=run_manager, **params) return self._create_chat_result(res) @property def _identifying_params(self) -> Dict[str, Any]: \"\"\"Get the identifying parameters.\"\"\" return { \"model\": self.model_name, \"temperature\": self.temperature, \"top_p\": self.top_p, \"top_k\": self.top_k, \"n\": self.n, } @property def _llm_type(self) -> str: return \"deepinfra-chat\" def _handle_status(self, code: int, text: Any) -> None: if code >= 500: raise ChatDeepInfraException(f\"DeepInfra Server: Error {code}\") elif code >= 400: raise ValueError(f\"DeepInfra received an invalid payload: {text}\") elif code != 200: raise Exception(f\"DeepInfra returned an unexpected response with status \" f\"{code}: {text}\") def _url(self) -> str: return \"https://stage.api.deepinfra.com/v1/openai/chat/completions\" def _headers(self) -> Dict: return { \"Authorization\": f\"bearer {self.deepinfra_api_token}\", \"Content-Type\": \"application/json\", } def _body(self, kwargs: Any) -> Dict: return kwargs def bind_tools( self, tools: Sequence[Union[Dict[str, Any], Type[BaseModel], Callable, BaseTool]], **kwargs: Any, ) -> Runnable[LanguageModelInput, BaseMessage]: \"\"\"Bind tool-like objects to this chat model. Assumes model is compatible with OpenAI tool-calling API. Args: tools: A list of tool definitions to bind to this chat model. Can be a dictionary, pydantic model, callable, or BaseTool. Pydantic models, callables, and BaseTools will be automatically converted to their schema dictionary representation. **kwargs: Any additional parameters to pass to the :class:`~langchain.runnable.Runnable` constructor. \"\"\" formatted_tools = [convert_to_openai_tool(tool) for tool in tools] return super().bind(tools=formatted_tools, **kwargs) model_kwargs: Dict[str, Any] = Field(default_factory=dict) class-attribute instance-attribute Run inference with this temperature. Must be in the closed interval [0.0, 1.0]. model_name: str = Field(alias='model') class-attribute instance-attribute The model name to use for the chat model. n: int = 1 class-attribute instance-attribute Number of chat completions to generate for each prompt. Note that the API may not return the full n completions if duplicates are generated. top_k: Optional[int] = None class-attribute instance-attribute Decode using top-k sampling: consider the set of top_k most probable tokens. Must be positive. top_p: Optional[float] = None class-attribute instance-attribute Decode using nucleus sampling: consider the smallest set of tokens whose probability sum is at least top_p. Must be in the closed interval [0.0, 1.0]. acompletion_with_retry(run_manager=None, **kwargs) async Use tenacity to retry the async completion call. Source code in promptolution\\llms\\deepinfra.py async def acompletion_with_retry( self, run_manager: Optional[AsyncCallbackManagerForLLMRun] = None, **kwargs: Any, ) -> Any: \"\"\"Use tenacity to retry the async completion call.\"\"\" retry_decorator = _create_retry_decorator(self, run_manager=run_manager) @retry_decorator async def _completion_with_retry(**kwargs: Any) -> Any: try: request_timeout = kwargs.pop(\"request_timeout\") request = Requests(headers=self._headers()) async with request.apost(url=self._url(), data=self._body(kwargs), timeout=request_timeout) as response: self._handle_status(response.status, response.text) return await response.json() except Exception as e: print(\"EX\", e) # noqa: T201 raise return await _completion_with_retry(**kwargs) bind_tools(tools, **kwargs) Bind tool-like objects to this chat model. Assumes model is compatible with OpenAI tool-calling API. Parameters: Name Type Description Default tools Sequence [ Union [ Dict [ str , Any ], Type [ BaseModel ], Callable , BaseTool ]] A list of tool definitions to bind to this chat model. Can be a dictionary, pydantic model, callable, or BaseTool. Pydantic models, callables, and BaseTools will be automatically converted to their schema dictionary representation. required **kwargs Any Any additional parameters to pass to the :class: ~langchain.runnable.Runnable constructor. {} Source code in promptolution\\llms\\deepinfra.py def bind_tools( self, tools: Sequence[Union[Dict[str, Any], Type[BaseModel], Callable, BaseTool]], **kwargs: Any, ) -> Runnable[LanguageModelInput, BaseMessage]: \"\"\"Bind tool-like objects to this chat model. Assumes model is compatible with OpenAI tool-calling API. Args: tools: A list of tool definitions to bind to this chat model. Can be a dictionary, pydantic model, callable, or BaseTool. Pydantic models, callables, and BaseTools will be automatically converted to their schema dictionary representation. **kwargs: Any additional parameters to pass to the :class:`~langchain.runnable.Runnable` constructor. \"\"\" formatted_tools = [convert_to_openai_tool(tool) for tool in tools] return super().bind(tools=formatted_tools, **kwargs) completion_with_retry(run_manager=None, **kwargs) Use tenacity to retry the completion call. Source code in promptolution\\llms\\deepinfra.py def completion_with_retry(self, run_manager: Optional[CallbackManagerForLLMRun] = None, **kwargs: Any) -> Any: \"\"\"Use tenacity to retry the completion call.\"\"\" retry_decorator = _create_retry_decorator(self, run_manager=run_manager) @retry_decorator def _completion_with_retry(**kwargs: Any) -> Any: try: request_timeout = kwargs.pop(\"request_timeout\") request = Requests(headers=self._headers()) response = request.post(url=self._url(), data=self._body(kwargs), timeout=request_timeout) self._handle_status(response.status_code, response.text) return response except Exception as e: # import pdb; pdb.set_trace() print(\"EX\", e) # noqa: T201 raise return _completion_with_retry(**kwargs) init_defaults(values) Validate api key, python package exists, temperature, top_p, and top_k. Source code in promptolution\\llms\\deepinfra.py @root_validator(pre=True) def init_defaults(cls, values: Dict) -> Dict: \"\"\"Validate api key, python package exists, temperature, top_p, and top_k.\"\"\" # For compatibility with LiteLLM api_key = get_from_dict_or_env( values, \"deepinfra_api_key\", \"DEEPINFRA_API_KEY\", default=\"\", ) values[\"deepinfra_api_token\"] = get_from_dict_or_env( values, \"deepinfra_api_token\", \"DEEPINFRA_API_TOKEN\", default=api_key, ) # set model id # values[\"model_name\"] = get_from_dict_or_env( # values, # \"model_name\", # \"DEEPINFRA_MODEL_NAME\", # default=\"\", # ) return values Local LLM LocalLLM A class for running language models locally using the Hugging Face Transformers library. This class sets up a text generation pipeline with specified model parameters and provides a method to generate responses for given prompts. Attributes: Name Type Description pipeline Pipeline The text generation pipeline. Methods: Name Description get_response Generate responses for a list of prompts. Source code in promptolution\\llms\\local_llm.py class LocalLLM: \"\"\" A class for running language models locally using the Hugging Face Transformers library. This class sets up a text generation pipeline with specified model parameters and provides a method to generate responses for given prompts. Attributes: pipeline (transformers.Pipeline): The text generation pipeline. Methods: get_response: Generate responses for a list of prompts. \"\"\" def __init__(self, model_id: str, batch_size=8): \"\"\" Initialize the LocalLLM with a specific model. Args: model_id (str): The identifier of the model to use (e.g., \"gpt2\", \"facebook/opt-1.3b\"). batch_size (int, optional): The batch size for text generation. Defaults to 8. Note: This method sets up a text generation pipeline with bfloat16 precision, automatic device mapping, and specific generation parameters. \"\"\" self.pipeline = transformers.pipeline( \"text-generation\", model=model_id, model_kwargs={\"torch_dtype\": torch.bfloat16}, device_map=\"auto\", max_new_tokens=256, batch_size=batch_size, num_return_sequences=1, return_full_text=False, ) self.pipeline.tokenizer.pad_token_id = self.pipeline.tokenizer.eos_token_id self.pipeline.tokenizer.padding_side = \"left\" def get_response(self, prompts: list[str]): \"\"\" Generate responses for a list of prompts using the local language model. Args: prompts (list[str]): A list of input prompts. Returns: list[str]: A list of generated responses corresponding to the input prompts. Note: This method uses torch.no_grad() for inference to reduce memory usage. It handles both single and batch inputs, ensuring consistent output format. \"\"\" with torch.no_grad(): response = self.pipeline(prompts, pad_token_id=self.pipeline.tokenizer.eos_token_id) if len(response) != 1: response = [r[0] if isinstance(r, list) else r for r in response] response = [r[\"generated_text\"] for r in response] return response def __del__(self): try: del self.pipeline torch.cuda.empty_cache() except Exception as e: logger.warning(f\"Error during LocalLLM cleanup: {e}\") __init__(model_id, batch_size=8) Initialize the LocalLLM with a specific model. Parameters: Name Type Description Default model_id str The identifier of the model to use (e.g., \"gpt2\", \"facebook/opt-1.3b\"). required batch_size int The batch size for text generation. Defaults to 8. 8 Note This method sets up a text generation pipeline with bfloat16 precision, automatic device mapping, and specific generation parameters. Source code in promptolution\\llms\\local_llm.py def __init__(self, model_id: str, batch_size=8): \"\"\" Initialize the LocalLLM with a specific model. Args: model_id (str): The identifier of the model to use (e.g., \"gpt2\", \"facebook/opt-1.3b\"). batch_size (int, optional): The batch size for text generation. Defaults to 8. Note: This method sets up a text generation pipeline with bfloat16 precision, automatic device mapping, and specific generation parameters. \"\"\" self.pipeline = transformers.pipeline( \"text-generation\", model=model_id, model_kwargs={\"torch_dtype\": torch.bfloat16}, device_map=\"auto\", max_new_tokens=256, batch_size=batch_size, num_return_sequences=1, return_full_text=False, ) self.pipeline.tokenizer.pad_token_id = self.pipeline.tokenizer.eos_token_id self.pipeline.tokenizer.padding_side = \"left\" get_response(prompts) Generate responses for a list of prompts using the local language model. Parameters: Name Type Description Default prompts list [ str ] A list of input prompts. required Returns: Type Description list[str]: A list of generated responses corresponding to the input prompts. Note This method uses torch.no_grad() for inference to reduce memory usage. It handles both single and batch inputs, ensuring consistent output format. Source code in promptolution\\llms\\local_llm.py def get_response(self, prompts: list[str]): \"\"\" Generate responses for a list of prompts using the local language model. Args: prompts (list[str]): A list of input prompts. Returns: list[str]: A list of generated responses corresponding to the input prompts. Note: This method uses torch.no_grad() for inference to reduce memory usage. It handles both single and batch inputs, ensuring consistent output format. \"\"\" with torch.no_grad(): response = self.pipeline(prompts, pad_token_id=self.pipeline.tokenizer.eos_token_id) if len(response) != 1: response = [r[0] if isinstance(r, list) else r for r in response] response = [r[\"generated_text\"] for r in response] return response","title":"LLMs"},{"location":"api/llms/#llms","text":"This module contains the LLM (Large Language Model) implementations.","title":"LLMs"},{"location":"api/llms/#promptolution.llms.get_llm","text":"Factory function to create and return a language model instance based on the provided model_id. This function supports three types of language models: 1. DummyLLM: A mock LLM for testing purposes. 2. LocalLLM: For running models locally (identified by 'local' in the model_id). 3. APILLM: For API-based models (default if not matching other types). Parameters: Name Type Description Default model_id str Identifier for the model to use. Special cases: - \"dummy\" for DummyLLM - \"local-{model_name}\" for LocalLLM - Any other string for APILLM required *args Variable length argument list passed to the LLM constructor. () **kwargs Arbitrary keyword arguments passed to the LLM constructor. {} Returns: Type Description An instance of DummyLLM, LocalLLM, or APILLM based on the model_id. Source code in promptolution\\llms\\__init__.py def get_llm(model_id: str, *args, **kwargs): \"\"\" Factory function to create and return a language model instance based on the provided model_id. This function supports three types of language models: 1. DummyLLM: A mock LLM for testing purposes. 2. LocalLLM: For running models locally (identified by 'local' in the model_id). 3. APILLM: For API-based models (default if not matching other types). Args: model_id (str): Identifier for the model to use. Special cases: - \"dummy\" for DummyLLM - \"local-{model_name}\" for LocalLLM - Any other string for APILLM *args: Variable length argument list passed to the LLM constructor. **kwargs: Arbitrary keyword arguments passed to the LLM constructor. Returns: An instance of DummyLLM, LocalLLM, or APILLM based on the model_id. \"\"\" if model_id == \"dummy\": return DummyLLM(*args, **kwargs) if \"local\" in model_id: model_id = \"-\".join(model_id.split(\"-\")[1:]) return LocalLLM(model_id, *args, **kwargs) return APILLM(model_id, *args, **kwargs)","title":"get_llm"},{"location":"api/llms/#promptolution.llms.api_llm","text":"","title":"api_llm"},{"location":"api/llms/#promptolution.llms.api_llm.APILLM","text":"A class to interface with various language models through their respective APIs. This class supports Claude (Anthropic), GPT (OpenAI), and LLaMA (DeepInfra) models. It handles API key management, model initialization, and provides methods for both synchronous and asynchronous inference. Attributes: Name Type Description model The initialized language model instance. Methods: Name Description get_response Synchronously get responses for a list of prompts. _get_response Asynchronously get responses for a list of prompts. Source code in promptolution\\llms\\api_llm.py class APILLM: \"\"\" A class to interface with various language models through their respective APIs. This class supports Claude (Anthropic), GPT (OpenAI), and LLaMA (DeepInfra) models. It handles API key management, model initialization, and provides methods for both synchronous and asynchronous inference. Attributes: model: The initialized language model instance. Methods: get_response: Synchronously get responses for a list of prompts. _get_response: Asynchronously get responses for a list of prompts. \"\"\" def __init__(self, model_id: str): \"\"\" Initialize the APILLM with a specific model. Args: model_id (str): Identifier for the model to use. Raises: ValueError: If an unknown model identifier is provided. \"\"\" if \"claude\" in model_id: ANTHROPIC_API_KEY = open(\"anthropictoken.txt\", \"r\").read() self.model = ChatAnthropic(model=model_id, api_key=ANTHROPIC_API_KEY) elif \"gpt\" in model_id: OPENAI_API_KEY = open(\"openaitoken.txt\", \"r\").read() self.model = ChatOpenAI(model=model_id, api_key=OPENAI_API_KEY) elif \"llama\" in model_id: DEEPINFRA_API_KEY = open(\"deepinfratoken.txt\", \"r\").read() self.model = ChatDeepInfra(model_name=model_id, deepinfra_api_token=DEEPINFRA_API_KEY) else: raise ValueError(f\"Unknown model: {model_id}\") def get_response(self, prompts: List[str]) -> List[str]: \"\"\" Synchronously get responses for a list of prompts. This method includes retry logic for handling connection errors and rate limits. Args: prompts (list[str]): List of input prompts. Returns: list[str]: List of model responses. Raises: requests.exceptions.ConnectionError: If max retries are exceeded. \"\"\" max_retries = 100 delay = 3 attempts = 0 while attempts < max_retries: try: responses = asyncio.run(self._get_response(prompts)) return responses except requests.exceptions.ConnectionError as e: attempts += 1 logger.critical( f\"Connection error: {e}. Attempt {attempts}/{max_retries}. Retrying in {delay} seconds...\" ) time.sleep(delay) except openai.RateLimitError as e: attempts += 1 logger.critical( f\"Rate limit error: {e}. Attempt {attempts}/{max_retries}. Retrying in {delay} seconds...\" ) time.sleep(delay) # If the loop exits, it means max retries were reached raise requests.exceptions.ConnectionError(\"Max retries exceeded. Connection could not be established.\") async def _get_response( self, prompts: list[str], max_concurrent_calls=200 ) -> list[str]: \"\"\" Asynchronously get responses for a list of prompts. This method uses a semaphore to limit the number of concurrent API calls. Args: prompts (list[str]): List of input prompts. max_concurrent_calls (int): Maximum number of concurrent API calls allowed. Returns: list[str]: List of model responses. \"\"\" semaphore = asyncio.Semaphore(max_concurrent_calls) # Limit the number of concurrent calls tasks = [] for prompt in prompts: tasks.append(invoke_model(prompt, self.model, semaphore)) responses = await asyncio.gather(*tasks) return responses","title":"APILLM"},{"location":"api/llms/#promptolution.llms.api_llm.APILLM.__init__","text":"Initialize the APILLM with a specific model. Parameters: Name Type Description Default model_id str Identifier for the model to use. required Raises: Type Description ValueError If an unknown model identifier is provided. Source code in promptolution\\llms\\api_llm.py def __init__(self, model_id: str): \"\"\" Initialize the APILLM with a specific model. Args: model_id (str): Identifier for the model to use. Raises: ValueError: If an unknown model identifier is provided. \"\"\" if \"claude\" in model_id: ANTHROPIC_API_KEY = open(\"anthropictoken.txt\", \"r\").read() self.model = ChatAnthropic(model=model_id, api_key=ANTHROPIC_API_KEY) elif \"gpt\" in model_id: OPENAI_API_KEY = open(\"openaitoken.txt\", \"r\").read() self.model = ChatOpenAI(model=model_id, api_key=OPENAI_API_KEY) elif \"llama\" in model_id: DEEPINFRA_API_KEY = open(\"deepinfratoken.txt\", \"r\").read() self.model = ChatDeepInfra(model_name=model_id, deepinfra_api_token=DEEPINFRA_API_KEY) else: raise ValueError(f\"Unknown model: {model_id}\")","title":"__init__"},{"location":"api/llms/#promptolution.llms.api_llm.APILLM.get_response","text":"Synchronously get responses for a list of prompts. This method includes retry logic for handling connection errors and rate limits. Parameters: Name Type Description Default prompts list [ str ] List of input prompts. required Returns: Type Description List [ str ] list[str]: List of model responses. Raises: Type Description ConnectionError If max retries are exceeded. Source code in promptolution\\llms\\api_llm.py def get_response(self, prompts: List[str]) -> List[str]: \"\"\" Synchronously get responses for a list of prompts. This method includes retry logic for handling connection errors and rate limits. Args: prompts (list[str]): List of input prompts. Returns: list[str]: List of model responses. Raises: requests.exceptions.ConnectionError: If max retries are exceeded. \"\"\" max_retries = 100 delay = 3 attempts = 0 while attempts < max_retries: try: responses = asyncio.run(self._get_response(prompts)) return responses except requests.exceptions.ConnectionError as e: attempts += 1 logger.critical( f\"Connection error: {e}. Attempt {attempts}/{max_retries}. Retrying in {delay} seconds...\" ) time.sleep(delay) except openai.RateLimitError as e: attempts += 1 logger.critical( f\"Rate limit error: {e}. Attempt {attempts}/{max_retries}. Retrying in {delay} seconds...\" ) time.sleep(delay) # If the loop exits, it means max retries were reached raise requests.exceptions.ConnectionError(\"Max retries exceeded. Connection could not be established.\")","title":"get_response"},{"location":"api/llms/#promptolution.llms.api_llm.invoke_model","text":"Asynchronously invoke a language model with retry logic. Parameters: Name Type Description Default prompt str The input prompt for the model. required model The language model to invoke. required semaphore Semaphore Semaphore to limit concurrent calls. required Returns: Name Type Description str The model's response content. Raises: Type Description ChatDeepInfraException If all retry attempts fail. Source code in promptolution\\llms\\api_llm.py async def invoke_model(prompt, model, semaphore): \"\"\" Asynchronously invoke a language model with retry logic. Args: prompt (str): The input prompt for the model. model: The language model to invoke. semaphore (asyncio.Semaphore): Semaphore to limit concurrent calls. Returns: str: The model's response content. Raises: ChatDeepInfraException: If all retry attempts fail. \"\"\" async with semaphore: max_retries = 100 delay = 3 attempts = 0 while attempts < max_retries: try: response = await asyncio.to_thread(model.invoke, [HumanMessage(content=prompt)]) return response.content except ChatDeepInfraException as e: print(f\"DeepInfra error: {e}. Attempt {attempts}/{max_retries}. Retrying in {delay} seconds...\") attempts += 1 time.sleep(delay)","title":"invoke_model"},{"location":"api/llms/#promptolution.llms.base_llm","text":"","title":"base_llm"},{"location":"api/llms/#promptolution.llms.base_llm.BaseLLM","text":"Bases: ABC Abstract base class for Language Models in the promptolution library. This class defines the interface that all concrete LLM implementations should follow. Methods: Name Description get_response An abstract method that should be implemented by subclasses to generate responses for given prompts. Source code in promptolution\\llms\\base_llm.py class BaseLLM(ABC): \"\"\" Abstract base class for Language Models in the promptolution library. This class defines the interface that all concrete LLM implementations should follow. Methods: get_response: An abstract method that should be implemented by subclasses to generate responses for given prompts. \"\"\" def __init__(self, *args, **kwargs): pass @abstractmethod def get_response(self, prompts: List[str]) -> List[str]: \"\"\" Generate responses for the given prompts. This method should be implemented by subclasses to define how the LLM generates responses. Args: prompts (List[str]): A list of input prompts. Returns: List[str]: A list of generated responses corresponding to the input prompts. \"\"\" pass","title":"BaseLLM"},{"location":"api/llms/#promptolution.llms.base_llm.BaseLLM.get_response","text":"Generate responses for the given prompts. This method should be implemented by subclasses to define how the LLM generates responses. Parameters: Name Type Description Default prompts List [ str ] A list of input prompts. required Returns: Type Description List [ str ] List[str]: A list of generated responses corresponding to the input prompts. Source code in promptolution\\llms\\base_llm.py @abstractmethod def get_response(self, prompts: List[str]) -> List[str]: \"\"\" Generate responses for the given prompts. This method should be implemented by subclasses to define how the LLM generates responses. Args: prompts (List[str]): A list of input prompts. Returns: List[str]: A list of generated responses corresponding to the input prompts. \"\"\" pass","title":"get_response"},{"location":"api/llms/#promptolution.llms.base_llm.DummyLLM","text":"Bases: BaseLLM A dummy implementation of the BaseLLM for testing purposes. This class generates random responses for given prompts, simulating the behavior of a language model without actually performing any complex natural language processing. Source code in promptolution\\llms\\base_llm.py class DummyLLM(BaseLLM): \"\"\" A dummy implementation of the BaseLLM for testing purposes. This class generates random responses for given prompts, simulating the behavior of a language model without actually performing any complex natural language processing. \"\"\" def __init__(self, *args, **kwargs): pass def get_response(self, prompts: str) -> str: \"\"\" Generate random responses for the given prompts. This method creates silly, random responses enclosed in <prompt> tags. It's designed for testing and demonstration purposes. Args: prompts (str or List[str]): Input prompt(s). If a single string is provided, it's converted to a list containing that string. Returns: List[str]: A list of randomly generated responses, one for each input prompt. \"\"\" if isinstance(prompts, str): prompts = [prompts] results = [] for _ in prompts: r = np.random.rand() if r < 0.3: results += [f\"Joooo wazzuppp <prompt>hier gehts los {r} </prompt>\"] if 0.3 <= r < 0.6: results += [f\"was das hier? <prompt>peter lustig{r}</prompt>\"] else: results += [f\"hier ist ein <prompt>test{r}</prompt>\"] return results","title":"DummyLLM"},{"location":"api/llms/#promptolution.llms.base_llm.DummyLLM.get_response","text":"Generate random responses for the given prompts. This method creates silly, random responses enclosed in tags. It's designed for testing and demonstration purposes. Parameters: Name Type Description Default prompts str or List [ str ] Input prompt(s). If a single string is provided, it's converted to a list containing that string. required Returns: Type Description str List[str]: A list of randomly generated responses, one for each input prompt. Source code in promptolution\\llms\\base_llm.py def get_response(self, prompts: str) -> str: \"\"\" Generate random responses for the given prompts. This method creates silly, random responses enclosed in <prompt> tags. It's designed for testing and demonstration purposes. Args: prompts (str or List[str]): Input prompt(s). If a single string is provided, it's converted to a list containing that string. Returns: List[str]: A list of randomly generated responses, one for each input prompt. \"\"\" if isinstance(prompts, str): prompts = [prompts] results = [] for _ in prompts: r = np.random.rand() if r < 0.3: results += [f\"Joooo wazzuppp <prompt>hier gehts los {r} </prompt>\"] if 0.3 <= r < 0.6: results += [f\"was das hier? <prompt>peter lustig{r}</prompt>\"] else: results += [f\"hier ist ein <prompt>test{r}</prompt>\"] return results","title":"get_response"},{"location":"api/llms/#promptolution.llms.deepinfra","text":"","title":"deepinfra"},{"location":"api/llms/#promptolution.llms.deepinfra.ChatDeepInfra","text":"Bases: BaseChatModel A chat model that uses the DeepInfra API. Source code in promptolution\\llms\\deepinfra.py class ChatDeepInfra(BaseChatModel): \"\"\"A chat model that uses the DeepInfra API.\"\"\" # client: Any #: :meta private: model_name: str = Field(alias=\"model\") \"\"\"The model name to use for the chat model.\"\"\" deepinfra_api_token: Optional[str] = None request_timeout: Optional[float] = Field(default=None, alias=\"timeout\") temperature: Optional[float] = 1 model_kwargs: Dict[str, Any] = Field(default_factory=dict) \"\"\"Run inference with this temperature. Must be in the closed interval [0.0, 1.0].\"\"\" top_p: Optional[float] = None \"\"\"Decode using nucleus sampling: consider the smallest set of tokens whose probability sum is at least top_p. Must be in the closed interval [0.0, 1.0].\"\"\" top_k: Optional[int] = None \"\"\"Decode using top-k sampling: consider the set of top_k most probable tokens. Must be positive.\"\"\" n: int = 1 \"\"\"Number of chat completions to generate for each prompt. Note that the API may not return the full n completions if duplicates are generated.\"\"\" max_tokens: int = 256 streaming: bool = False max_retries: int = 1 def __init__(self, model_name: str, **kwargs: Any): super().__init__(model=model_name, **kwargs) @property def _default_params(self) -> Dict[str, Any]: \"\"\"Get the default parameters for calling OpenAI API.\"\"\" return { \"model\": self.model_name, \"max_tokens\": self.max_tokens, \"stream\": self.streaming, \"n\": self.n, \"temperature\": self.temperature, \"request_timeout\": self.request_timeout, **self.model_kwargs, } @property def _client_params(self) -> Dict[str, Any]: \"\"\"Get the parameters used for the openai client.\"\"\" return {**self._default_params} def completion_with_retry(self, run_manager: Optional[CallbackManagerForLLMRun] = None, **kwargs: Any) -> Any: \"\"\"Use tenacity to retry the completion call.\"\"\" retry_decorator = _create_retry_decorator(self, run_manager=run_manager) @retry_decorator def _completion_with_retry(**kwargs: Any) -> Any: try: request_timeout = kwargs.pop(\"request_timeout\") request = Requests(headers=self._headers()) response = request.post(url=self._url(), data=self._body(kwargs), timeout=request_timeout) self._handle_status(response.status_code, response.text) return response except Exception as e: # import pdb; pdb.set_trace() print(\"EX\", e) # noqa: T201 raise return _completion_with_retry(**kwargs) async def acompletion_with_retry( self, run_manager: Optional[AsyncCallbackManagerForLLMRun] = None, **kwargs: Any, ) -> Any: \"\"\"Use tenacity to retry the async completion call.\"\"\" retry_decorator = _create_retry_decorator(self, run_manager=run_manager) @retry_decorator async def _completion_with_retry(**kwargs: Any) -> Any: try: request_timeout = kwargs.pop(\"request_timeout\") request = Requests(headers=self._headers()) async with request.apost(url=self._url(), data=self._body(kwargs), timeout=request_timeout) as response: self._handle_status(response.status, response.text) return await response.json() except Exception as e: print(\"EX\", e) # noqa: T201 raise return await _completion_with_retry(**kwargs) @root_validator(pre=True) def init_defaults(cls, values: Dict) -> Dict: \"\"\"Validate api key, python package exists, temperature, top_p, and top_k.\"\"\" # For compatibility with LiteLLM api_key = get_from_dict_or_env( values, \"deepinfra_api_key\", \"DEEPINFRA_API_KEY\", default=\"\", ) values[\"deepinfra_api_token\"] = get_from_dict_or_env( values, \"deepinfra_api_token\", \"DEEPINFRA_API_TOKEN\", default=api_key, ) # set model id # values[\"model_name\"] = get_from_dict_or_env( # values, # \"model_name\", # \"DEEPINFRA_MODEL_NAME\", # default=\"\", # ) return values @root_validator(pre=False, skip_on_failure=True) def validate_environment(cls, values: Dict) -> Dict: if values[\"temperature\"] is not None and not 0 <= values[\"temperature\"] <= 1: raise ValueError(\"temperature must be in the range [0.0, 1.0]\") if values[\"top_p\"] is not None and not 0 <= values[\"top_p\"] <= 1: raise ValueError(\"top_p must be in the range [0.0, 1.0]\") if values[\"top_k\"] is not None and values[\"top_k\"] <= 0: raise ValueError(\"top_k must be positive\") return values def _generate( self, messages: List[BaseMessage], stop: Optional[List[str]] = None, run_manager: Optional[CallbackManagerForLLMRun] = None, stream: Optional[bool] = None, **kwargs: Any, ) -> ChatResult: should_stream = stream if stream is not None else self.streaming if should_stream: stream_iter = self._stream(messages, stop=stop, run_manager=run_manager, **kwargs) return generate_from_stream(stream_iter) message_dicts, params = self._create_message_dicts(messages, stop) params = {**params, **kwargs} response = self.completion_with_retry(messages=message_dicts, run_manager=run_manager, **params) return self._create_chat_result(response.json()) def _create_chat_result(self, response: Mapping[str, Any]) -> ChatResult: generations = [] for res in response[\"choices\"]: message = _convert_dict_to_message(res[\"message\"]) gen = ChatGeneration( message=message, generation_info=dict(finish_reason=res.get(\"finish_reason\")), ) generations.append(gen) token_usage = response.get(\"usage\", {}) llm_output = {\"token_usage\": token_usage, \"model\": self.model_name} res = ChatResult(generations=generations, llm_output=llm_output) return res def _create_message_dicts( self, messages: List[BaseMessage], stop: Optional[List[str]] ) -> Tuple[List[Dict[str, Any]], Dict[str, Any]]: params = self._client_params if stop is not None: if \"stop\" in params: raise ValueError(\"`stop` found in both the input and default params.\") params[\"stop\"] = stop message_dicts = [_convert_message_to_dict(m) for m in messages] return message_dicts, params def _stream( self, messages: List[BaseMessage], stop: Optional[List[str]] = None, run_manager: Optional[CallbackManagerForLLMRun] = None, **kwargs: Any, ) -> Iterator[ChatGenerationChunk]: message_dicts, params = self._create_message_dicts(messages, stop) params = {**params, **kwargs, \"stream\": True} response = self.completion_with_retry(messages=message_dicts, run_manager=run_manager, **params) for line in _parse_stream(response.iter_lines()): chunk = _handle_sse_line(line) if chunk: cg_chunk = ChatGenerationChunk(message=chunk, generation_info=None) if run_manager: run_manager.on_llm_new_token(str(chunk.content), chunk=cg_chunk) yield cg_chunk async def _astream( self, messages: List[BaseMessage], stop: Optional[List[str]] = None, run_manager: Optional[AsyncCallbackManagerForLLMRun] = None, **kwargs: Any, ) -> AsyncIterator[ChatGenerationChunk]: message_dicts, params = self._create_message_dicts(messages, stop) params = {\"messages\": message_dicts, \"stream\": True, **params, **kwargs} request_timeout = params.pop(\"request_timeout\") request = Requests(headers=self._headers()) async with request.apost(url=self._url(), data=self._body(params), timeout=request_timeout) as response: async for line in _parse_stream_async(response.content): chunk = _handle_sse_line(line) if chunk: cg_chunk = ChatGenerationChunk(message=chunk, generation_info=None) if run_manager: await run_manager.on_llm_new_token(str(chunk.content), chunk=cg_chunk) yield cg_chunk async def _agenerate( self, messages: List[BaseMessage], stop: Optional[List[str]] = None, run_manager: Optional[AsyncCallbackManagerForLLMRun] = None, stream: Optional[bool] = None, **kwargs: Any, ) -> ChatResult: should_stream = stream if stream is not None else self.streaming if should_stream: stream_iter = self._astream(messages, stop=stop, run_manager=run_manager, **kwargs) return await agenerate_from_stream(stream_iter) message_dicts, params = self._create_message_dicts(messages, stop) params = {\"messages\": message_dicts, **params, **kwargs} res = await self.acompletion_with_retry(run_manager=run_manager, **params) return self._create_chat_result(res) @property def _identifying_params(self) -> Dict[str, Any]: \"\"\"Get the identifying parameters.\"\"\" return { \"model\": self.model_name, \"temperature\": self.temperature, \"top_p\": self.top_p, \"top_k\": self.top_k, \"n\": self.n, } @property def _llm_type(self) -> str: return \"deepinfra-chat\" def _handle_status(self, code: int, text: Any) -> None: if code >= 500: raise ChatDeepInfraException(f\"DeepInfra Server: Error {code}\") elif code >= 400: raise ValueError(f\"DeepInfra received an invalid payload: {text}\") elif code != 200: raise Exception(f\"DeepInfra returned an unexpected response with status \" f\"{code}: {text}\") def _url(self) -> str: return \"https://stage.api.deepinfra.com/v1/openai/chat/completions\" def _headers(self) -> Dict: return { \"Authorization\": f\"bearer {self.deepinfra_api_token}\", \"Content-Type\": \"application/json\", } def _body(self, kwargs: Any) -> Dict: return kwargs def bind_tools( self, tools: Sequence[Union[Dict[str, Any], Type[BaseModel], Callable, BaseTool]], **kwargs: Any, ) -> Runnable[LanguageModelInput, BaseMessage]: \"\"\"Bind tool-like objects to this chat model. Assumes model is compatible with OpenAI tool-calling API. Args: tools: A list of tool definitions to bind to this chat model. Can be a dictionary, pydantic model, callable, or BaseTool. Pydantic models, callables, and BaseTools will be automatically converted to their schema dictionary representation. **kwargs: Any additional parameters to pass to the :class:`~langchain.runnable.Runnable` constructor. \"\"\" formatted_tools = [convert_to_openai_tool(tool) for tool in tools] return super().bind(tools=formatted_tools, **kwargs)","title":"ChatDeepInfra"},{"location":"api/llms/#promptolution.llms.deepinfra.ChatDeepInfra.model_kwargs","text":"Run inference with this temperature. Must be in the closed interval [0.0, 1.0].","title":"model_kwargs"},{"location":"api/llms/#promptolution.llms.deepinfra.ChatDeepInfra.model_name","text":"The model name to use for the chat model.","title":"model_name"},{"location":"api/llms/#promptolution.llms.deepinfra.ChatDeepInfra.n","text":"Number of chat completions to generate for each prompt. Note that the API may not return the full n completions if duplicates are generated.","title":"n"},{"location":"api/llms/#promptolution.llms.deepinfra.ChatDeepInfra.top_k","text":"Decode using top-k sampling: consider the set of top_k most probable tokens. Must be positive.","title":"top_k"},{"location":"api/llms/#promptolution.llms.deepinfra.ChatDeepInfra.top_p","text":"Decode using nucleus sampling: consider the smallest set of tokens whose probability sum is at least top_p. Must be in the closed interval [0.0, 1.0].","title":"top_p"},{"location":"api/llms/#promptolution.llms.deepinfra.ChatDeepInfra.acompletion_with_retry","text":"Use tenacity to retry the async completion call. Source code in promptolution\\llms\\deepinfra.py async def acompletion_with_retry( self, run_manager: Optional[AsyncCallbackManagerForLLMRun] = None, **kwargs: Any, ) -> Any: \"\"\"Use tenacity to retry the async completion call.\"\"\" retry_decorator = _create_retry_decorator(self, run_manager=run_manager) @retry_decorator async def _completion_with_retry(**kwargs: Any) -> Any: try: request_timeout = kwargs.pop(\"request_timeout\") request = Requests(headers=self._headers()) async with request.apost(url=self._url(), data=self._body(kwargs), timeout=request_timeout) as response: self._handle_status(response.status, response.text) return await response.json() except Exception as e: print(\"EX\", e) # noqa: T201 raise return await _completion_with_retry(**kwargs)","title":"acompletion_with_retry"},{"location":"api/llms/#promptolution.llms.deepinfra.ChatDeepInfra.bind_tools","text":"Bind tool-like objects to this chat model. Assumes model is compatible with OpenAI tool-calling API. Parameters: Name Type Description Default tools Sequence [ Union [ Dict [ str , Any ], Type [ BaseModel ], Callable , BaseTool ]] A list of tool definitions to bind to this chat model. Can be a dictionary, pydantic model, callable, or BaseTool. Pydantic models, callables, and BaseTools will be automatically converted to their schema dictionary representation. required **kwargs Any Any additional parameters to pass to the :class: ~langchain.runnable.Runnable constructor. {} Source code in promptolution\\llms\\deepinfra.py def bind_tools( self, tools: Sequence[Union[Dict[str, Any], Type[BaseModel], Callable, BaseTool]], **kwargs: Any, ) -> Runnable[LanguageModelInput, BaseMessage]: \"\"\"Bind tool-like objects to this chat model. Assumes model is compatible with OpenAI tool-calling API. Args: tools: A list of tool definitions to bind to this chat model. Can be a dictionary, pydantic model, callable, or BaseTool. Pydantic models, callables, and BaseTools will be automatically converted to their schema dictionary representation. **kwargs: Any additional parameters to pass to the :class:`~langchain.runnable.Runnable` constructor. \"\"\" formatted_tools = [convert_to_openai_tool(tool) for tool in tools] return super().bind(tools=formatted_tools, **kwargs)","title":"bind_tools"},{"location":"api/llms/#promptolution.llms.deepinfra.ChatDeepInfra.completion_with_retry","text":"Use tenacity to retry the completion call. Source code in promptolution\\llms\\deepinfra.py def completion_with_retry(self, run_manager: Optional[CallbackManagerForLLMRun] = None, **kwargs: Any) -> Any: \"\"\"Use tenacity to retry the completion call.\"\"\" retry_decorator = _create_retry_decorator(self, run_manager=run_manager) @retry_decorator def _completion_with_retry(**kwargs: Any) -> Any: try: request_timeout = kwargs.pop(\"request_timeout\") request = Requests(headers=self._headers()) response = request.post(url=self._url(), data=self._body(kwargs), timeout=request_timeout) self._handle_status(response.status_code, response.text) return response except Exception as e: # import pdb; pdb.set_trace() print(\"EX\", e) # noqa: T201 raise return _completion_with_retry(**kwargs)","title":"completion_with_retry"},{"location":"api/llms/#promptolution.llms.deepinfra.ChatDeepInfra.init_defaults","text":"Validate api key, python package exists, temperature, top_p, and top_k. Source code in promptolution\\llms\\deepinfra.py @root_validator(pre=True) def init_defaults(cls, values: Dict) -> Dict: \"\"\"Validate api key, python package exists, temperature, top_p, and top_k.\"\"\" # For compatibility with LiteLLM api_key = get_from_dict_or_env( values, \"deepinfra_api_key\", \"DEEPINFRA_API_KEY\", default=\"\", ) values[\"deepinfra_api_token\"] = get_from_dict_or_env( values, \"deepinfra_api_token\", \"DEEPINFRA_API_TOKEN\", default=api_key, ) # set model id # values[\"model_name\"] = get_from_dict_or_env( # values, # \"model_name\", # \"DEEPINFRA_MODEL_NAME\", # default=\"\", # ) return values","title":"init_defaults"},{"location":"api/llms/#promptolution.llms.local_llm","text":"","title":"local_llm"},{"location":"api/llms/#promptolution.llms.local_llm.LocalLLM","text":"A class for running language models locally using the Hugging Face Transformers library. This class sets up a text generation pipeline with specified model parameters and provides a method to generate responses for given prompts. Attributes: Name Type Description pipeline Pipeline The text generation pipeline. Methods: Name Description get_response Generate responses for a list of prompts. Source code in promptolution\\llms\\local_llm.py class LocalLLM: \"\"\" A class for running language models locally using the Hugging Face Transformers library. This class sets up a text generation pipeline with specified model parameters and provides a method to generate responses for given prompts. Attributes: pipeline (transformers.Pipeline): The text generation pipeline. Methods: get_response: Generate responses for a list of prompts. \"\"\" def __init__(self, model_id: str, batch_size=8): \"\"\" Initialize the LocalLLM with a specific model. Args: model_id (str): The identifier of the model to use (e.g., \"gpt2\", \"facebook/opt-1.3b\"). batch_size (int, optional): The batch size for text generation. Defaults to 8. Note: This method sets up a text generation pipeline with bfloat16 precision, automatic device mapping, and specific generation parameters. \"\"\" self.pipeline = transformers.pipeline( \"text-generation\", model=model_id, model_kwargs={\"torch_dtype\": torch.bfloat16}, device_map=\"auto\", max_new_tokens=256, batch_size=batch_size, num_return_sequences=1, return_full_text=False, ) self.pipeline.tokenizer.pad_token_id = self.pipeline.tokenizer.eos_token_id self.pipeline.tokenizer.padding_side = \"left\" def get_response(self, prompts: list[str]): \"\"\" Generate responses for a list of prompts using the local language model. Args: prompts (list[str]): A list of input prompts. Returns: list[str]: A list of generated responses corresponding to the input prompts. Note: This method uses torch.no_grad() for inference to reduce memory usage. It handles both single and batch inputs, ensuring consistent output format. \"\"\" with torch.no_grad(): response = self.pipeline(prompts, pad_token_id=self.pipeline.tokenizer.eos_token_id) if len(response) != 1: response = [r[0] if isinstance(r, list) else r for r in response] response = [r[\"generated_text\"] for r in response] return response def __del__(self): try: del self.pipeline torch.cuda.empty_cache() except Exception as e: logger.warning(f\"Error during LocalLLM cleanup: {e}\")","title":"LocalLLM"},{"location":"api/llms/#promptolution.llms.local_llm.LocalLLM.__init__","text":"Initialize the LocalLLM with a specific model. Parameters: Name Type Description Default model_id str The identifier of the model to use (e.g., \"gpt2\", \"facebook/opt-1.3b\"). required batch_size int The batch size for text generation. Defaults to 8. 8 Note This method sets up a text generation pipeline with bfloat16 precision, automatic device mapping, and specific generation parameters. Source code in promptolution\\llms\\local_llm.py def __init__(self, model_id: str, batch_size=8): \"\"\" Initialize the LocalLLM with a specific model. Args: model_id (str): The identifier of the model to use (e.g., \"gpt2\", \"facebook/opt-1.3b\"). batch_size (int, optional): The batch size for text generation. Defaults to 8. Note: This method sets up a text generation pipeline with bfloat16 precision, automatic device mapping, and specific generation parameters. \"\"\" self.pipeline = transformers.pipeline( \"text-generation\", model=model_id, model_kwargs={\"torch_dtype\": torch.bfloat16}, device_map=\"auto\", max_new_tokens=256, batch_size=batch_size, num_return_sequences=1, return_full_text=False, ) self.pipeline.tokenizer.pad_token_id = self.pipeline.tokenizer.eos_token_id self.pipeline.tokenizer.padding_side = \"left\"","title":"__init__"},{"location":"api/llms/#promptolution.llms.local_llm.LocalLLM.get_response","text":"Generate responses for a list of prompts using the local language model. Parameters: Name Type Description Default prompts list [ str ] A list of input prompts. required Returns: Type Description list[str]: A list of generated responses corresponding to the input prompts. Note This method uses torch.no_grad() for inference to reduce memory usage. It handles both single and batch inputs, ensuring consistent output format. Source code in promptolution\\llms\\local_llm.py def get_response(self, prompts: list[str]): \"\"\" Generate responses for a list of prompts using the local language model. Args: prompts (list[str]): A list of input prompts. Returns: list[str]: A list of generated responses corresponding to the input prompts. Note: This method uses torch.no_grad() for inference to reduce memory usage. It handles both single and batch inputs, ensuring consistent output format. \"\"\" with torch.no_grad(): response = self.pipeline(prompts, pad_token_id=self.pipeline.tokenizer.eos_token_id) if len(response) != 1: response = [r[0] if isinstance(r, list) else r for r in response] response = [r[\"generated_text\"] for r in response] return response","title":"get_response"},{"location":"api/llms/#api-llm","text":"","title":"API LLM"},{"location":"api/llms/#promptolution.llms.api_llm.APILLM","text":"A class to interface with various language models through their respective APIs. This class supports Claude (Anthropic), GPT (OpenAI), and LLaMA (DeepInfra) models. It handles API key management, model initialization, and provides methods for both synchronous and asynchronous inference. Attributes: Name Type Description model The initialized language model instance. Methods: Name Description get_response Synchronously get responses for a list of prompts. _get_response Asynchronously get responses for a list of prompts. Source code in promptolution\\llms\\api_llm.py class APILLM: \"\"\" A class to interface with various language models through their respective APIs. This class supports Claude (Anthropic), GPT (OpenAI), and LLaMA (DeepInfra) models. It handles API key management, model initialization, and provides methods for both synchronous and asynchronous inference. Attributes: model: The initialized language model instance. Methods: get_response: Synchronously get responses for a list of prompts. _get_response: Asynchronously get responses for a list of prompts. \"\"\" def __init__(self, model_id: str): \"\"\" Initialize the APILLM with a specific model. Args: model_id (str): Identifier for the model to use. Raises: ValueError: If an unknown model identifier is provided. \"\"\" if \"claude\" in model_id: ANTHROPIC_API_KEY = open(\"anthropictoken.txt\", \"r\").read() self.model = ChatAnthropic(model=model_id, api_key=ANTHROPIC_API_KEY) elif \"gpt\" in model_id: OPENAI_API_KEY = open(\"openaitoken.txt\", \"r\").read() self.model = ChatOpenAI(model=model_id, api_key=OPENAI_API_KEY) elif \"llama\" in model_id: DEEPINFRA_API_KEY = open(\"deepinfratoken.txt\", \"r\").read() self.model = ChatDeepInfra(model_name=model_id, deepinfra_api_token=DEEPINFRA_API_KEY) else: raise ValueError(f\"Unknown model: {model_id}\") def get_response(self, prompts: List[str]) -> List[str]: \"\"\" Synchronously get responses for a list of prompts. This method includes retry logic for handling connection errors and rate limits. Args: prompts (list[str]): List of input prompts. Returns: list[str]: List of model responses. Raises: requests.exceptions.ConnectionError: If max retries are exceeded. \"\"\" max_retries = 100 delay = 3 attempts = 0 while attempts < max_retries: try: responses = asyncio.run(self._get_response(prompts)) return responses except requests.exceptions.ConnectionError as e: attempts += 1 logger.critical( f\"Connection error: {e}. Attempt {attempts}/{max_retries}. Retrying in {delay} seconds...\" ) time.sleep(delay) except openai.RateLimitError as e: attempts += 1 logger.critical( f\"Rate limit error: {e}. Attempt {attempts}/{max_retries}. Retrying in {delay} seconds...\" ) time.sleep(delay) # If the loop exits, it means max retries were reached raise requests.exceptions.ConnectionError(\"Max retries exceeded. Connection could not be established.\") async def _get_response( self, prompts: list[str], max_concurrent_calls=200 ) -> list[str]: \"\"\" Asynchronously get responses for a list of prompts. This method uses a semaphore to limit the number of concurrent API calls. Args: prompts (list[str]): List of input prompts. max_concurrent_calls (int): Maximum number of concurrent API calls allowed. Returns: list[str]: List of model responses. \"\"\" semaphore = asyncio.Semaphore(max_concurrent_calls) # Limit the number of concurrent calls tasks = [] for prompt in prompts: tasks.append(invoke_model(prompt, self.model, semaphore)) responses = await asyncio.gather(*tasks) return responses","title":"APILLM"},{"location":"api/llms/#promptolution.llms.api_llm.APILLM.__init__","text":"Initialize the APILLM with a specific model. Parameters: Name Type Description Default model_id str Identifier for the model to use. required Raises: Type Description ValueError If an unknown model identifier is provided. Source code in promptolution\\llms\\api_llm.py def __init__(self, model_id: str): \"\"\" Initialize the APILLM with a specific model. Args: model_id (str): Identifier for the model to use. Raises: ValueError: If an unknown model identifier is provided. \"\"\" if \"claude\" in model_id: ANTHROPIC_API_KEY = open(\"anthropictoken.txt\", \"r\").read() self.model = ChatAnthropic(model=model_id, api_key=ANTHROPIC_API_KEY) elif \"gpt\" in model_id: OPENAI_API_KEY = open(\"openaitoken.txt\", \"r\").read() self.model = ChatOpenAI(model=model_id, api_key=OPENAI_API_KEY) elif \"llama\" in model_id: DEEPINFRA_API_KEY = open(\"deepinfratoken.txt\", \"r\").read() self.model = ChatDeepInfra(model_name=model_id, deepinfra_api_token=DEEPINFRA_API_KEY) else: raise ValueError(f\"Unknown model: {model_id}\")","title":"__init__"},{"location":"api/llms/#promptolution.llms.api_llm.APILLM.get_response","text":"Synchronously get responses for a list of prompts. This method includes retry logic for handling connection errors and rate limits. Parameters: Name Type Description Default prompts list [ str ] List of input prompts. required Returns: Type Description List [ str ] list[str]: List of model responses. Raises: Type Description ConnectionError If max retries are exceeded. Source code in promptolution\\llms\\api_llm.py def get_response(self, prompts: List[str]) -> List[str]: \"\"\" Synchronously get responses for a list of prompts. This method includes retry logic for handling connection errors and rate limits. Args: prompts (list[str]): List of input prompts. Returns: list[str]: List of model responses. Raises: requests.exceptions.ConnectionError: If max retries are exceeded. \"\"\" max_retries = 100 delay = 3 attempts = 0 while attempts < max_retries: try: responses = asyncio.run(self._get_response(prompts)) return responses except requests.exceptions.ConnectionError as e: attempts += 1 logger.critical( f\"Connection error: {e}. Attempt {attempts}/{max_retries}. Retrying in {delay} seconds...\" ) time.sleep(delay) except openai.RateLimitError as e: attempts += 1 logger.critical( f\"Rate limit error: {e}. Attempt {attempts}/{max_retries}. Retrying in {delay} seconds...\" ) time.sleep(delay) # If the loop exits, it means max retries were reached raise requests.exceptions.ConnectionError(\"Max retries exceeded. Connection could not be established.\")","title":"get_response"},{"location":"api/llms/#promptolution.llms.api_llm.invoke_model","text":"Asynchronously invoke a language model with retry logic. Parameters: Name Type Description Default prompt str The input prompt for the model. required model The language model to invoke. required semaphore Semaphore Semaphore to limit concurrent calls. required Returns: Name Type Description str The model's response content. Raises: Type Description ChatDeepInfraException If all retry attempts fail. Source code in promptolution\\llms\\api_llm.py async def invoke_model(prompt, model, semaphore): \"\"\" Asynchronously invoke a language model with retry logic. Args: prompt (str): The input prompt for the model. model: The language model to invoke. semaphore (asyncio.Semaphore): Semaphore to limit concurrent calls. Returns: str: The model's response content. Raises: ChatDeepInfraException: If all retry attempts fail. \"\"\" async with semaphore: max_retries = 100 delay = 3 attempts = 0 while attempts < max_retries: try: response = await asyncio.to_thread(model.invoke, [HumanMessage(content=prompt)]) return response.content except ChatDeepInfraException as e: print(f\"DeepInfra error: {e}. Attempt {attempts}/{max_retries}. Retrying in {delay} seconds...\") attempts += 1 time.sleep(delay)","title":"invoke_model"},{"location":"api/llms/#base-llm","text":"","title":"Base LLM"},{"location":"api/llms/#promptolution.llms.base_llm.BaseLLM","text":"Bases: ABC Abstract base class for Language Models in the promptolution library. This class defines the interface that all concrete LLM implementations should follow. Methods: Name Description get_response An abstract method that should be implemented by subclasses to generate responses for given prompts. Source code in promptolution\\llms\\base_llm.py class BaseLLM(ABC): \"\"\" Abstract base class for Language Models in the promptolution library. This class defines the interface that all concrete LLM implementations should follow. Methods: get_response: An abstract method that should be implemented by subclasses to generate responses for given prompts. \"\"\" def __init__(self, *args, **kwargs): pass @abstractmethod def get_response(self, prompts: List[str]) -> List[str]: \"\"\" Generate responses for the given prompts. This method should be implemented by subclasses to define how the LLM generates responses. Args: prompts (List[str]): A list of input prompts. Returns: List[str]: A list of generated responses corresponding to the input prompts. \"\"\" pass","title":"BaseLLM"},{"location":"api/llms/#promptolution.llms.base_llm.BaseLLM.get_response","text":"Generate responses for the given prompts. This method should be implemented by subclasses to define how the LLM generates responses. Parameters: Name Type Description Default prompts List [ str ] A list of input prompts. required Returns: Type Description List [ str ] List[str]: A list of generated responses corresponding to the input prompts. Source code in promptolution\\llms\\base_llm.py @abstractmethod def get_response(self, prompts: List[str]) -> List[str]: \"\"\" Generate responses for the given prompts. This method should be implemented by subclasses to define how the LLM generates responses. Args: prompts (List[str]): A list of input prompts. Returns: List[str]: A list of generated responses corresponding to the input prompts. \"\"\" pass","title":"get_response"},{"location":"api/llms/#promptolution.llms.base_llm.DummyLLM","text":"Bases: BaseLLM A dummy implementation of the BaseLLM for testing purposes. This class generates random responses for given prompts, simulating the behavior of a language model without actually performing any complex natural language processing. Source code in promptolution\\llms\\base_llm.py class DummyLLM(BaseLLM): \"\"\" A dummy implementation of the BaseLLM for testing purposes. This class generates random responses for given prompts, simulating the behavior of a language model without actually performing any complex natural language processing. \"\"\" def __init__(self, *args, **kwargs): pass def get_response(self, prompts: str) -> str: \"\"\" Generate random responses for the given prompts. This method creates silly, random responses enclosed in <prompt> tags. It's designed for testing and demonstration purposes. Args: prompts (str or List[str]): Input prompt(s). If a single string is provided, it's converted to a list containing that string. Returns: List[str]: A list of randomly generated responses, one for each input prompt. \"\"\" if isinstance(prompts, str): prompts = [prompts] results = [] for _ in prompts: r = np.random.rand() if r < 0.3: results += [f\"Joooo wazzuppp <prompt>hier gehts los {r} </prompt>\"] if 0.3 <= r < 0.6: results += [f\"was das hier? <prompt>peter lustig{r}</prompt>\"] else: results += [f\"hier ist ein <prompt>test{r}</prompt>\"] return results","title":"DummyLLM"},{"location":"api/llms/#promptolution.llms.base_llm.DummyLLM.get_response","text":"Generate random responses for the given prompts. This method creates silly, random responses enclosed in tags. It's designed for testing and demonstration purposes. Parameters: Name Type Description Default prompts str or List [ str ] Input prompt(s). If a single string is provided, it's converted to a list containing that string. required Returns: Type Description str List[str]: A list of randomly generated responses, one for each input prompt. Source code in promptolution\\llms\\base_llm.py def get_response(self, prompts: str) -> str: \"\"\" Generate random responses for the given prompts. This method creates silly, random responses enclosed in <prompt> tags. It's designed for testing and demonstration purposes. Args: prompts (str or List[str]): Input prompt(s). If a single string is provided, it's converted to a list containing that string. Returns: List[str]: A list of randomly generated responses, one for each input prompt. \"\"\" if isinstance(prompts, str): prompts = [prompts] results = [] for _ in prompts: r = np.random.rand() if r < 0.3: results += [f\"Joooo wazzuppp <prompt>hier gehts los {r} </prompt>\"] if 0.3 <= r < 0.6: results += [f\"was das hier? <prompt>peter lustig{r}</prompt>\"] else: results += [f\"hier ist ein <prompt>test{r}</prompt>\"] return results","title":"get_response"},{"location":"api/llms/#deepinfra-llm","text":"","title":"DeepInfra LLM"},{"location":"api/llms/#promptolution.llms.deepinfra.ChatDeepInfra","text":"Bases: BaseChatModel A chat model that uses the DeepInfra API. Source code in promptolution\\llms\\deepinfra.py class ChatDeepInfra(BaseChatModel): \"\"\"A chat model that uses the DeepInfra API.\"\"\" # client: Any #: :meta private: model_name: str = Field(alias=\"model\") \"\"\"The model name to use for the chat model.\"\"\" deepinfra_api_token: Optional[str] = None request_timeout: Optional[float] = Field(default=None, alias=\"timeout\") temperature: Optional[float] = 1 model_kwargs: Dict[str, Any] = Field(default_factory=dict) \"\"\"Run inference with this temperature. Must be in the closed interval [0.0, 1.0].\"\"\" top_p: Optional[float] = None \"\"\"Decode using nucleus sampling: consider the smallest set of tokens whose probability sum is at least top_p. Must be in the closed interval [0.0, 1.0].\"\"\" top_k: Optional[int] = None \"\"\"Decode using top-k sampling: consider the set of top_k most probable tokens. Must be positive.\"\"\" n: int = 1 \"\"\"Number of chat completions to generate for each prompt. Note that the API may not return the full n completions if duplicates are generated.\"\"\" max_tokens: int = 256 streaming: bool = False max_retries: int = 1 def __init__(self, model_name: str, **kwargs: Any): super().__init__(model=model_name, **kwargs) @property def _default_params(self) -> Dict[str, Any]: \"\"\"Get the default parameters for calling OpenAI API.\"\"\" return { \"model\": self.model_name, \"max_tokens\": self.max_tokens, \"stream\": self.streaming, \"n\": self.n, \"temperature\": self.temperature, \"request_timeout\": self.request_timeout, **self.model_kwargs, } @property def _client_params(self) -> Dict[str, Any]: \"\"\"Get the parameters used for the openai client.\"\"\" return {**self._default_params} def completion_with_retry(self, run_manager: Optional[CallbackManagerForLLMRun] = None, **kwargs: Any) -> Any: \"\"\"Use tenacity to retry the completion call.\"\"\" retry_decorator = _create_retry_decorator(self, run_manager=run_manager) @retry_decorator def _completion_with_retry(**kwargs: Any) -> Any: try: request_timeout = kwargs.pop(\"request_timeout\") request = Requests(headers=self._headers()) response = request.post(url=self._url(), data=self._body(kwargs), timeout=request_timeout) self._handle_status(response.status_code, response.text) return response except Exception as e: # import pdb; pdb.set_trace() print(\"EX\", e) # noqa: T201 raise return _completion_with_retry(**kwargs) async def acompletion_with_retry( self, run_manager: Optional[AsyncCallbackManagerForLLMRun] = None, **kwargs: Any, ) -> Any: \"\"\"Use tenacity to retry the async completion call.\"\"\" retry_decorator = _create_retry_decorator(self, run_manager=run_manager) @retry_decorator async def _completion_with_retry(**kwargs: Any) -> Any: try: request_timeout = kwargs.pop(\"request_timeout\") request = Requests(headers=self._headers()) async with request.apost(url=self._url(), data=self._body(kwargs), timeout=request_timeout) as response: self._handle_status(response.status, response.text) return await response.json() except Exception as e: print(\"EX\", e) # noqa: T201 raise return await _completion_with_retry(**kwargs) @root_validator(pre=True) def init_defaults(cls, values: Dict) -> Dict: \"\"\"Validate api key, python package exists, temperature, top_p, and top_k.\"\"\" # For compatibility with LiteLLM api_key = get_from_dict_or_env( values, \"deepinfra_api_key\", \"DEEPINFRA_API_KEY\", default=\"\", ) values[\"deepinfra_api_token\"] = get_from_dict_or_env( values, \"deepinfra_api_token\", \"DEEPINFRA_API_TOKEN\", default=api_key, ) # set model id # values[\"model_name\"] = get_from_dict_or_env( # values, # \"model_name\", # \"DEEPINFRA_MODEL_NAME\", # default=\"\", # ) return values @root_validator(pre=False, skip_on_failure=True) def validate_environment(cls, values: Dict) -> Dict: if values[\"temperature\"] is not None and not 0 <= values[\"temperature\"] <= 1: raise ValueError(\"temperature must be in the range [0.0, 1.0]\") if values[\"top_p\"] is not None and not 0 <= values[\"top_p\"] <= 1: raise ValueError(\"top_p must be in the range [0.0, 1.0]\") if values[\"top_k\"] is not None and values[\"top_k\"] <= 0: raise ValueError(\"top_k must be positive\") return values def _generate( self, messages: List[BaseMessage], stop: Optional[List[str]] = None, run_manager: Optional[CallbackManagerForLLMRun] = None, stream: Optional[bool] = None, **kwargs: Any, ) -> ChatResult: should_stream = stream if stream is not None else self.streaming if should_stream: stream_iter = self._stream(messages, stop=stop, run_manager=run_manager, **kwargs) return generate_from_stream(stream_iter) message_dicts, params = self._create_message_dicts(messages, stop) params = {**params, **kwargs} response = self.completion_with_retry(messages=message_dicts, run_manager=run_manager, **params) return self._create_chat_result(response.json()) def _create_chat_result(self, response: Mapping[str, Any]) -> ChatResult: generations = [] for res in response[\"choices\"]: message = _convert_dict_to_message(res[\"message\"]) gen = ChatGeneration( message=message, generation_info=dict(finish_reason=res.get(\"finish_reason\")), ) generations.append(gen) token_usage = response.get(\"usage\", {}) llm_output = {\"token_usage\": token_usage, \"model\": self.model_name} res = ChatResult(generations=generations, llm_output=llm_output) return res def _create_message_dicts( self, messages: List[BaseMessage], stop: Optional[List[str]] ) -> Tuple[List[Dict[str, Any]], Dict[str, Any]]: params = self._client_params if stop is not None: if \"stop\" in params: raise ValueError(\"`stop` found in both the input and default params.\") params[\"stop\"] = stop message_dicts = [_convert_message_to_dict(m) for m in messages] return message_dicts, params def _stream( self, messages: List[BaseMessage], stop: Optional[List[str]] = None, run_manager: Optional[CallbackManagerForLLMRun] = None, **kwargs: Any, ) -> Iterator[ChatGenerationChunk]: message_dicts, params = self._create_message_dicts(messages, stop) params = {**params, **kwargs, \"stream\": True} response = self.completion_with_retry(messages=message_dicts, run_manager=run_manager, **params) for line in _parse_stream(response.iter_lines()): chunk = _handle_sse_line(line) if chunk: cg_chunk = ChatGenerationChunk(message=chunk, generation_info=None) if run_manager: run_manager.on_llm_new_token(str(chunk.content), chunk=cg_chunk) yield cg_chunk async def _astream( self, messages: List[BaseMessage], stop: Optional[List[str]] = None, run_manager: Optional[AsyncCallbackManagerForLLMRun] = None, **kwargs: Any, ) -> AsyncIterator[ChatGenerationChunk]: message_dicts, params = self._create_message_dicts(messages, stop) params = {\"messages\": message_dicts, \"stream\": True, **params, **kwargs} request_timeout = params.pop(\"request_timeout\") request = Requests(headers=self._headers()) async with request.apost(url=self._url(), data=self._body(params), timeout=request_timeout) as response: async for line in _parse_stream_async(response.content): chunk = _handle_sse_line(line) if chunk: cg_chunk = ChatGenerationChunk(message=chunk, generation_info=None) if run_manager: await run_manager.on_llm_new_token(str(chunk.content), chunk=cg_chunk) yield cg_chunk async def _agenerate( self, messages: List[BaseMessage], stop: Optional[List[str]] = None, run_manager: Optional[AsyncCallbackManagerForLLMRun] = None, stream: Optional[bool] = None, **kwargs: Any, ) -> ChatResult: should_stream = stream if stream is not None else self.streaming if should_stream: stream_iter = self._astream(messages, stop=stop, run_manager=run_manager, **kwargs) return await agenerate_from_stream(stream_iter) message_dicts, params = self._create_message_dicts(messages, stop) params = {\"messages\": message_dicts, **params, **kwargs} res = await self.acompletion_with_retry(run_manager=run_manager, **params) return self._create_chat_result(res) @property def _identifying_params(self) -> Dict[str, Any]: \"\"\"Get the identifying parameters.\"\"\" return { \"model\": self.model_name, \"temperature\": self.temperature, \"top_p\": self.top_p, \"top_k\": self.top_k, \"n\": self.n, } @property def _llm_type(self) -> str: return \"deepinfra-chat\" def _handle_status(self, code: int, text: Any) -> None: if code >= 500: raise ChatDeepInfraException(f\"DeepInfra Server: Error {code}\") elif code >= 400: raise ValueError(f\"DeepInfra received an invalid payload: {text}\") elif code != 200: raise Exception(f\"DeepInfra returned an unexpected response with status \" f\"{code}: {text}\") def _url(self) -> str: return \"https://stage.api.deepinfra.com/v1/openai/chat/completions\" def _headers(self) -> Dict: return { \"Authorization\": f\"bearer {self.deepinfra_api_token}\", \"Content-Type\": \"application/json\", } def _body(self, kwargs: Any) -> Dict: return kwargs def bind_tools( self, tools: Sequence[Union[Dict[str, Any], Type[BaseModel], Callable, BaseTool]], **kwargs: Any, ) -> Runnable[LanguageModelInput, BaseMessage]: \"\"\"Bind tool-like objects to this chat model. Assumes model is compatible with OpenAI tool-calling API. Args: tools: A list of tool definitions to bind to this chat model. Can be a dictionary, pydantic model, callable, or BaseTool. Pydantic models, callables, and BaseTools will be automatically converted to their schema dictionary representation. **kwargs: Any additional parameters to pass to the :class:`~langchain.runnable.Runnable` constructor. \"\"\" formatted_tools = [convert_to_openai_tool(tool) for tool in tools] return super().bind(tools=formatted_tools, **kwargs)","title":"ChatDeepInfra"},{"location":"api/llms/#promptolution.llms.deepinfra.ChatDeepInfra.model_kwargs","text":"Run inference with this temperature. Must be in the closed interval [0.0, 1.0].","title":"model_kwargs"},{"location":"api/llms/#promptolution.llms.deepinfra.ChatDeepInfra.model_name","text":"The model name to use for the chat model.","title":"model_name"},{"location":"api/llms/#promptolution.llms.deepinfra.ChatDeepInfra.n","text":"Number of chat completions to generate for each prompt. Note that the API may not return the full n completions if duplicates are generated.","title":"n"},{"location":"api/llms/#promptolution.llms.deepinfra.ChatDeepInfra.top_k","text":"Decode using top-k sampling: consider the set of top_k most probable tokens. Must be positive.","title":"top_k"},{"location":"api/llms/#promptolution.llms.deepinfra.ChatDeepInfra.top_p","text":"Decode using nucleus sampling: consider the smallest set of tokens whose probability sum is at least top_p. Must be in the closed interval [0.0, 1.0].","title":"top_p"},{"location":"api/llms/#promptolution.llms.deepinfra.ChatDeepInfra.acompletion_with_retry","text":"Use tenacity to retry the async completion call. Source code in promptolution\\llms\\deepinfra.py async def acompletion_with_retry( self, run_manager: Optional[AsyncCallbackManagerForLLMRun] = None, **kwargs: Any, ) -> Any: \"\"\"Use tenacity to retry the async completion call.\"\"\" retry_decorator = _create_retry_decorator(self, run_manager=run_manager) @retry_decorator async def _completion_with_retry(**kwargs: Any) -> Any: try: request_timeout = kwargs.pop(\"request_timeout\") request = Requests(headers=self._headers()) async with request.apost(url=self._url(), data=self._body(kwargs), timeout=request_timeout) as response: self._handle_status(response.status, response.text) return await response.json() except Exception as e: print(\"EX\", e) # noqa: T201 raise return await _completion_with_retry(**kwargs)","title":"acompletion_with_retry"},{"location":"api/llms/#promptolution.llms.deepinfra.ChatDeepInfra.bind_tools","text":"Bind tool-like objects to this chat model. Assumes model is compatible with OpenAI tool-calling API. Parameters: Name Type Description Default tools Sequence [ Union [ Dict [ str , Any ], Type [ BaseModel ], Callable , BaseTool ]] A list of tool definitions to bind to this chat model. Can be a dictionary, pydantic model, callable, or BaseTool. Pydantic models, callables, and BaseTools will be automatically converted to their schema dictionary representation. required **kwargs Any Any additional parameters to pass to the :class: ~langchain.runnable.Runnable constructor. {} Source code in promptolution\\llms\\deepinfra.py def bind_tools( self, tools: Sequence[Union[Dict[str, Any], Type[BaseModel], Callable, BaseTool]], **kwargs: Any, ) -> Runnable[LanguageModelInput, BaseMessage]: \"\"\"Bind tool-like objects to this chat model. Assumes model is compatible with OpenAI tool-calling API. Args: tools: A list of tool definitions to bind to this chat model. Can be a dictionary, pydantic model, callable, or BaseTool. Pydantic models, callables, and BaseTools will be automatically converted to their schema dictionary representation. **kwargs: Any additional parameters to pass to the :class:`~langchain.runnable.Runnable` constructor. \"\"\" formatted_tools = [convert_to_openai_tool(tool) for tool in tools] return super().bind(tools=formatted_tools, **kwargs)","title":"bind_tools"},{"location":"api/llms/#promptolution.llms.deepinfra.ChatDeepInfra.completion_with_retry","text":"Use tenacity to retry the completion call. Source code in promptolution\\llms\\deepinfra.py def completion_with_retry(self, run_manager: Optional[CallbackManagerForLLMRun] = None, **kwargs: Any) -> Any: \"\"\"Use tenacity to retry the completion call.\"\"\" retry_decorator = _create_retry_decorator(self, run_manager=run_manager) @retry_decorator def _completion_with_retry(**kwargs: Any) -> Any: try: request_timeout = kwargs.pop(\"request_timeout\") request = Requests(headers=self._headers()) response = request.post(url=self._url(), data=self._body(kwargs), timeout=request_timeout) self._handle_status(response.status_code, response.text) return response except Exception as e: # import pdb; pdb.set_trace() print(\"EX\", e) # noqa: T201 raise return _completion_with_retry(**kwargs)","title":"completion_with_retry"},{"location":"api/llms/#promptolution.llms.deepinfra.ChatDeepInfra.init_defaults","text":"Validate api key, python package exists, temperature, top_p, and top_k. Source code in promptolution\\llms\\deepinfra.py @root_validator(pre=True) def init_defaults(cls, values: Dict) -> Dict: \"\"\"Validate api key, python package exists, temperature, top_p, and top_k.\"\"\" # For compatibility with LiteLLM api_key = get_from_dict_or_env( values, \"deepinfra_api_key\", \"DEEPINFRA_API_KEY\", default=\"\", ) values[\"deepinfra_api_token\"] = get_from_dict_or_env( values, \"deepinfra_api_token\", \"DEEPINFRA_API_TOKEN\", default=api_key, ) # set model id # values[\"model_name\"] = get_from_dict_or_env( # values, # \"model_name\", # \"DEEPINFRA_MODEL_NAME\", # default=\"\", # ) return values","title":"init_defaults"},{"location":"api/llms/#local-llm","text":"","title":"Local LLM"},{"location":"api/llms/#promptolution.llms.local_llm.LocalLLM","text":"A class for running language models locally using the Hugging Face Transformers library. This class sets up a text generation pipeline with specified model parameters and provides a method to generate responses for given prompts. Attributes: Name Type Description pipeline Pipeline The text generation pipeline. Methods: Name Description get_response Generate responses for a list of prompts. Source code in promptolution\\llms\\local_llm.py class LocalLLM: \"\"\" A class for running language models locally using the Hugging Face Transformers library. This class sets up a text generation pipeline with specified model parameters and provides a method to generate responses for given prompts. Attributes: pipeline (transformers.Pipeline): The text generation pipeline. Methods: get_response: Generate responses for a list of prompts. \"\"\" def __init__(self, model_id: str, batch_size=8): \"\"\" Initialize the LocalLLM with a specific model. Args: model_id (str): The identifier of the model to use (e.g., \"gpt2\", \"facebook/opt-1.3b\"). batch_size (int, optional): The batch size for text generation. Defaults to 8. Note: This method sets up a text generation pipeline with bfloat16 precision, automatic device mapping, and specific generation parameters. \"\"\" self.pipeline = transformers.pipeline( \"text-generation\", model=model_id, model_kwargs={\"torch_dtype\": torch.bfloat16}, device_map=\"auto\", max_new_tokens=256, batch_size=batch_size, num_return_sequences=1, return_full_text=False, ) self.pipeline.tokenizer.pad_token_id = self.pipeline.tokenizer.eos_token_id self.pipeline.tokenizer.padding_side = \"left\" def get_response(self, prompts: list[str]): \"\"\" Generate responses for a list of prompts using the local language model. Args: prompts (list[str]): A list of input prompts. Returns: list[str]: A list of generated responses corresponding to the input prompts. Note: This method uses torch.no_grad() for inference to reduce memory usage. It handles both single and batch inputs, ensuring consistent output format. \"\"\" with torch.no_grad(): response = self.pipeline(prompts, pad_token_id=self.pipeline.tokenizer.eos_token_id) if len(response) != 1: response = [r[0] if isinstance(r, list) else r for r in response] response = [r[\"generated_text\"] for r in response] return response def __del__(self): try: del self.pipeline torch.cuda.empty_cache() except Exception as e: logger.warning(f\"Error during LocalLLM cleanup: {e}\")","title":"LocalLLM"},{"location":"api/llms/#promptolution.llms.local_llm.LocalLLM.__init__","text":"Initialize the LocalLLM with a specific model. Parameters: Name Type Description Default model_id str The identifier of the model to use (e.g., \"gpt2\", \"facebook/opt-1.3b\"). required batch_size int The batch size for text generation. Defaults to 8. 8 Note This method sets up a text generation pipeline with bfloat16 precision, automatic device mapping, and specific generation parameters. Source code in promptolution\\llms\\local_llm.py def __init__(self, model_id: str, batch_size=8): \"\"\" Initialize the LocalLLM with a specific model. Args: model_id (str): The identifier of the model to use (e.g., \"gpt2\", \"facebook/opt-1.3b\"). batch_size (int, optional): The batch size for text generation. Defaults to 8. Note: This method sets up a text generation pipeline with bfloat16 precision, automatic device mapping, and specific generation parameters. \"\"\" self.pipeline = transformers.pipeline( \"text-generation\", model=model_id, model_kwargs={\"torch_dtype\": torch.bfloat16}, device_map=\"auto\", max_new_tokens=256, batch_size=batch_size, num_return_sequences=1, return_full_text=False, ) self.pipeline.tokenizer.pad_token_id = self.pipeline.tokenizer.eos_token_id self.pipeline.tokenizer.padding_side = \"left\"","title":"__init__"},{"location":"api/llms/#promptolution.llms.local_llm.LocalLLM.get_response","text":"Generate responses for a list of prompts using the local language model. Parameters: Name Type Description Default prompts list [ str ] A list of input prompts. required Returns: Type Description list[str]: A list of generated responses corresponding to the input prompts. Note This method uses torch.no_grad() for inference to reduce memory usage. It handles both single and batch inputs, ensuring consistent output format. Source code in promptolution\\llms\\local_llm.py def get_response(self, prompts: list[str]): \"\"\" Generate responses for a list of prompts using the local language model. Args: prompts (list[str]): A list of input prompts. Returns: list[str]: A list of generated responses corresponding to the input prompts. Note: This method uses torch.no_grad() for inference to reduce memory usage. It handles both single and batch inputs, ensuring consistent output format. \"\"\" with torch.no_grad(): response = self.pipeline(prompts, pad_token_id=self.pipeline.tokenizer.eos_token_id) if len(response) != 1: response = [r[0] if isinstance(r, list) else r for r in response] response = [r[\"generated_text\"] for r in response] return response","title":"get_response"},{"location":"api/optimizers/","text":"Optimizers This module contains various optimization algorithms for prompt tuning. get_optimizer(config, *args, **kwargs) Factory function to create and return an optimizer instance based on the provided configuration. This function selects and instantiates the appropriate optimizer class based on the 'optimizer' field in the config object. It supports three types of optimizers: 'dummy', 'evopromptde', and 'evopromptga'. Parameters: Name Type Description Default config A configuration object that must have an 'optimizer' attribute. For 'evopromptde', it should also have a 'donor_random' attribute. For 'evopromptga', it should also have a 'selection_mode' attribute. required *args Variable length argument list passed to the optimizer constructor. () **kwargs Arbitrary keyword arguments passed to the optimizer constructor. {} Returns: Type Description An instance of the specified optimizer class. Raises: Type Description ValueError If an unknown optimizer type is specified in the config. Source code in promptolution\\optimizers\\__init__.py def get_optimizer(config, *args, **kwargs): \"\"\" Factory function to create and return an optimizer instance based on the provided configuration. This function selects and instantiates the appropriate optimizer class based on the 'optimizer' field in the config object. It supports three types of optimizers: 'dummy', 'evopromptde', and 'evopromptga'. Args: config: A configuration object that must have an 'optimizer' attribute. For 'evopromptde', it should also have a 'donor_random' attribute. For 'evopromptga', it should also have a 'selection_mode' attribute. *args: Variable length argument list passed to the optimizer constructor. **kwargs: Arbitrary keyword arguments passed to the optimizer constructor. Returns: An instance of the specified optimizer class. Raises: ValueError: If an unknown optimizer type is specified in the config. \"\"\" if config.optimizer == \"dummy\": return DummyOptimizer(*args, **kwargs) if config.optimizer == \"evopromptde\": return EvoPromptDE(donor_random=config.donor_random, *args, **kwargs) if config.optimizer == \"evopromptga\": return EvoPromptGA(selection_mode=config.selection_mode, *args, **kwargs) raise ValueError(f\"Unknown optimizer: {config.optimizer}\") base_optimizer BaseOptimizer Bases: ABC Abstract base class for prompt optimizers. This class defines the basic structure and interface for prompt optimization algorithms. Concrete optimizer implementations should inherit from this class and implement the optimize method. Attributes: Name Type Description prompts List [ str ] List of current prompts being optimized. task BaseTask The task object used for evaluating prompts. callbacks List [ Callable ] List of callback functions to be called during optimization. predictor The predictor used for prompt evaluation (if applicable). Parameters: Name Type Description Default initial_prompts List [ str ] Initial set of prompts to start optimization with. required task BaseTask Task object for prompt evaluation. required callbacks List [ Callable ] List of callback functions. Defaults to an empty list. [] predictor optional Predictor for prompt evaluation. Defaults to None. None Source code in promptolution\\optimizers\\base_optimizer.py class BaseOptimizer(ABC): \"\"\" Abstract base class for prompt optimizers. This class defines the basic structure and interface for prompt optimization algorithms. Concrete optimizer implementations should inherit from this class and implement the `optimize` method. Attributes: prompts (List[str]): List of current prompts being optimized. task (BaseTask): The task object used for evaluating prompts. callbacks (List[Callable]): List of callback functions to be called during optimization. predictor: The predictor used for prompt evaluation (if applicable). Args: initial_prompts (List[str]): Initial set of prompts to start optimization with. task (BaseTask): Task object for prompt evaluation. callbacks (List[Callable], optional): List of callback functions. Defaults to an empty list. predictor (optional): Predictor for prompt evaluation. Defaults to None. \"\"\" def __init__(self, initial_prompts: list[str], task: BaseTask, callbacks: list[Callable] = [], predictor=None): self.prompts = initial_prompts self.task = task self.callbacks = callbacks self.predictor = predictor @abstractmethod def optimize(self, n_steps: int) -> List[str]: \"\"\" Abstract method to perform the optimization process. This method should be implemented by concrete optimizer classes to define the specific optimization algorithm. Args: n_steps (int): Number of optimization steps to perform. Returns: List[str]: The optimized list of prompts after all steps. Raises: NotImplementedError: If not implemented by a concrete class. \"\"\" raise NotImplementedError def _on_step_end(self): \"\"\" Call all registered callbacks at the end of each optimization step. \"\"\" for callback in self.callbacks: callback.on_step_end(self) def _on_epoch_end(self): \"\"\" Call all registered callbacks at the end of each optimization epoch. \"\"\" for callback in self.callbacks: callback.on_epoch_end(self) def _on_train_end(self): \"\"\" Call all registered callbacks at the end of the entire optimization process. \"\"\" for callback in self.callbacks: callback.on_train_end(self) optimize(n_steps) abstractmethod Abstract method to perform the optimization process. This method should be implemented by concrete optimizer classes to define the specific optimization algorithm. Parameters: Name Type Description Default n_steps int Number of optimization steps to perform. required Returns: Type Description List [ str ] List[str]: The optimized list of prompts after all steps. Raises: Type Description NotImplementedError If not implemented by a concrete class. Source code in promptolution\\optimizers\\base_optimizer.py @abstractmethod def optimize(self, n_steps: int) -> List[str]: \"\"\" Abstract method to perform the optimization process. This method should be implemented by concrete optimizer classes to define the specific optimization algorithm. Args: n_steps (int): Number of optimization steps to perform. Returns: List[str]: The optimized list of prompts after all steps. Raises: NotImplementedError: If not implemented by a concrete class. \"\"\" raise NotImplementedError DummyOptimizer Bases: BaseOptimizer A dummy optimizer that doesn't perform any actual optimization. This optimizer simply returns the initial prompts without modification. It's useful for testing or as a baseline comparison. Attributes: Name Type Description prompts List [ str ] List of prompts (unchanged from initialization). callbacks List [ Callable ] Empty list of callbacks. Parameters: Name Type Description Default initial_prompts List [ str ] Initial set of prompts. required *args Variable length argument list (unused). () **kwargs Arbitrary keyword arguments (unused). {} Source code in promptolution\\optimizers\\base_optimizer.py class DummyOptimizer(BaseOptimizer): \"\"\" A dummy optimizer that doesn't perform any actual optimization. This optimizer simply returns the initial prompts without modification. It's useful for testing or as a baseline comparison. Attributes: prompts (List[str]): List of prompts (unchanged from initialization). callbacks (List[Callable]): Empty list of callbacks. Args: initial_prompts (List[str]): Initial set of prompts. *args: Variable length argument list (unused). **kwargs: Arbitrary keyword arguments (unused). \"\"\" def __init__(self, initial_prompts, *args, **kwargs): self.callbacks = [] self.prompts = initial_prompts def optimize(self, n_steps) -> list[str]: \"\"\" Simulate an optimization process without actually modifying the prompts. This method calls the callback methods to simulate a complete optimization cycle, but returns the initial prompts unchanged. Args: n_steps (int): Number of optimization steps (unused in this implementation). Returns: List[str]: The original list of prompts, unchanged. \"\"\" self._on_step_end() self._on_epoch_end() self._on_train_end() return self.prompts optimize(n_steps) Simulate an optimization process without actually modifying the prompts. This method calls the callback methods to simulate a complete optimization cycle, but returns the initial prompts unchanged. Parameters: Name Type Description Default n_steps int Number of optimization steps (unused in this implementation). required Returns: Type Description list [ str ] List[str]: The original list of prompts, unchanged. Source code in promptolution\\optimizers\\base_optimizer.py def optimize(self, n_steps) -> list[str]: \"\"\" Simulate an optimization process without actually modifying the prompts. This method calls the callback methods to simulate a complete optimization cycle, but returns the initial prompts unchanged. Args: n_steps (int): Number of optimization steps (unused in this implementation). Returns: List[str]: The original list of prompts, unchanged. \"\"\" self._on_step_end() self._on_epoch_end() self._on_train_end() return self.prompts evoprompt_de EvoPromptDE Bases: BaseOptimizer EvoPromptDE: Differential Evolution-based Prompt Optimizer This class implements a differential evolution algorithm for optimizing prompts in large language models. It is adapted from the paper \"Connecting Large Language Models with Evolutionary Algorithms Yields Powerful Prompt Optimizers\" by Guo et al., 2023. The optimizer uses a differential evolution strategy to generate new prompts from existing ones, with an option to use the current best prompt as a donor. Attributes: Name Type Description prompt_template str Template for generating meta-prompts during evolution. donor_random bool If False, uses the current best prompt as a donor; if True, uses a random prompt. meta_llm Language model used for generating child prompts from meta-prompts. Parameters: Name Type Description Default prompt_template str Template for meta-prompts. required meta_llm Language model for child prompt generation. required donor_random bool Whether to use a random donor. Defaults to False. False **args Additional arguments passed to the BaseOptimizer. {} Source code in promptolution\\optimizers\\evoprompt_de.py class EvoPromptDE(BaseOptimizer): \"\"\" EvoPromptDE: Differential Evolution-based Prompt Optimizer This class implements a differential evolution algorithm for optimizing prompts in large language models. It is adapted from the paper \"Connecting Large Language Models with Evolutionary Algorithms Yields Powerful Prompt Optimizers\" by Guo et al., 2023. The optimizer uses a differential evolution strategy to generate new prompts from existing ones, with an option to use the current best prompt as a donor. Attributes: prompt_template (str): Template for generating meta-prompts during evolution. donor_random (bool): If False, uses the current best prompt as a donor; if True, uses a random prompt. meta_llm: Language model used for generating child prompts from meta-prompts. Args: prompt_template (str): Template for meta-prompts. meta_llm: Language model for child prompt generation. donor_random (bool, optional): Whether to use a random donor. Defaults to False. **args: Additional arguments passed to the BaseOptimizer. \"\"\" def __init__(self, prompt_template, meta_llm, donor_random=False, **args): self.prompt_template = prompt_template self.donor_random = donor_random self.meta_llm = meta_llm super().__init__(**args) def optimize(self, n_steps: int) -> List[str]: \"\"\" Perform the optimization process for a specified number of steps. This method iteratively improves the prompts using a differential evolution strategy. It evaluates prompts, generates new prompts using the DE algorithm, and replaces prompts if the new ones perform better. Args: n_steps (int): Number of optimization steps to perform. Returns: List[str]: The optimized list of prompts after all steps. \"\"\" self.scores = self.task.evaluate(self.prompts, self.predictor) self.prompts = [prompt for _, prompt in sorted(zip(self.scores, self.prompts), reverse=True)] self.scores = sorted(self.scores, reverse=True) for _ in range(n_steps): cur_best = self.prompts[0] meta_prompts = [] for i in range(len(self.prompts)): # create meta prompts old_prompt = self.prompts[i] candidates = [prompt for prompt in self.prompts if prompt != old_prompt] a, b, c = np.random.choice(candidates, size=3, replace=False) if not self.donor_random: c = cur_best meta_prompt = ( self.prompt_template.replace(\"<prompt0>\", old_prompt) .replace(\"<prompt1>\", a) .replace(\"<prompt2>\", b) .replace(\"<prompt3>\", c) ) meta_prompts.append(meta_prompt) child_prompts = self.meta_llm.get_response(meta_prompts) child_prompts = [prompt.split(\"<prompt>\")[-1].split(\"</prompt>\")[0].strip() for prompt in child_prompts] child_scores = self.task.evaluate(child_prompts, self.predictor) for i in range(len(self.prompts)): if child_scores[i] > self.scores[i]: self.prompts[i] = child_prompts[i] self.scores[i] = child_scores[i] self._on_step_end() self._on_train_end() return self.prompts optimize(n_steps) Perform the optimization process for a specified number of steps. This method iteratively improves the prompts using a differential evolution strategy. It evaluates prompts, generates new prompts using the DE algorithm, and replaces prompts if the new ones perform better. Parameters: Name Type Description Default n_steps int Number of optimization steps to perform. required Returns: Type Description List [ str ] List[str]: The optimized list of prompts after all steps. Source code in promptolution\\optimizers\\evoprompt_de.py def optimize(self, n_steps: int) -> List[str]: \"\"\" Perform the optimization process for a specified number of steps. This method iteratively improves the prompts using a differential evolution strategy. It evaluates prompts, generates new prompts using the DE algorithm, and replaces prompts if the new ones perform better. Args: n_steps (int): Number of optimization steps to perform. Returns: List[str]: The optimized list of prompts after all steps. \"\"\" self.scores = self.task.evaluate(self.prompts, self.predictor) self.prompts = [prompt for _, prompt in sorted(zip(self.scores, self.prompts), reverse=True)] self.scores = sorted(self.scores, reverse=True) for _ in range(n_steps): cur_best = self.prompts[0] meta_prompts = [] for i in range(len(self.prompts)): # create meta prompts old_prompt = self.prompts[i] candidates = [prompt for prompt in self.prompts if prompt != old_prompt] a, b, c = np.random.choice(candidates, size=3, replace=False) if not self.donor_random: c = cur_best meta_prompt = ( self.prompt_template.replace(\"<prompt0>\", old_prompt) .replace(\"<prompt1>\", a) .replace(\"<prompt2>\", b) .replace(\"<prompt3>\", c) ) meta_prompts.append(meta_prompt) child_prompts = self.meta_llm.get_response(meta_prompts) child_prompts = [prompt.split(\"<prompt>\")[-1].split(\"</prompt>\")[0].strip() for prompt in child_prompts] child_scores = self.task.evaluate(child_prompts, self.predictor) for i in range(len(self.prompts)): if child_scores[i] > self.scores[i]: self.prompts[i] = child_prompts[i] self.scores[i] = child_scores[i] self._on_step_end() self._on_train_end() return self.prompts evoprompt_ga EvoPromptGA Bases: BaseOptimizer EvoPromptGA: Genetic Algorithm-based Prompt Optimizer This class implements a genetic algorithm for optimizing prompts in large language models. It is adapted from the paper \"Connecting Large Language Models with Evolutionary Algorithms Yields Powerful Prompt Optimizers\" by Guo et al., 2023. The optimizer uses crossover operations to generate new prompts from existing ones, with different selection methods available for choosing parent prompts. Attributes: Name Type Description prompt_template str Template for generating meta-prompts during crossover. meta_llm Language model used for generating child prompts from meta-prompts. selection_mode str Method for selecting parent prompts ('random', 'wheel', or 'tour'). Parameters: Name Type Description Default prompt_template str Template for meta-prompts. required meta_llm Language model for child prompt generation. required selection_mode str Parent selection method. Defaults to \"wheel\". 'wheel' **args Additional arguments passed to the BaseOptimizer. {} Raises: Type Description AssertionError If an invalid selection mode is provided. Source code in promptolution\\optimizers\\evoprompt_ga.py class EvoPromptGA(BaseOptimizer): \"\"\" EvoPromptGA: Genetic Algorithm-based Prompt Optimizer This class implements a genetic algorithm for optimizing prompts in large language models. It is adapted from the paper \"Connecting Large Language Models with Evolutionary Algorithms Yields Powerful Prompt Optimizers\" by Guo et al., 2023. The optimizer uses crossover operations to generate new prompts from existing ones, with different selection methods available for choosing parent prompts. Attributes: prompt_template (str): Template for generating meta-prompts during crossover. meta_llm: Language model used for generating child prompts from meta-prompts. selection_mode (str): Method for selecting parent prompts ('random', 'wheel', or 'tour'). Args: prompt_template (str): Template for meta-prompts. meta_llm: Language model for child prompt generation. selection_mode (str, optional): Parent selection method. Defaults to \"wheel\". **args: Additional arguments passed to the BaseOptimizer. Raises: AssertionError: If an invalid selection mode is provided. \"\"\" def __init__(self, prompt_template, meta_llm, selection_mode=\"wheel\", **args): self.prompt_template = prompt_template self.meta_llm = meta_llm assert selection_mode in [\"random\", \"wheel\", \"tour\"], \"Invalid selection mode.\" self.selection_mode = selection_mode super().__init__(**args) def optimize(self, n_steps: int) -> List[str]: \"\"\" Perform the optimization process for a specified number of steps. This method iteratively improves the prompts using genetic algorithm techniques. It evaluates prompts, performs crossover to generate new prompts, and selects the best prompts for the next generation. Args: n_steps (int): Number of optimization steps to perform. Returns: List[str]: The optimized list of prompts after all steps. \"\"\" # get scores from task self.scores = self.task.evaluate(self.prompts, self.predictor).tolist() # sort prompts by score self.prompts = [prompt for _, prompt in sorted(zip(self.scores, self.prompts), reverse=True)] self.scores = sorted(self.scores, reverse=True) for _ in range(n_steps): new_prompts = self._crossover(self.prompts, self.scores) prompts = self.prompts + new_prompts scores = self.scores + self.task.evaluate(new_prompts, self.predictor).tolist() # sort scores and prompts self.prompts = [prompt for _, prompt in sorted(zip(scores, prompts), reverse=True)][: len(self.prompts)] self.scores = sorted(scores, reverse=True)[: len(self.prompts)] self._on_step_end() return self.prompts def _crossover(self, prompts, scores) -> str: \"\"\" Perform crossover operation to generate new child prompts. This method selects parent prompts based on the chosen selection mode, creates meta-prompts using the prompt template, and generates new child prompts using the meta language model. Args: prompts (List[str]): List of current prompts. scores (List[float]): Corresponding scores for the prompts. Returns: List[str]: Newly generated child prompts. \"\"\" # parent selection if self.selection_mode == \"wheel\": wheel_idx = np.random.choice( np.arange(0, len(prompts)), size=len(prompts), replace=True, p=np.array(scores) / np.sum(scores) if np.sum(scores) > 0 else np.ones(len(scores)) / len(scores), ).tolist() parent_pop = [self.prompts[idx] for idx in wheel_idx] elif self.selection_mode in [\"random\", \"tour\"]: parent_pop = self.prompts # crossover meta_prompts = [] for _ in self.prompts: if self.selection_mode in [\"random\", \"wheel\"]: parent_1, parent_2 = np.random.choice(parent_pop, size=2, replace=False) elif self.selection_mode == \"tour\": group_1 = np.random.choice(parent_pop, size=2, replace=False) group_2 = np.random.choice(parent_pop, size=2, replace=False) # use the best of each group based on scores parent_1 = group_1[np.argmax([self.scores[self.prompts.index(p)] for p in group_1])] parent_2 = group_2[np.argmax([self.scores[self.prompts.index(p)] for p in group_2])] meta_prompt = self.prompt_template.replace(\"<prompt1>\", parent_1).replace(\"<prompt2>\", parent_2) meta_prompts.append(meta_prompt) child_prompts = self.meta_llm.get_response(meta_prompts) child_prompts = [prompt.split(\"<prompt>\")[-1].split(\"</prompt>\")[0].strip() for prompt in child_prompts] return child_prompts optimize(n_steps) Perform the optimization process for a specified number of steps. This method iteratively improves the prompts using genetic algorithm techniques. It evaluates prompts, performs crossover to generate new prompts, and selects the best prompts for the next generation. Parameters: Name Type Description Default n_steps int Number of optimization steps to perform. required Returns: Type Description List [ str ] List[str]: The optimized list of prompts after all steps. Source code in promptolution\\optimizers\\evoprompt_ga.py def optimize(self, n_steps: int) -> List[str]: \"\"\" Perform the optimization process for a specified number of steps. This method iteratively improves the prompts using genetic algorithm techniques. It evaluates prompts, performs crossover to generate new prompts, and selects the best prompts for the next generation. Args: n_steps (int): Number of optimization steps to perform. Returns: List[str]: The optimized list of prompts after all steps. \"\"\" # get scores from task self.scores = self.task.evaluate(self.prompts, self.predictor).tolist() # sort prompts by score self.prompts = [prompt for _, prompt in sorted(zip(self.scores, self.prompts), reverse=True)] self.scores = sorted(self.scores, reverse=True) for _ in range(n_steps): new_prompts = self._crossover(self.prompts, self.scores) prompts = self.prompts + new_prompts scores = self.scores + self.task.evaluate(new_prompts, self.predictor).tolist() # sort scores and prompts self.prompts = [prompt for _, prompt in sorted(zip(scores, prompts), reverse=True)][: len(self.prompts)] self.scores = sorted(scores, reverse=True)[: len(self.prompts)] self._on_step_end() return self.prompts Base Optimizer BaseOptimizer Bases: ABC Abstract base class for prompt optimizers. This class defines the basic structure and interface for prompt optimization algorithms. Concrete optimizer implementations should inherit from this class and implement the optimize method. Attributes: Name Type Description prompts List [ str ] List of current prompts being optimized. task BaseTask The task object used for evaluating prompts. callbacks List [ Callable ] List of callback functions to be called during optimization. predictor The predictor used for prompt evaluation (if applicable). Parameters: Name Type Description Default initial_prompts List [ str ] Initial set of prompts to start optimization with. required task BaseTask Task object for prompt evaluation. required callbacks List [ Callable ] List of callback functions. Defaults to an empty list. [] predictor optional Predictor for prompt evaluation. Defaults to None. None Source code in promptolution\\optimizers\\base_optimizer.py class BaseOptimizer(ABC): \"\"\" Abstract base class for prompt optimizers. This class defines the basic structure and interface for prompt optimization algorithms. Concrete optimizer implementations should inherit from this class and implement the `optimize` method. Attributes: prompts (List[str]): List of current prompts being optimized. task (BaseTask): The task object used for evaluating prompts. callbacks (List[Callable]): List of callback functions to be called during optimization. predictor: The predictor used for prompt evaluation (if applicable). Args: initial_prompts (List[str]): Initial set of prompts to start optimization with. task (BaseTask): Task object for prompt evaluation. callbacks (List[Callable], optional): List of callback functions. Defaults to an empty list. predictor (optional): Predictor for prompt evaluation. Defaults to None. \"\"\" def __init__(self, initial_prompts: list[str], task: BaseTask, callbacks: list[Callable] = [], predictor=None): self.prompts = initial_prompts self.task = task self.callbacks = callbacks self.predictor = predictor @abstractmethod def optimize(self, n_steps: int) -> List[str]: \"\"\" Abstract method to perform the optimization process. This method should be implemented by concrete optimizer classes to define the specific optimization algorithm. Args: n_steps (int): Number of optimization steps to perform. Returns: List[str]: The optimized list of prompts after all steps. Raises: NotImplementedError: If not implemented by a concrete class. \"\"\" raise NotImplementedError def _on_step_end(self): \"\"\" Call all registered callbacks at the end of each optimization step. \"\"\" for callback in self.callbacks: callback.on_step_end(self) def _on_epoch_end(self): \"\"\" Call all registered callbacks at the end of each optimization epoch. \"\"\" for callback in self.callbacks: callback.on_epoch_end(self) def _on_train_end(self): \"\"\" Call all registered callbacks at the end of the entire optimization process. \"\"\" for callback in self.callbacks: callback.on_train_end(self) optimize(n_steps) abstractmethod Abstract method to perform the optimization process. This method should be implemented by concrete optimizer classes to define the specific optimization algorithm. Parameters: Name Type Description Default n_steps int Number of optimization steps to perform. required Returns: Type Description List [ str ] List[str]: The optimized list of prompts after all steps. Raises: Type Description NotImplementedError If not implemented by a concrete class. Source code in promptolution\\optimizers\\base_optimizer.py @abstractmethod def optimize(self, n_steps: int) -> List[str]: \"\"\" Abstract method to perform the optimization process. This method should be implemented by concrete optimizer classes to define the specific optimization algorithm. Args: n_steps (int): Number of optimization steps to perform. Returns: List[str]: The optimized list of prompts after all steps. Raises: NotImplementedError: If not implemented by a concrete class. \"\"\" raise NotImplementedError DummyOptimizer Bases: BaseOptimizer A dummy optimizer that doesn't perform any actual optimization. This optimizer simply returns the initial prompts without modification. It's useful for testing or as a baseline comparison. Attributes: Name Type Description prompts List [ str ] List of prompts (unchanged from initialization). callbacks List [ Callable ] Empty list of callbacks. Parameters: Name Type Description Default initial_prompts List [ str ] Initial set of prompts. required *args Variable length argument list (unused). () **kwargs Arbitrary keyword arguments (unused). {} Source code in promptolution\\optimizers\\base_optimizer.py class DummyOptimizer(BaseOptimizer): \"\"\" A dummy optimizer that doesn't perform any actual optimization. This optimizer simply returns the initial prompts without modification. It's useful for testing or as a baseline comparison. Attributes: prompts (List[str]): List of prompts (unchanged from initialization). callbacks (List[Callable]): Empty list of callbacks. Args: initial_prompts (List[str]): Initial set of prompts. *args: Variable length argument list (unused). **kwargs: Arbitrary keyword arguments (unused). \"\"\" def __init__(self, initial_prompts, *args, **kwargs): self.callbacks = [] self.prompts = initial_prompts def optimize(self, n_steps) -> list[str]: \"\"\" Simulate an optimization process without actually modifying the prompts. This method calls the callback methods to simulate a complete optimization cycle, but returns the initial prompts unchanged. Args: n_steps (int): Number of optimization steps (unused in this implementation). Returns: List[str]: The original list of prompts, unchanged. \"\"\" self._on_step_end() self._on_epoch_end() self._on_train_end() return self.prompts optimize(n_steps) Simulate an optimization process without actually modifying the prompts. This method calls the callback methods to simulate a complete optimization cycle, but returns the initial prompts unchanged. Parameters: Name Type Description Default n_steps int Number of optimization steps (unused in this implementation). required Returns: Type Description list [ str ] List[str]: The original list of prompts, unchanged. Source code in promptolution\\optimizers\\base_optimizer.py def optimize(self, n_steps) -> list[str]: \"\"\" Simulate an optimization process without actually modifying the prompts. This method calls the callback methods to simulate a complete optimization cycle, but returns the initial prompts unchanged. Args: n_steps (int): Number of optimization steps (unused in this implementation). Returns: List[str]: The original list of prompts, unchanged. \"\"\" self._on_step_end() self._on_epoch_end() self._on_train_end() return self.prompts EvoPrompt DE EvoPromptDE Bases: BaseOptimizer EvoPromptDE: Differential Evolution-based Prompt Optimizer This class implements a differential evolution algorithm for optimizing prompts in large language models. It is adapted from the paper \"Connecting Large Language Models with Evolutionary Algorithms Yields Powerful Prompt Optimizers\" by Guo et al., 2023. The optimizer uses a differential evolution strategy to generate new prompts from existing ones, with an option to use the current best prompt as a donor. Attributes: Name Type Description prompt_template str Template for generating meta-prompts during evolution. donor_random bool If False, uses the current best prompt as a donor; if True, uses a random prompt. meta_llm Language model used for generating child prompts from meta-prompts. Parameters: Name Type Description Default prompt_template str Template for meta-prompts. required meta_llm Language model for child prompt generation. required donor_random bool Whether to use a random donor. Defaults to False. False **args Additional arguments passed to the BaseOptimizer. {} Source code in promptolution\\optimizers\\evoprompt_de.py class EvoPromptDE(BaseOptimizer): \"\"\" EvoPromptDE: Differential Evolution-based Prompt Optimizer This class implements a differential evolution algorithm for optimizing prompts in large language models. It is adapted from the paper \"Connecting Large Language Models with Evolutionary Algorithms Yields Powerful Prompt Optimizers\" by Guo et al., 2023. The optimizer uses a differential evolution strategy to generate new prompts from existing ones, with an option to use the current best prompt as a donor. Attributes: prompt_template (str): Template for generating meta-prompts during evolution. donor_random (bool): If False, uses the current best prompt as a donor; if True, uses a random prompt. meta_llm: Language model used for generating child prompts from meta-prompts. Args: prompt_template (str): Template for meta-prompts. meta_llm: Language model for child prompt generation. donor_random (bool, optional): Whether to use a random donor. Defaults to False. **args: Additional arguments passed to the BaseOptimizer. \"\"\" def __init__(self, prompt_template, meta_llm, donor_random=False, **args): self.prompt_template = prompt_template self.donor_random = donor_random self.meta_llm = meta_llm super().__init__(**args) def optimize(self, n_steps: int) -> List[str]: \"\"\" Perform the optimization process for a specified number of steps. This method iteratively improves the prompts using a differential evolution strategy. It evaluates prompts, generates new prompts using the DE algorithm, and replaces prompts if the new ones perform better. Args: n_steps (int): Number of optimization steps to perform. Returns: List[str]: The optimized list of prompts after all steps. \"\"\" self.scores = self.task.evaluate(self.prompts, self.predictor) self.prompts = [prompt for _, prompt in sorted(zip(self.scores, self.prompts), reverse=True)] self.scores = sorted(self.scores, reverse=True) for _ in range(n_steps): cur_best = self.prompts[0] meta_prompts = [] for i in range(len(self.prompts)): # create meta prompts old_prompt = self.prompts[i] candidates = [prompt for prompt in self.prompts if prompt != old_prompt] a, b, c = np.random.choice(candidates, size=3, replace=False) if not self.donor_random: c = cur_best meta_prompt = ( self.prompt_template.replace(\"<prompt0>\", old_prompt) .replace(\"<prompt1>\", a) .replace(\"<prompt2>\", b) .replace(\"<prompt3>\", c) ) meta_prompts.append(meta_prompt) child_prompts = self.meta_llm.get_response(meta_prompts) child_prompts = [prompt.split(\"<prompt>\")[-1].split(\"</prompt>\")[0].strip() for prompt in child_prompts] child_scores = self.task.evaluate(child_prompts, self.predictor) for i in range(len(self.prompts)): if child_scores[i] > self.scores[i]: self.prompts[i] = child_prompts[i] self.scores[i] = child_scores[i] self._on_step_end() self._on_train_end() return self.prompts optimize(n_steps) Perform the optimization process for a specified number of steps. This method iteratively improves the prompts using a differential evolution strategy. It evaluates prompts, generates new prompts using the DE algorithm, and replaces prompts if the new ones perform better. Parameters: Name Type Description Default n_steps int Number of optimization steps to perform. required Returns: Type Description List [ str ] List[str]: The optimized list of prompts after all steps. Source code in promptolution\\optimizers\\evoprompt_de.py def optimize(self, n_steps: int) -> List[str]: \"\"\" Perform the optimization process for a specified number of steps. This method iteratively improves the prompts using a differential evolution strategy. It evaluates prompts, generates new prompts using the DE algorithm, and replaces prompts if the new ones perform better. Args: n_steps (int): Number of optimization steps to perform. Returns: List[str]: The optimized list of prompts after all steps. \"\"\" self.scores = self.task.evaluate(self.prompts, self.predictor) self.prompts = [prompt for _, prompt in sorted(zip(self.scores, self.prompts), reverse=True)] self.scores = sorted(self.scores, reverse=True) for _ in range(n_steps): cur_best = self.prompts[0] meta_prompts = [] for i in range(len(self.prompts)): # create meta prompts old_prompt = self.prompts[i] candidates = [prompt for prompt in self.prompts if prompt != old_prompt] a, b, c = np.random.choice(candidates, size=3, replace=False) if not self.donor_random: c = cur_best meta_prompt = ( self.prompt_template.replace(\"<prompt0>\", old_prompt) .replace(\"<prompt1>\", a) .replace(\"<prompt2>\", b) .replace(\"<prompt3>\", c) ) meta_prompts.append(meta_prompt) child_prompts = self.meta_llm.get_response(meta_prompts) child_prompts = [prompt.split(\"<prompt>\")[-1].split(\"</prompt>\")[0].strip() for prompt in child_prompts] child_scores = self.task.evaluate(child_prompts, self.predictor) for i in range(len(self.prompts)): if child_scores[i] > self.scores[i]: self.prompts[i] = child_prompts[i] self.scores[i] = child_scores[i] self._on_step_end() self._on_train_end() return self.prompts EvoPrompt GA EvoPromptGA Bases: BaseOptimizer EvoPromptGA: Genetic Algorithm-based Prompt Optimizer This class implements a genetic algorithm for optimizing prompts in large language models. It is adapted from the paper \"Connecting Large Language Models with Evolutionary Algorithms Yields Powerful Prompt Optimizers\" by Guo et al., 2023. The optimizer uses crossover operations to generate new prompts from existing ones, with different selection methods available for choosing parent prompts. Attributes: Name Type Description prompt_template str Template for generating meta-prompts during crossover. meta_llm Language model used for generating child prompts from meta-prompts. selection_mode str Method for selecting parent prompts ('random', 'wheel', or 'tour'). Parameters: Name Type Description Default prompt_template str Template for meta-prompts. required meta_llm Language model for child prompt generation. required selection_mode str Parent selection method. Defaults to \"wheel\". 'wheel' **args Additional arguments passed to the BaseOptimizer. {} Raises: Type Description AssertionError If an invalid selection mode is provided. Source code in promptolution\\optimizers\\evoprompt_ga.py class EvoPromptGA(BaseOptimizer): \"\"\" EvoPromptGA: Genetic Algorithm-based Prompt Optimizer This class implements a genetic algorithm for optimizing prompts in large language models. It is adapted from the paper \"Connecting Large Language Models with Evolutionary Algorithms Yields Powerful Prompt Optimizers\" by Guo et al., 2023. The optimizer uses crossover operations to generate new prompts from existing ones, with different selection methods available for choosing parent prompts. Attributes: prompt_template (str): Template for generating meta-prompts during crossover. meta_llm: Language model used for generating child prompts from meta-prompts. selection_mode (str): Method for selecting parent prompts ('random', 'wheel', or 'tour'). Args: prompt_template (str): Template for meta-prompts. meta_llm: Language model for child prompt generation. selection_mode (str, optional): Parent selection method. Defaults to \"wheel\". **args: Additional arguments passed to the BaseOptimizer. Raises: AssertionError: If an invalid selection mode is provided. \"\"\" def __init__(self, prompt_template, meta_llm, selection_mode=\"wheel\", **args): self.prompt_template = prompt_template self.meta_llm = meta_llm assert selection_mode in [\"random\", \"wheel\", \"tour\"], \"Invalid selection mode.\" self.selection_mode = selection_mode super().__init__(**args) def optimize(self, n_steps: int) -> List[str]: \"\"\" Perform the optimization process for a specified number of steps. This method iteratively improves the prompts using genetic algorithm techniques. It evaluates prompts, performs crossover to generate new prompts, and selects the best prompts for the next generation. Args: n_steps (int): Number of optimization steps to perform. Returns: List[str]: The optimized list of prompts after all steps. \"\"\" # get scores from task self.scores = self.task.evaluate(self.prompts, self.predictor).tolist() # sort prompts by score self.prompts = [prompt for _, prompt in sorted(zip(self.scores, self.prompts), reverse=True)] self.scores = sorted(self.scores, reverse=True) for _ in range(n_steps): new_prompts = self._crossover(self.prompts, self.scores) prompts = self.prompts + new_prompts scores = self.scores + self.task.evaluate(new_prompts, self.predictor).tolist() # sort scores and prompts self.prompts = [prompt for _, prompt in sorted(zip(scores, prompts), reverse=True)][: len(self.prompts)] self.scores = sorted(scores, reverse=True)[: len(self.prompts)] self._on_step_end() return self.prompts def _crossover(self, prompts, scores) -> str: \"\"\" Perform crossover operation to generate new child prompts. This method selects parent prompts based on the chosen selection mode, creates meta-prompts using the prompt template, and generates new child prompts using the meta language model. Args: prompts (List[str]): List of current prompts. scores (List[float]): Corresponding scores for the prompts. Returns: List[str]: Newly generated child prompts. \"\"\" # parent selection if self.selection_mode == \"wheel\": wheel_idx = np.random.choice( np.arange(0, len(prompts)), size=len(prompts), replace=True, p=np.array(scores) / np.sum(scores) if np.sum(scores) > 0 else np.ones(len(scores)) / len(scores), ).tolist() parent_pop = [self.prompts[idx] for idx in wheel_idx] elif self.selection_mode in [\"random\", \"tour\"]: parent_pop = self.prompts # crossover meta_prompts = [] for _ in self.prompts: if self.selection_mode in [\"random\", \"wheel\"]: parent_1, parent_2 = np.random.choice(parent_pop, size=2, replace=False) elif self.selection_mode == \"tour\": group_1 = np.random.choice(parent_pop, size=2, replace=False) group_2 = np.random.choice(parent_pop, size=2, replace=False) # use the best of each group based on scores parent_1 = group_1[np.argmax([self.scores[self.prompts.index(p)] for p in group_1])] parent_2 = group_2[np.argmax([self.scores[self.prompts.index(p)] for p in group_2])] meta_prompt = self.prompt_template.replace(\"<prompt1>\", parent_1).replace(\"<prompt2>\", parent_2) meta_prompts.append(meta_prompt) child_prompts = self.meta_llm.get_response(meta_prompts) child_prompts = [prompt.split(\"<prompt>\")[-1].split(\"</prompt>\")[0].strip() for prompt in child_prompts] return child_prompts optimize(n_steps) Perform the optimization process for a specified number of steps. This method iteratively improves the prompts using genetic algorithm techniques. It evaluates prompts, performs crossover to generate new prompts, and selects the best prompts for the next generation. Parameters: Name Type Description Default n_steps int Number of optimization steps to perform. required Returns: Type Description List [ str ] List[str]: The optimized list of prompts after all steps. Source code in promptolution\\optimizers\\evoprompt_ga.py def optimize(self, n_steps: int) -> List[str]: \"\"\" Perform the optimization process for a specified number of steps. This method iteratively improves the prompts using genetic algorithm techniques. It evaluates prompts, performs crossover to generate new prompts, and selects the best prompts for the next generation. Args: n_steps (int): Number of optimization steps to perform. Returns: List[str]: The optimized list of prompts after all steps. \"\"\" # get scores from task self.scores = self.task.evaluate(self.prompts, self.predictor).tolist() # sort prompts by score self.prompts = [prompt for _, prompt in sorted(zip(self.scores, self.prompts), reverse=True)] self.scores = sorted(self.scores, reverse=True) for _ in range(n_steps): new_prompts = self._crossover(self.prompts, self.scores) prompts = self.prompts + new_prompts scores = self.scores + self.task.evaluate(new_prompts, self.predictor).tolist() # sort scores and prompts self.prompts = [prompt for _, prompt in sorted(zip(scores, prompts), reverse=True)][: len(self.prompts)] self.scores = sorted(scores, reverse=True)[: len(self.prompts)] self._on_step_end() return self.prompts","title":"Optimizers"},{"location":"api/optimizers/#optimizers","text":"This module contains various optimization algorithms for prompt tuning.","title":"Optimizers"},{"location":"api/optimizers/#promptolution.optimizers.get_optimizer","text":"Factory function to create and return an optimizer instance based on the provided configuration. This function selects and instantiates the appropriate optimizer class based on the 'optimizer' field in the config object. It supports three types of optimizers: 'dummy', 'evopromptde', and 'evopromptga'. Parameters: Name Type Description Default config A configuration object that must have an 'optimizer' attribute. For 'evopromptde', it should also have a 'donor_random' attribute. For 'evopromptga', it should also have a 'selection_mode' attribute. required *args Variable length argument list passed to the optimizer constructor. () **kwargs Arbitrary keyword arguments passed to the optimizer constructor. {} Returns: Type Description An instance of the specified optimizer class. Raises: Type Description ValueError If an unknown optimizer type is specified in the config. Source code in promptolution\\optimizers\\__init__.py def get_optimizer(config, *args, **kwargs): \"\"\" Factory function to create and return an optimizer instance based on the provided configuration. This function selects and instantiates the appropriate optimizer class based on the 'optimizer' field in the config object. It supports three types of optimizers: 'dummy', 'evopromptde', and 'evopromptga'. Args: config: A configuration object that must have an 'optimizer' attribute. For 'evopromptde', it should also have a 'donor_random' attribute. For 'evopromptga', it should also have a 'selection_mode' attribute. *args: Variable length argument list passed to the optimizer constructor. **kwargs: Arbitrary keyword arguments passed to the optimizer constructor. Returns: An instance of the specified optimizer class. Raises: ValueError: If an unknown optimizer type is specified in the config. \"\"\" if config.optimizer == \"dummy\": return DummyOptimizer(*args, **kwargs) if config.optimizer == \"evopromptde\": return EvoPromptDE(donor_random=config.donor_random, *args, **kwargs) if config.optimizer == \"evopromptga\": return EvoPromptGA(selection_mode=config.selection_mode, *args, **kwargs) raise ValueError(f\"Unknown optimizer: {config.optimizer}\")","title":"get_optimizer"},{"location":"api/optimizers/#promptolution.optimizers.base_optimizer","text":"","title":"base_optimizer"},{"location":"api/optimizers/#promptolution.optimizers.base_optimizer.BaseOptimizer","text":"Bases: ABC Abstract base class for prompt optimizers. This class defines the basic structure and interface for prompt optimization algorithms. Concrete optimizer implementations should inherit from this class and implement the optimize method. Attributes: Name Type Description prompts List [ str ] List of current prompts being optimized. task BaseTask The task object used for evaluating prompts. callbacks List [ Callable ] List of callback functions to be called during optimization. predictor The predictor used for prompt evaluation (if applicable). Parameters: Name Type Description Default initial_prompts List [ str ] Initial set of prompts to start optimization with. required task BaseTask Task object for prompt evaluation. required callbacks List [ Callable ] List of callback functions. Defaults to an empty list. [] predictor optional Predictor for prompt evaluation. Defaults to None. None Source code in promptolution\\optimizers\\base_optimizer.py class BaseOptimizer(ABC): \"\"\" Abstract base class for prompt optimizers. This class defines the basic structure and interface for prompt optimization algorithms. Concrete optimizer implementations should inherit from this class and implement the `optimize` method. Attributes: prompts (List[str]): List of current prompts being optimized. task (BaseTask): The task object used for evaluating prompts. callbacks (List[Callable]): List of callback functions to be called during optimization. predictor: The predictor used for prompt evaluation (if applicable). Args: initial_prompts (List[str]): Initial set of prompts to start optimization with. task (BaseTask): Task object for prompt evaluation. callbacks (List[Callable], optional): List of callback functions. Defaults to an empty list. predictor (optional): Predictor for prompt evaluation. Defaults to None. \"\"\" def __init__(self, initial_prompts: list[str], task: BaseTask, callbacks: list[Callable] = [], predictor=None): self.prompts = initial_prompts self.task = task self.callbacks = callbacks self.predictor = predictor @abstractmethod def optimize(self, n_steps: int) -> List[str]: \"\"\" Abstract method to perform the optimization process. This method should be implemented by concrete optimizer classes to define the specific optimization algorithm. Args: n_steps (int): Number of optimization steps to perform. Returns: List[str]: The optimized list of prompts after all steps. Raises: NotImplementedError: If not implemented by a concrete class. \"\"\" raise NotImplementedError def _on_step_end(self): \"\"\" Call all registered callbacks at the end of each optimization step. \"\"\" for callback in self.callbacks: callback.on_step_end(self) def _on_epoch_end(self): \"\"\" Call all registered callbacks at the end of each optimization epoch. \"\"\" for callback in self.callbacks: callback.on_epoch_end(self) def _on_train_end(self): \"\"\" Call all registered callbacks at the end of the entire optimization process. \"\"\" for callback in self.callbacks: callback.on_train_end(self)","title":"BaseOptimizer"},{"location":"api/optimizers/#promptolution.optimizers.base_optimizer.BaseOptimizer.optimize","text":"Abstract method to perform the optimization process. This method should be implemented by concrete optimizer classes to define the specific optimization algorithm. Parameters: Name Type Description Default n_steps int Number of optimization steps to perform. required Returns: Type Description List [ str ] List[str]: The optimized list of prompts after all steps. Raises: Type Description NotImplementedError If not implemented by a concrete class. Source code in promptolution\\optimizers\\base_optimizer.py @abstractmethod def optimize(self, n_steps: int) -> List[str]: \"\"\" Abstract method to perform the optimization process. This method should be implemented by concrete optimizer classes to define the specific optimization algorithm. Args: n_steps (int): Number of optimization steps to perform. Returns: List[str]: The optimized list of prompts after all steps. Raises: NotImplementedError: If not implemented by a concrete class. \"\"\" raise NotImplementedError","title":"optimize"},{"location":"api/optimizers/#promptolution.optimizers.base_optimizer.DummyOptimizer","text":"Bases: BaseOptimizer A dummy optimizer that doesn't perform any actual optimization. This optimizer simply returns the initial prompts without modification. It's useful for testing or as a baseline comparison. Attributes: Name Type Description prompts List [ str ] List of prompts (unchanged from initialization). callbacks List [ Callable ] Empty list of callbacks. Parameters: Name Type Description Default initial_prompts List [ str ] Initial set of prompts. required *args Variable length argument list (unused). () **kwargs Arbitrary keyword arguments (unused). {} Source code in promptolution\\optimizers\\base_optimizer.py class DummyOptimizer(BaseOptimizer): \"\"\" A dummy optimizer that doesn't perform any actual optimization. This optimizer simply returns the initial prompts without modification. It's useful for testing or as a baseline comparison. Attributes: prompts (List[str]): List of prompts (unchanged from initialization). callbacks (List[Callable]): Empty list of callbacks. Args: initial_prompts (List[str]): Initial set of prompts. *args: Variable length argument list (unused). **kwargs: Arbitrary keyword arguments (unused). \"\"\" def __init__(self, initial_prompts, *args, **kwargs): self.callbacks = [] self.prompts = initial_prompts def optimize(self, n_steps) -> list[str]: \"\"\" Simulate an optimization process without actually modifying the prompts. This method calls the callback methods to simulate a complete optimization cycle, but returns the initial prompts unchanged. Args: n_steps (int): Number of optimization steps (unused in this implementation). Returns: List[str]: The original list of prompts, unchanged. \"\"\" self._on_step_end() self._on_epoch_end() self._on_train_end() return self.prompts","title":"DummyOptimizer"},{"location":"api/optimizers/#promptolution.optimizers.base_optimizer.DummyOptimizer.optimize","text":"Simulate an optimization process without actually modifying the prompts. This method calls the callback methods to simulate a complete optimization cycle, but returns the initial prompts unchanged. Parameters: Name Type Description Default n_steps int Number of optimization steps (unused in this implementation). required Returns: Type Description list [ str ] List[str]: The original list of prompts, unchanged. Source code in promptolution\\optimizers\\base_optimizer.py def optimize(self, n_steps) -> list[str]: \"\"\" Simulate an optimization process without actually modifying the prompts. This method calls the callback methods to simulate a complete optimization cycle, but returns the initial prompts unchanged. Args: n_steps (int): Number of optimization steps (unused in this implementation). Returns: List[str]: The original list of prompts, unchanged. \"\"\" self._on_step_end() self._on_epoch_end() self._on_train_end() return self.prompts","title":"optimize"},{"location":"api/optimizers/#promptolution.optimizers.evoprompt_de","text":"","title":"evoprompt_de"},{"location":"api/optimizers/#promptolution.optimizers.evoprompt_de.EvoPromptDE","text":"Bases: BaseOptimizer EvoPromptDE: Differential Evolution-based Prompt Optimizer This class implements a differential evolution algorithm for optimizing prompts in large language models. It is adapted from the paper \"Connecting Large Language Models with Evolutionary Algorithms Yields Powerful Prompt Optimizers\" by Guo et al., 2023. The optimizer uses a differential evolution strategy to generate new prompts from existing ones, with an option to use the current best prompt as a donor. Attributes: Name Type Description prompt_template str Template for generating meta-prompts during evolution. donor_random bool If False, uses the current best prompt as a donor; if True, uses a random prompt. meta_llm Language model used for generating child prompts from meta-prompts. Parameters: Name Type Description Default prompt_template str Template for meta-prompts. required meta_llm Language model for child prompt generation. required donor_random bool Whether to use a random donor. Defaults to False. False **args Additional arguments passed to the BaseOptimizer. {} Source code in promptolution\\optimizers\\evoprompt_de.py class EvoPromptDE(BaseOptimizer): \"\"\" EvoPromptDE: Differential Evolution-based Prompt Optimizer This class implements a differential evolution algorithm for optimizing prompts in large language models. It is adapted from the paper \"Connecting Large Language Models with Evolutionary Algorithms Yields Powerful Prompt Optimizers\" by Guo et al., 2023. The optimizer uses a differential evolution strategy to generate new prompts from existing ones, with an option to use the current best prompt as a donor. Attributes: prompt_template (str): Template for generating meta-prompts during evolution. donor_random (bool): If False, uses the current best prompt as a donor; if True, uses a random prompt. meta_llm: Language model used for generating child prompts from meta-prompts. Args: prompt_template (str): Template for meta-prompts. meta_llm: Language model for child prompt generation. donor_random (bool, optional): Whether to use a random donor. Defaults to False. **args: Additional arguments passed to the BaseOptimizer. \"\"\" def __init__(self, prompt_template, meta_llm, donor_random=False, **args): self.prompt_template = prompt_template self.donor_random = donor_random self.meta_llm = meta_llm super().__init__(**args) def optimize(self, n_steps: int) -> List[str]: \"\"\" Perform the optimization process for a specified number of steps. This method iteratively improves the prompts using a differential evolution strategy. It evaluates prompts, generates new prompts using the DE algorithm, and replaces prompts if the new ones perform better. Args: n_steps (int): Number of optimization steps to perform. Returns: List[str]: The optimized list of prompts after all steps. \"\"\" self.scores = self.task.evaluate(self.prompts, self.predictor) self.prompts = [prompt for _, prompt in sorted(zip(self.scores, self.prompts), reverse=True)] self.scores = sorted(self.scores, reverse=True) for _ in range(n_steps): cur_best = self.prompts[0] meta_prompts = [] for i in range(len(self.prompts)): # create meta prompts old_prompt = self.prompts[i] candidates = [prompt for prompt in self.prompts if prompt != old_prompt] a, b, c = np.random.choice(candidates, size=3, replace=False) if not self.donor_random: c = cur_best meta_prompt = ( self.prompt_template.replace(\"<prompt0>\", old_prompt) .replace(\"<prompt1>\", a) .replace(\"<prompt2>\", b) .replace(\"<prompt3>\", c) ) meta_prompts.append(meta_prompt) child_prompts = self.meta_llm.get_response(meta_prompts) child_prompts = [prompt.split(\"<prompt>\")[-1].split(\"</prompt>\")[0].strip() for prompt in child_prompts] child_scores = self.task.evaluate(child_prompts, self.predictor) for i in range(len(self.prompts)): if child_scores[i] > self.scores[i]: self.prompts[i] = child_prompts[i] self.scores[i] = child_scores[i] self._on_step_end() self._on_train_end() return self.prompts","title":"EvoPromptDE"},{"location":"api/optimizers/#promptolution.optimizers.evoprompt_de.EvoPromptDE.optimize","text":"Perform the optimization process for a specified number of steps. This method iteratively improves the prompts using a differential evolution strategy. It evaluates prompts, generates new prompts using the DE algorithm, and replaces prompts if the new ones perform better. Parameters: Name Type Description Default n_steps int Number of optimization steps to perform. required Returns: Type Description List [ str ] List[str]: The optimized list of prompts after all steps. Source code in promptolution\\optimizers\\evoprompt_de.py def optimize(self, n_steps: int) -> List[str]: \"\"\" Perform the optimization process for a specified number of steps. This method iteratively improves the prompts using a differential evolution strategy. It evaluates prompts, generates new prompts using the DE algorithm, and replaces prompts if the new ones perform better. Args: n_steps (int): Number of optimization steps to perform. Returns: List[str]: The optimized list of prompts after all steps. \"\"\" self.scores = self.task.evaluate(self.prompts, self.predictor) self.prompts = [prompt for _, prompt in sorted(zip(self.scores, self.prompts), reverse=True)] self.scores = sorted(self.scores, reverse=True) for _ in range(n_steps): cur_best = self.prompts[0] meta_prompts = [] for i in range(len(self.prompts)): # create meta prompts old_prompt = self.prompts[i] candidates = [prompt for prompt in self.prompts if prompt != old_prompt] a, b, c = np.random.choice(candidates, size=3, replace=False) if not self.donor_random: c = cur_best meta_prompt = ( self.prompt_template.replace(\"<prompt0>\", old_prompt) .replace(\"<prompt1>\", a) .replace(\"<prompt2>\", b) .replace(\"<prompt3>\", c) ) meta_prompts.append(meta_prompt) child_prompts = self.meta_llm.get_response(meta_prompts) child_prompts = [prompt.split(\"<prompt>\")[-1].split(\"</prompt>\")[0].strip() for prompt in child_prompts] child_scores = self.task.evaluate(child_prompts, self.predictor) for i in range(len(self.prompts)): if child_scores[i] > self.scores[i]: self.prompts[i] = child_prompts[i] self.scores[i] = child_scores[i] self._on_step_end() self._on_train_end() return self.prompts","title":"optimize"},{"location":"api/optimizers/#promptolution.optimizers.evoprompt_ga","text":"","title":"evoprompt_ga"},{"location":"api/optimizers/#promptolution.optimizers.evoprompt_ga.EvoPromptGA","text":"Bases: BaseOptimizer EvoPromptGA: Genetic Algorithm-based Prompt Optimizer This class implements a genetic algorithm for optimizing prompts in large language models. It is adapted from the paper \"Connecting Large Language Models with Evolutionary Algorithms Yields Powerful Prompt Optimizers\" by Guo et al., 2023. The optimizer uses crossover operations to generate new prompts from existing ones, with different selection methods available for choosing parent prompts. Attributes: Name Type Description prompt_template str Template for generating meta-prompts during crossover. meta_llm Language model used for generating child prompts from meta-prompts. selection_mode str Method for selecting parent prompts ('random', 'wheel', or 'tour'). Parameters: Name Type Description Default prompt_template str Template for meta-prompts. required meta_llm Language model for child prompt generation. required selection_mode str Parent selection method. Defaults to \"wheel\". 'wheel' **args Additional arguments passed to the BaseOptimizer. {} Raises: Type Description AssertionError If an invalid selection mode is provided. Source code in promptolution\\optimizers\\evoprompt_ga.py class EvoPromptGA(BaseOptimizer): \"\"\" EvoPromptGA: Genetic Algorithm-based Prompt Optimizer This class implements a genetic algorithm for optimizing prompts in large language models. It is adapted from the paper \"Connecting Large Language Models with Evolutionary Algorithms Yields Powerful Prompt Optimizers\" by Guo et al., 2023. The optimizer uses crossover operations to generate new prompts from existing ones, with different selection methods available for choosing parent prompts. Attributes: prompt_template (str): Template for generating meta-prompts during crossover. meta_llm: Language model used for generating child prompts from meta-prompts. selection_mode (str): Method for selecting parent prompts ('random', 'wheel', or 'tour'). Args: prompt_template (str): Template for meta-prompts. meta_llm: Language model for child prompt generation. selection_mode (str, optional): Parent selection method. Defaults to \"wheel\". **args: Additional arguments passed to the BaseOptimizer. Raises: AssertionError: If an invalid selection mode is provided. \"\"\" def __init__(self, prompt_template, meta_llm, selection_mode=\"wheel\", **args): self.prompt_template = prompt_template self.meta_llm = meta_llm assert selection_mode in [\"random\", \"wheel\", \"tour\"], \"Invalid selection mode.\" self.selection_mode = selection_mode super().__init__(**args) def optimize(self, n_steps: int) -> List[str]: \"\"\" Perform the optimization process for a specified number of steps. This method iteratively improves the prompts using genetic algorithm techniques. It evaluates prompts, performs crossover to generate new prompts, and selects the best prompts for the next generation. Args: n_steps (int): Number of optimization steps to perform. Returns: List[str]: The optimized list of prompts after all steps. \"\"\" # get scores from task self.scores = self.task.evaluate(self.prompts, self.predictor).tolist() # sort prompts by score self.prompts = [prompt for _, prompt in sorted(zip(self.scores, self.prompts), reverse=True)] self.scores = sorted(self.scores, reverse=True) for _ in range(n_steps): new_prompts = self._crossover(self.prompts, self.scores) prompts = self.prompts + new_prompts scores = self.scores + self.task.evaluate(new_prompts, self.predictor).tolist() # sort scores and prompts self.prompts = [prompt for _, prompt in sorted(zip(scores, prompts), reverse=True)][: len(self.prompts)] self.scores = sorted(scores, reverse=True)[: len(self.prompts)] self._on_step_end() return self.prompts def _crossover(self, prompts, scores) -> str: \"\"\" Perform crossover operation to generate new child prompts. This method selects parent prompts based on the chosen selection mode, creates meta-prompts using the prompt template, and generates new child prompts using the meta language model. Args: prompts (List[str]): List of current prompts. scores (List[float]): Corresponding scores for the prompts. Returns: List[str]: Newly generated child prompts. \"\"\" # parent selection if self.selection_mode == \"wheel\": wheel_idx = np.random.choice( np.arange(0, len(prompts)), size=len(prompts), replace=True, p=np.array(scores) / np.sum(scores) if np.sum(scores) > 0 else np.ones(len(scores)) / len(scores), ).tolist() parent_pop = [self.prompts[idx] for idx in wheel_idx] elif self.selection_mode in [\"random\", \"tour\"]: parent_pop = self.prompts # crossover meta_prompts = [] for _ in self.prompts: if self.selection_mode in [\"random\", \"wheel\"]: parent_1, parent_2 = np.random.choice(parent_pop, size=2, replace=False) elif self.selection_mode == \"tour\": group_1 = np.random.choice(parent_pop, size=2, replace=False) group_2 = np.random.choice(parent_pop, size=2, replace=False) # use the best of each group based on scores parent_1 = group_1[np.argmax([self.scores[self.prompts.index(p)] for p in group_1])] parent_2 = group_2[np.argmax([self.scores[self.prompts.index(p)] for p in group_2])] meta_prompt = self.prompt_template.replace(\"<prompt1>\", parent_1).replace(\"<prompt2>\", parent_2) meta_prompts.append(meta_prompt) child_prompts = self.meta_llm.get_response(meta_prompts) child_prompts = [prompt.split(\"<prompt>\")[-1].split(\"</prompt>\")[0].strip() for prompt in child_prompts] return child_prompts","title":"EvoPromptGA"},{"location":"api/optimizers/#promptolution.optimizers.evoprompt_ga.EvoPromptGA.optimize","text":"Perform the optimization process for a specified number of steps. This method iteratively improves the prompts using genetic algorithm techniques. It evaluates prompts, performs crossover to generate new prompts, and selects the best prompts for the next generation. Parameters: Name Type Description Default n_steps int Number of optimization steps to perform. required Returns: Type Description List [ str ] List[str]: The optimized list of prompts after all steps. Source code in promptolution\\optimizers\\evoprompt_ga.py def optimize(self, n_steps: int) -> List[str]: \"\"\" Perform the optimization process for a specified number of steps. This method iteratively improves the prompts using genetic algorithm techniques. It evaluates prompts, performs crossover to generate new prompts, and selects the best prompts for the next generation. Args: n_steps (int): Number of optimization steps to perform. Returns: List[str]: The optimized list of prompts after all steps. \"\"\" # get scores from task self.scores = self.task.evaluate(self.prompts, self.predictor).tolist() # sort prompts by score self.prompts = [prompt for _, prompt in sorted(zip(self.scores, self.prompts), reverse=True)] self.scores = sorted(self.scores, reverse=True) for _ in range(n_steps): new_prompts = self._crossover(self.prompts, self.scores) prompts = self.prompts + new_prompts scores = self.scores + self.task.evaluate(new_prompts, self.predictor).tolist() # sort scores and prompts self.prompts = [prompt for _, prompt in sorted(zip(scores, prompts), reverse=True)][: len(self.prompts)] self.scores = sorted(scores, reverse=True)[: len(self.prompts)] self._on_step_end() return self.prompts","title":"optimize"},{"location":"api/optimizers/#base-optimizer","text":"","title":"Base Optimizer"},{"location":"api/optimizers/#promptolution.optimizers.base_optimizer.BaseOptimizer","text":"Bases: ABC Abstract base class for prompt optimizers. This class defines the basic structure and interface for prompt optimization algorithms. Concrete optimizer implementations should inherit from this class and implement the optimize method. Attributes: Name Type Description prompts List [ str ] List of current prompts being optimized. task BaseTask The task object used for evaluating prompts. callbacks List [ Callable ] List of callback functions to be called during optimization. predictor The predictor used for prompt evaluation (if applicable). Parameters: Name Type Description Default initial_prompts List [ str ] Initial set of prompts to start optimization with. required task BaseTask Task object for prompt evaluation. required callbacks List [ Callable ] List of callback functions. Defaults to an empty list. [] predictor optional Predictor for prompt evaluation. Defaults to None. None Source code in promptolution\\optimizers\\base_optimizer.py class BaseOptimizer(ABC): \"\"\" Abstract base class for prompt optimizers. This class defines the basic structure and interface for prompt optimization algorithms. Concrete optimizer implementations should inherit from this class and implement the `optimize` method. Attributes: prompts (List[str]): List of current prompts being optimized. task (BaseTask): The task object used for evaluating prompts. callbacks (List[Callable]): List of callback functions to be called during optimization. predictor: The predictor used for prompt evaluation (if applicable). Args: initial_prompts (List[str]): Initial set of prompts to start optimization with. task (BaseTask): Task object for prompt evaluation. callbacks (List[Callable], optional): List of callback functions. Defaults to an empty list. predictor (optional): Predictor for prompt evaluation. Defaults to None. \"\"\" def __init__(self, initial_prompts: list[str], task: BaseTask, callbacks: list[Callable] = [], predictor=None): self.prompts = initial_prompts self.task = task self.callbacks = callbacks self.predictor = predictor @abstractmethod def optimize(self, n_steps: int) -> List[str]: \"\"\" Abstract method to perform the optimization process. This method should be implemented by concrete optimizer classes to define the specific optimization algorithm. Args: n_steps (int): Number of optimization steps to perform. Returns: List[str]: The optimized list of prompts after all steps. Raises: NotImplementedError: If not implemented by a concrete class. \"\"\" raise NotImplementedError def _on_step_end(self): \"\"\" Call all registered callbacks at the end of each optimization step. \"\"\" for callback in self.callbacks: callback.on_step_end(self) def _on_epoch_end(self): \"\"\" Call all registered callbacks at the end of each optimization epoch. \"\"\" for callback in self.callbacks: callback.on_epoch_end(self) def _on_train_end(self): \"\"\" Call all registered callbacks at the end of the entire optimization process. \"\"\" for callback in self.callbacks: callback.on_train_end(self)","title":"BaseOptimizer"},{"location":"api/optimizers/#promptolution.optimizers.base_optimizer.BaseOptimizer.optimize","text":"Abstract method to perform the optimization process. This method should be implemented by concrete optimizer classes to define the specific optimization algorithm. Parameters: Name Type Description Default n_steps int Number of optimization steps to perform. required Returns: Type Description List [ str ] List[str]: The optimized list of prompts after all steps. Raises: Type Description NotImplementedError If not implemented by a concrete class. Source code in promptolution\\optimizers\\base_optimizer.py @abstractmethod def optimize(self, n_steps: int) -> List[str]: \"\"\" Abstract method to perform the optimization process. This method should be implemented by concrete optimizer classes to define the specific optimization algorithm. Args: n_steps (int): Number of optimization steps to perform. Returns: List[str]: The optimized list of prompts after all steps. Raises: NotImplementedError: If not implemented by a concrete class. \"\"\" raise NotImplementedError","title":"optimize"},{"location":"api/optimizers/#promptolution.optimizers.base_optimizer.DummyOptimizer","text":"Bases: BaseOptimizer A dummy optimizer that doesn't perform any actual optimization. This optimizer simply returns the initial prompts without modification. It's useful for testing or as a baseline comparison. Attributes: Name Type Description prompts List [ str ] List of prompts (unchanged from initialization). callbacks List [ Callable ] Empty list of callbacks. Parameters: Name Type Description Default initial_prompts List [ str ] Initial set of prompts. required *args Variable length argument list (unused). () **kwargs Arbitrary keyword arguments (unused). {} Source code in promptolution\\optimizers\\base_optimizer.py class DummyOptimizer(BaseOptimizer): \"\"\" A dummy optimizer that doesn't perform any actual optimization. This optimizer simply returns the initial prompts without modification. It's useful for testing or as a baseline comparison. Attributes: prompts (List[str]): List of prompts (unchanged from initialization). callbacks (List[Callable]): Empty list of callbacks. Args: initial_prompts (List[str]): Initial set of prompts. *args: Variable length argument list (unused). **kwargs: Arbitrary keyword arguments (unused). \"\"\" def __init__(self, initial_prompts, *args, **kwargs): self.callbacks = [] self.prompts = initial_prompts def optimize(self, n_steps) -> list[str]: \"\"\" Simulate an optimization process without actually modifying the prompts. This method calls the callback methods to simulate a complete optimization cycle, but returns the initial prompts unchanged. Args: n_steps (int): Number of optimization steps (unused in this implementation). Returns: List[str]: The original list of prompts, unchanged. \"\"\" self._on_step_end() self._on_epoch_end() self._on_train_end() return self.prompts","title":"DummyOptimizer"},{"location":"api/optimizers/#promptolution.optimizers.base_optimizer.DummyOptimizer.optimize","text":"Simulate an optimization process without actually modifying the prompts. This method calls the callback methods to simulate a complete optimization cycle, but returns the initial prompts unchanged. Parameters: Name Type Description Default n_steps int Number of optimization steps (unused in this implementation). required Returns: Type Description list [ str ] List[str]: The original list of prompts, unchanged. Source code in promptolution\\optimizers\\base_optimizer.py def optimize(self, n_steps) -> list[str]: \"\"\" Simulate an optimization process without actually modifying the prompts. This method calls the callback methods to simulate a complete optimization cycle, but returns the initial prompts unchanged. Args: n_steps (int): Number of optimization steps (unused in this implementation). Returns: List[str]: The original list of prompts, unchanged. \"\"\" self._on_step_end() self._on_epoch_end() self._on_train_end() return self.prompts","title":"optimize"},{"location":"api/optimizers/#evoprompt-de","text":"","title":"EvoPrompt DE"},{"location":"api/optimizers/#promptolution.optimizers.evoprompt_de.EvoPromptDE","text":"Bases: BaseOptimizer EvoPromptDE: Differential Evolution-based Prompt Optimizer This class implements a differential evolution algorithm for optimizing prompts in large language models. It is adapted from the paper \"Connecting Large Language Models with Evolutionary Algorithms Yields Powerful Prompt Optimizers\" by Guo et al., 2023. The optimizer uses a differential evolution strategy to generate new prompts from existing ones, with an option to use the current best prompt as a donor. Attributes: Name Type Description prompt_template str Template for generating meta-prompts during evolution. donor_random bool If False, uses the current best prompt as a donor; if True, uses a random prompt. meta_llm Language model used for generating child prompts from meta-prompts. Parameters: Name Type Description Default prompt_template str Template for meta-prompts. required meta_llm Language model for child prompt generation. required donor_random bool Whether to use a random donor. Defaults to False. False **args Additional arguments passed to the BaseOptimizer. {} Source code in promptolution\\optimizers\\evoprompt_de.py class EvoPromptDE(BaseOptimizer): \"\"\" EvoPromptDE: Differential Evolution-based Prompt Optimizer This class implements a differential evolution algorithm for optimizing prompts in large language models. It is adapted from the paper \"Connecting Large Language Models with Evolutionary Algorithms Yields Powerful Prompt Optimizers\" by Guo et al., 2023. The optimizer uses a differential evolution strategy to generate new prompts from existing ones, with an option to use the current best prompt as a donor. Attributes: prompt_template (str): Template for generating meta-prompts during evolution. donor_random (bool): If False, uses the current best prompt as a donor; if True, uses a random prompt. meta_llm: Language model used for generating child prompts from meta-prompts. Args: prompt_template (str): Template for meta-prompts. meta_llm: Language model for child prompt generation. donor_random (bool, optional): Whether to use a random donor. Defaults to False. **args: Additional arguments passed to the BaseOptimizer. \"\"\" def __init__(self, prompt_template, meta_llm, donor_random=False, **args): self.prompt_template = prompt_template self.donor_random = donor_random self.meta_llm = meta_llm super().__init__(**args) def optimize(self, n_steps: int) -> List[str]: \"\"\" Perform the optimization process for a specified number of steps. This method iteratively improves the prompts using a differential evolution strategy. It evaluates prompts, generates new prompts using the DE algorithm, and replaces prompts if the new ones perform better. Args: n_steps (int): Number of optimization steps to perform. Returns: List[str]: The optimized list of prompts after all steps. \"\"\" self.scores = self.task.evaluate(self.prompts, self.predictor) self.prompts = [prompt for _, prompt in sorted(zip(self.scores, self.prompts), reverse=True)] self.scores = sorted(self.scores, reverse=True) for _ in range(n_steps): cur_best = self.prompts[0] meta_prompts = [] for i in range(len(self.prompts)): # create meta prompts old_prompt = self.prompts[i] candidates = [prompt for prompt in self.prompts if prompt != old_prompt] a, b, c = np.random.choice(candidates, size=3, replace=False) if not self.donor_random: c = cur_best meta_prompt = ( self.prompt_template.replace(\"<prompt0>\", old_prompt) .replace(\"<prompt1>\", a) .replace(\"<prompt2>\", b) .replace(\"<prompt3>\", c) ) meta_prompts.append(meta_prompt) child_prompts = self.meta_llm.get_response(meta_prompts) child_prompts = [prompt.split(\"<prompt>\")[-1].split(\"</prompt>\")[0].strip() for prompt in child_prompts] child_scores = self.task.evaluate(child_prompts, self.predictor) for i in range(len(self.prompts)): if child_scores[i] > self.scores[i]: self.prompts[i] = child_prompts[i] self.scores[i] = child_scores[i] self._on_step_end() self._on_train_end() return self.prompts","title":"EvoPromptDE"},{"location":"api/optimizers/#promptolution.optimizers.evoprompt_de.EvoPromptDE.optimize","text":"Perform the optimization process for a specified number of steps. This method iteratively improves the prompts using a differential evolution strategy. It evaluates prompts, generates new prompts using the DE algorithm, and replaces prompts if the new ones perform better. Parameters: Name Type Description Default n_steps int Number of optimization steps to perform. required Returns: Type Description List [ str ] List[str]: The optimized list of prompts after all steps. Source code in promptolution\\optimizers\\evoprompt_de.py def optimize(self, n_steps: int) -> List[str]: \"\"\" Perform the optimization process for a specified number of steps. This method iteratively improves the prompts using a differential evolution strategy. It evaluates prompts, generates new prompts using the DE algorithm, and replaces prompts if the new ones perform better. Args: n_steps (int): Number of optimization steps to perform. Returns: List[str]: The optimized list of prompts after all steps. \"\"\" self.scores = self.task.evaluate(self.prompts, self.predictor) self.prompts = [prompt for _, prompt in sorted(zip(self.scores, self.prompts), reverse=True)] self.scores = sorted(self.scores, reverse=True) for _ in range(n_steps): cur_best = self.prompts[0] meta_prompts = [] for i in range(len(self.prompts)): # create meta prompts old_prompt = self.prompts[i] candidates = [prompt for prompt in self.prompts if prompt != old_prompt] a, b, c = np.random.choice(candidates, size=3, replace=False) if not self.donor_random: c = cur_best meta_prompt = ( self.prompt_template.replace(\"<prompt0>\", old_prompt) .replace(\"<prompt1>\", a) .replace(\"<prompt2>\", b) .replace(\"<prompt3>\", c) ) meta_prompts.append(meta_prompt) child_prompts = self.meta_llm.get_response(meta_prompts) child_prompts = [prompt.split(\"<prompt>\")[-1].split(\"</prompt>\")[0].strip() for prompt in child_prompts] child_scores = self.task.evaluate(child_prompts, self.predictor) for i in range(len(self.prompts)): if child_scores[i] > self.scores[i]: self.prompts[i] = child_prompts[i] self.scores[i] = child_scores[i] self._on_step_end() self._on_train_end() return self.prompts","title":"optimize"},{"location":"api/optimizers/#evoprompt-ga","text":"","title":"EvoPrompt GA"},{"location":"api/optimizers/#promptolution.optimizers.evoprompt_ga.EvoPromptGA","text":"Bases: BaseOptimizer EvoPromptGA: Genetic Algorithm-based Prompt Optimizer This class implements a genetic algorithm for optimizing prompts in large language models. It is adapted from the paper \"Connecting Large Language Models with Evolutionary Algorithms Yields Powerful Prompt Optimizers\" by Guo et al., 2023. The optimizer uses crossover operations to generate new prompts from existing ones, with different selection methods available for choosing parent prompts. Attributes: Name Type Description prompt_template str Template for generating meta-prompts during crossover. meta_llm Language model used for generating child prompts from meta-prompts. selection_mode str Method for selecting parent prompts ('random', 'wheel', or 'tour'). Parameters: Name Type Description Default prompt_template str Template for meta-prompts. required meta_llm Language model for child prompt generation. required selection_mode str Parent selection method. Defaults to \"wheel\". 'wheel' **args Additional arguments passed to the BaseOptimizer. {} Raises: Type Description AssertionError If an invalid selection mode is provided. Source code in promptolution\\optimizers\\evoprompt_ga.py class EvoPromptGA(BaseOptimizer): \"\"\" EvoPromptGA: Genetic Algorithm-based Prompt Optimizer This class implements a genetic algorithm for optimizing prompts in large language models. It is adapted from the paper \"Connecting Large Language Models with Evolutionary Algorithms Yields Powerful Prompt Optimizers\" by Guo et al., 2023. The optimizer uses crossover operations to generate new prompts from existing ones, with different selection methods available for choosing parent prompts. Attributes: prompt_template (str): Template for generating meta-prompts during crossover. meta_llm: Language model used for generating child prompts from meta-prompts. selection_mode (str): Method for selecting parent prompts ('random', 'wheel', or 'tour'). Args: prompt_template (str): Template for meta-prompts. meta_llm: Language model for child prompt generation. selection_mode (str, optional): Parent selection method. Defaults to \"wheel\". **args: Additional arguments passed to the BaseOptimizer. Raises: AssertionError: If an invalid selection mode is provided. \"\"\" def __init__(self, prompt_template, meta_llm, selection_mode=\"wheel\", **args): self.prompt_template = prompt_template self.meta_llm = meta_llm assert selection_mode in [\"random\", \"wheel\", \"tour\"], \"Invalid selection mode.\" self.selection_mode = selection_mode super().__init__(**args) def optimize(self, n_steps: int) -> List[str]: \"\"\" Perform the optimization process for a specified number of steps. This method iteratively improves the prompts using genetic algorithm techniques. It evaluates prompts, performs crossover to generate new prompts, and selects the best prompts for the next generation. Args: n_steps (int): Number of optimization steps to perform. Returns: List[str]: The optimized list of prompts after all steps. \"\"\" # get scores from task self.scores = self.task.evaluate(self.prompts, self.predictor).tolist() # sort prompts by score self.prompts = [prompt for _, prompt in sorted(zip(self.scores, self.prompts), reverse=True)] self.scores = sorted(self.scores, reverse=True) for _ in range(n_steps): new_prompts = self._crossover(self.prompts, self.scores) prompts = self.prompts + new_prompts scores = self.scores + self.task.evaluate(new_prompts, self.predictor).tolist() # sort scores and prompts self.prompts = [prompt for _, prompt in sorted(zip(scores, prompts), reverse=True)][: len(self.prompts)] self.scores = sorted(scores, reverse=True)[: len(self.prompts)] self._on_step_end() return self.prompts def _crossover(self, prompts, scores) -> str: \"\"\" Perform crossover operation to generate new child prompts. This method selects parent prompts based on the chosen selection mode, creates meta-prompts using the prompt template, and generates new child prompts using the meta language model. Args: prompts (List[str]): List of current prompts. scores (List[float]): Corresponding scores for the prompts. Returns: List[str]: Newly generated child prompts. \"\"\" # parent selection if self.selection_mode == \"wheel\": wheel_idx = np.random.choice( np.arange(0, len(prompts)), size=len(prompts), replace=True, p=np.array(scores) / np.sum(scores) if np.sum(scores) > 0 else np.ones(len(scores)) / len(scores), ).tolist() parent_pop = [self.prompts[idx] for idx in wheel_idx] elif self.selection_mode in [\"random\", \"tour\"]: parent_pop = self.prompts # crossover meta_prompts = [] for _ in self.prompts: if self.selection_mode in [\"random\", \"wheel\"]: parent_1, parent_2 = np.random.choice(parent_pop, size=2, replace=False) elif self.selection_mode == \"tour\": group_1 = np.random.choice(parent_pop, size=2, replace=False) group_2 = np.random.choice(parent_pop, size=2, replace=False) # use the best of each group based on scores parent_1 = group_1[np.argmax([self.scores[self.prompts.index(p)] for p in group_1])] parent_2 = group_2[np.argmax([self.scores[self.prompts.index(p)] for p in group_2])] meta_prompt = self.prompt_template.replace(\"<prompt1>\", parent_1).replace(\"<prompt2>\", parent_2) meta_prompts.append(meta_prompt) child_prompts = self.meta_llm.get_response(meta_prompts) child_prompts = [prompt.split(\"<prompt>\")[-1].split(\"</prompt>\")[0].strip() for prompt in child_prompts] return child_prompts","title":"EvoPromptGA"},{"location":"api/optimizers/#promptolution.optimizers.evoprompt_ga.EvoPromptGA.optimize","text":"Perform the optimization process for a specified number of steps. This method iteratively improves the prompts using genetic algorithm techniques. It evaluates prompts, performs crossover to generate new prompts, and selects the best prompts for the next generation. Parameters: Name Type Description Default n_steps int Number of optimization steps to perform. required Returns: Type Description List [ str ] List[str]: The optimized list of prompts after all steps. Source code in promptolution\\optimizers\\evoprompt_ga.py def optimize(self, n_steps: int) -> List[str]: \"\"\" Perform the optimization process for a specified number of steps. This method iteratively improves the prompts using genetic algorithm techniques. It evaluates prompts, performs crossover to generate new prompts, and selects the best prompts for the next generation. Args: n_steps (int): Number of optimization steps to perform. Returns: List[str]: The optimized list of prompts after all steps. \"\"\" # get scores from task self.scores = self.task.evaluate(self.prompts, self.predictor).tolist() # sort prompts by score self.prompts = [prompt for _, prompt in sorted(zip(self.scores, self.prompts), reverse=True)] self.scores = sorted(self.scores, reverse=True) for _ in range(n_steps): new_prompts = self._crossover(self.prompts, self.scores) prompts = self.prompts + new_prompts scores = self.scores + self.task.evaluate(new_prompts, self.predictor).tolist() # sort scores and prompts self.prompts = [prompt for _, prompt in sorted(zip(scores, prompts), reverse=True)][: len(self.prompts)] self.scores = sorted(scores, reverse=True)[: len(self.prompts)] self._on_step_end() return self.prompts","title":"optimize"},{"location":"api/predictors/","text":"Predictors This module contains predictor implementations for various tasks. get_predictor(name, *args, **kwargs) Factory function to create and return a predictor instance based on the provided name. This function supports two types of predictors: 1. DummyPredictor: A mock predictor for testing purposes. 2. Classificator: A real predictor using a language model for classification tasks. Parameters: Name Type Description Default name str Identifier for the predictor to use. Special case: - \"dummy\" for DummyPredictor - Any other string for Classificator with the specified LLM required *args Variable length argument list passed to the predictor constructor. () **kwargs Arbitrary keyword arguments passed to the predictor constructor. {} Returns: Type Description An instance of DummyPredictor or Classificator based on the name. Notes For non-dummy predictors, this function calls get_llm to obtain the language model. The batch_size for the language model is currently commented out and not used. Examples: >>> dummy_pred = get_predictor(\"dummy\", classes=[\"A\", \"B\", \"C\"]) >>> real_pred = get_predictor(\"gpt-3.5-turbo\", classes=[\"positive\", \"negative\"]) Source code in promptolution\\predictors\\__init__.py def get_predictor(name, *args, **kwargs): \"\"\" Factory function to create and return a predictor instance based on the provided name. This function supports two types of predictors: 1. DummyPredictor: A mock predictor for testing purposes. 2. Classificator: A real predictor using a language model for classification tasks. Args: name (str): Identifier for the predictor to use. Special case: - \"dummy\" for DummyPredictor - Any other string for Classificator with the specified LLM *args: Variable length argument list passed to the predictor constructor. **kwargs: Arbitrary keyword arguments passed to the predictor constructor. Returns: An instance of DummyPredictor or Classificator based on the name. Notes: - For non-dummy predictors, this function calls get_llm to obtain the language model. - The batch_size for the language model is currently commented out and not used. Examples: >>> dummy_pred = get_predictor(\"dummy\", classes=[\"A\", \"B\", \"C\"]) >>> real_pred = get_predictor(\"gpt-3.5-turbo\", classes=[\"positive\", \"negative\"]) \"\"\" if name == \"dummy\": return DummyPredictor(\"\", *args, **kwargs) downstream_llm = get_llm(name) # , batch_size=config.downstream_bs) return Classificator(downstream_llm, *args, **kwargs) base_predictor BasePredictor Abstract base class for predictors in the promptolution library. This class defines the interface that all concrete predictor implementations should follow. Attributes: Name Type Description model_id str Identifier for the model used by the predictor. classes List [ str ] List of possible class labels for classification tasks. Methods: Name Description predict An abstract method that should be implemented by subclasses to make predictions based on prompts and input data. Source code in promptolution\\predictors\\base_predictor.py class BasePredictor: \"\"\" Abstract base class for predictors in the promptolution library. This class defines the interface that all concrete predictor implementations should follow. Attributes: model_id (str): Identifier for the model used by the predictor. classes (List[str]): List of possible class labels for classification tasks. Methods: predict: An abstract method that should be implemented by subclasses to make predictions based on prompts and input data. \"\"\" def __init__(self, model_id, classes, *args, **kwargs): \"\"\" Initialize the BasePredictor. Args: model_id (str): Identifier for the model to use. classes (List[str]): List of possible class labels. *args: Variable length argument list. **kwargs: Arbitrary keyword arguments. \"\"\" self.model_id = model_id self.classes = classes @abstractmethod def predict( self, prompts: List[str], xs: np.ndarray, ) -> np.ndarray: \"\"\" Abstract method to make predictions based on prompts and input data. Args: prompts (List[str]): List of prompts to use for prediction. xs (np.ndarray): Array of input data. Returns: np.ndarray: Array of predictions. Raises: NotImplementedError: If not implemented by a subclass. \"\"\" raise NotImplementedError __init__(model_id, classes, *args, **kwargs) Initialize the BasePredictor. Parameters: Name Type Description Default model_id str Identifier for the model to use. required classes List [ str ] List of possible class labels. required *args Variable length argument list. () **kwargs Arbitrary keyword arguments. {} Source code in promptolution\\predictors\\base_predictor.py def __init__(self, model_id, classes, *args, **kwargs): \"\"\" Initialize the BasePredictor. Args: model_id (str): Identifier for the model to use. classes (List[str]): List of possible class labels. *args: Variable length argument list. **kwargs: Arbitrary keyword arguments. \"\"\" self.model_id = model_id self.classes = classes predict(prompts, xs) abstractmethod Abstract method to make predictions based on prompts and input data. Parameters: Name Type Description Default prompts List [ str ] List of prompts to use for prediction. required xs ndarray Array of input data. required Returns: Type Description ndarray np.ndarray: Array of predictions. Raises: Type Description NotImplementedError If not implemented by a subclass. Source code in promptolution\\predictors\\base_predictor.py @abstractmethod def predict( self, prompts: List[str], xs: np.ndarray, ) -> np.ndarray: \"\"\" Abstract method to make predictions based on prompts and input data. Args: prompts (List[str]): List of prompts to use for prediction. xs (np.ndarray): Array of input data. Returns: np.ndarray: Array of predictions. Raises: NotImplementedError: If not implemented by a subclass. \"\"\" raise NotImplementedError DummyPredictor Bases: BasePredictor A dummy predictor implementation for testing purposes. This predictor generates random predictions from the list of possible classes. Attributes: Name Type Description model_id str Always set to \"dummy\". classes List [ str ] List of possible class labels. Methods: Name Description predict Generates random predictions for the given prompts and input data. Source code in promptolution\\predictors\\base_predictor.py class DummyPredictor(BasePredictor): \"\"\" A dummy predictor implementation for testing purposes. This predictor generates random predictions from the list of possible classes. Attributes: model_id (str): Always set to \"dummy\". classes (List[str]): List of possible class labels. Methods: predict: Generates random predictions for the given prompts and input data. \"\"\" def __init__(self, model_id, classes, *args, **kwargs): self.model_id = \"dummy\" self.classes = classes def predict( self, prompts: List[str], xs: np.ndarray, ) -> np.ndarray: \"\"\" Generate random predictions for the given prompts and input data. Args: prompts (List[str]): List of prompts (ignored in this implementation). xs (np.ndarray): Array of input data (only the length is used). Returns: np.ndarray: 2D array of random predictions, shape (len(prompts), len(xs)). \"\"\" return np.array([np.random.choice(self.classes, len(xs)) for _ in prompts]) predict(prompts, xs) Generate random predictions for the given prompts and input data. Parameters: Name Type Description Default prompts List [ str ] List of prompts (ignored in this implementation). required xs ndarray Array of input data (only the length is used). required Returns: Type Description ndarray np.ndarray: 2D array of random predictions, shape (len(prompts), len(xs)). Source code in promptolution\\predictors\\base_predictor.py def predict( self, prompts: List[str], xs: np.ndarray, ) -> np.ndarray: \"\"\" Generate random predictions for the given prompts and input data. Args: prompts (List[str]): List of prompts (ignored in this implementation). xs (np.ndarray): Array of input data (only the length is used). Returns: np.ndarray: 2D array of random predictions, shape (len(prompts), len(xs)). \"\"\" return np.array([np.random.choice(self.classes, len(xs)) for _ in prompts]) classificator Classificator Bases: BasePredictor A predictor class for classification tasks using language models. This class takes a language model and a list of classes, and provides a method to predict classes for given prompts and input data. Attributes: Name Type Description llm The language model used for generating predictions. classes List [ str ] The list of valid class labels. Inherits from BasePredictor: The base class for predictors in the promptolution library. Source code in promptolution\\predictors\\classificator.py class Classificator(BasePredictor): \"\"\" A predictor class for classification tasks using language models. This class takes a language model and a list of classes, and provides a method to predict classes for given prompts and input data. Attributes: llm: The language model used for generating predictions. classes (List[str]): The list of valid class labels. Inherits from: BasePredictor: The base class for predictors in the promptolution library. \"\"\" def __init__(self, llm, classes, *args, **kwargs): \"\"\" Initialize the Classificator. Args: llm: The language model to use for predictions. classes (List[str]): The list of valid class labels. \"\"\" self.llm = llm self.classes = classes def predict( self, prompts: List[str], xs: np.ndarray, ) -> np.ndarray: \"\"\" Predict classes for given prompts and input data. This method generates predictions using the language model and then extracts the predicted class from the model's output. Args: prompts (List[str]): The list of prompts to use for prediction. xs (np.ndarray): The input data array. Returns: np.ndarray: A 2D array of predicted classes, with shape (len(prompts), len(xs)). Note: The method concatenates each prompt with each input data point, passes it to the language model, and then extracts the first word in the response that matches a class in self.classes. \"\"\" if isinstance(prompts, str): prompts = [prompts] preds = self.llm.get_response([prompt + \"\\n\" + x for prompt in prompts for x in xs]) response = [] for pred in preds: predicted_class = \"\" for word in pred.split(\" \"): word = \"\".join([c for c in word if c.isalpha()]) if word in self.classes: predicted_class = word break response.append(predicted_class) response = np.array(response).reshape(len(prompts), len(xs)) return response __init__(llm, classes, *args, **kwargs) Initialize the Classificator. Parameters: Name Type Description Default llm The language model to use for predictions. required classes List [ str ] The list of valid class labels. required Source code in promptolution\\predictors\\classificator.py def __init__(self, llm, classes, *args, **kwargs): \"\"\" Initialize the Classificator. Args: llm: The language model to use for predictions. classes (List[str]): The list of valid class labels. \"\"\" self.llm = llm self.classes = classes predict(prompts, xs) Predict classes for given prompts and input data. This method generates predictions using the language model and then extracts the predicted class from the model's output. Parameters: Name Type Description Default prompts List [ str ] The list of prompts to use for prediction. required xs ndarray The input data array. required Returns: Type Description ndarray np.ndarray: A 2D array of predicted classes, with shape (len(prompts), len(xs)). Note The method concatenates each prompt with each input data point, passes it to the language model, and then extracts the first word in the response that matches a class in self.classes. Source code in promptolution\\predictors\\classificator.py def predict( self, prompts: List[str], xs: np.ndarray, ) -> np.ndarray: \"\"\" Predict classes for given prompts and input data. This method generates predictions using the language model and then extracts the predicted class from the model's output. Args: prompts (List[str]): The list of prompts to use for prediction. xs (np.ndarray): The input data array. Returns: np.ndarray: A 2D array of predicted classes, with shape (len(prompts), len(xs)). Note: The method concatenates each prompt with each input data point, passes it to the language model, and then extracts the first word in the response that matches a class in self.classes. \"\"\" if isinstance(prompts, str): prompts = [prompts] preds = self.llm.get_response([prompt + \"\\n\" + x for prompt in prompts for x in xs]) response = [] for pred in preds: predicted_class = \"\" for word in pred.split(\" \"): word = \"\".join([c for c in word if c.isalpha()]) if word in self.classes: predicted_class = word break response.append(predicted_class) response = np.array(response).reshape(len(prompts), len(xs)) return response Base Predictor BasePredictor Abstract base class for predictors in the promptolution library. This class defines the interface that all concrete predictor implementations should follow. Attributes: Name Type Description model_id str Identifier for the model used by the predictor. classes List [ str ] List of possible class labels for classification tasks. Methods: Name Description predict An abstract method that should be implemented by subclasses to make predictions based on prompts and input data. Source code in promptolution\\predictors\\base_predictor.py class BasePredictor: \"\"\" Abstract base class for predictors in the promptolution library. This class defines the interface that all concrete predictor implementations should follow. Attributes: model_id (str): Identifier for the model used by the predictor. classes (List[str]): List of possible class labels for classification tasks. Methods: predict: An abstract method that should be implemented by subclasses to make predictions based on prompts and input data. \"\"\" def __init__(self, model_id, classes, *args, **kwargs): \"\"\" Initialize the BasePredictor. Args: model_id (str): Identifier for the model to use. classes (List[str]): List of possible class labels. *args: Variable length argument list. **kwargs: Arbitrary keyword arguments. \"\"\" self.model_id = model_id self.classes = classes @abstractmethod def predict( self, prompts: List[str], xs: np.ndarray, ) -> np.ndarray: \"\"\" Abstract method to make predictions based on prompts and input data. Args: prompts (List[str]): List of prompts to use for prediction. xs (np.ndarray): Array of input data. Returns: np.ndarray: Array of predictions. Raises: NotImplementedError: If not implemented by a subclass. \"\"\" raise NotImplementedError __init__(model_id, classes, *args, **kwargs) Initialize the BasePredictor. Parameters: Name Type Description Default model_id str Identifier for the model to use. required classes List [ str ] List of possible class labels. required *args Variable length argument list. () **kwargs Arbitrary keyword arguments. {} Source code in promptolution\\predictors\\base_predictor.py def __init__(self, model_id, classes, *args, **kwargs): \"\"\" Initialize the BasePredictor. Args: model_id (str): Identifier for the model to use. classes (List[str]): List of possible class labels. *args: Variable length argument list. **kwargs: Arbitrary keyword arguments. \"\"\" self.model_id = model_id self.classes = classes predict(prompts, xs) abstractmethod Abstract method to make predictions based on prompts and input data. Parameters: Name Type Description Default prompts List [ str ] List of prompts to use for prediction. required xs ndarray Array of input data. required Returns: Type Description ndarray np.ndarray: Array of predictions. Raises: Type Description NotImplementedError If not implemented by a subclass. Source code in promptolution\\predictors\\base_predictor.py @abstractmethod def predict( self, prompts: List[str], xs: np.ndarray, ) -> np.ndarray: \"\"\" Abstract method to make predictions based on prompts and input data. Args: prompts (List[str]): List of prompts to use for prediction. xs (np.ndarray): Array of input data. Returns: np.ndarray: Array of predictions. Raises: NotImplementedError: If not implemented by a subclass. \"\"\" raise NotImplementedError DummyPredictor Bases: BasePredictor A dummy predictor implementation for testing purposes. This predictor generates random predictions from the list of possible classes. Attributes: Name Type Description model_id str Always set to \"dummy\". classes List [ str ] List of possible class labels. Methods: Name Description predict Generates random predictions for the given prompts and input data. Source code in promptolution\\predictors\\base_predictor.py class DummyPredictor(BasePredictor): \"\"\" A dummy predictor implementation for testing purposes. This predictor generates random predictions from the list of possible classes. Attributes: model_id (str): Always set to \"dummy\". classes (List[str]): List of possible class labels. Methods: predict: Generates random predictions for the given prompts and input data. \"\"\" def __init__(self, model_id, classes, *args, **kwargs): self.model_id = \"dummy\" self.classes = classes def predict( self, prompts: List[str], xs: np.ndarray, ) -> np.ndarray: \"\"\" Generate random predictions for the given prompts and input data. Args: prompts (List[str]): List of prompts (ignored in this implementation). xs (np.ndarray): Array of input data (only the length is used). Returns: np.ndarray: 2D array of random predictions, shape (len(prompts), len(xs)). \"\"\" return np.array([np.random.choice(self.classes, len(xs)) for _ in prompts]) predict(prompts, xs) Generate random predictions for the given prompts and input data. Parameters: Name Type Description Default prompts List [ str ] List of prompts (ignored in this implementation). required xs ndarray Array of input data (only the length is used). required Returns: Type Description ndarray np.ndarray: 2D array of random predictions, shape (len(prompts), len(xs)). Source code in promptolution\\predictors\\base_predictor.py def predict( self, prompts: List[str], xs: np.ndarray, ) -> np.ndarray: \"\"\" Generate random predictions for the given prompts and input data. Args: prompts (List[str]): List of prompts (ignored in this implementation). xs (np.ndarray): Array of input data (only the length is used). Returns: np.ndarray: 2D array of random predictions, shape (len(prompts), len(xs)). \"\"\" return np.array([np.random.choice(self.classes, len(xs)) for _ in prompts]) Classifier Classificator Bases: BasePredictor A predictor class for classification tasks using language models. This class takes a language model and a list of classes, and provides a method to predict classes for given prompts and input data. Attributes: Name Type Description llm The language model used for generating predictions. classes List [ str ] The list of valid class labels. Inherits from BasePredictor: The base class for predictors in the promptolution library. Source code in promptolution\\predictors\\classificator.py class Classificator(BasePredictor): \"\"\" A predictor class for classification tasks using language models. This class takes a language model and a list of classes, and provides a method to predict classes for given prompts and input data. Attributes: llm: The language model used for generating predictions. classes (List[str]): The list of valid class labels. Inherits from: BasePredictor: The base class for predictors in the promptolution library. \"\"\" def __init__(self, llm, classes, *args, **kwargs): \"\"\" Initialize the Classificator. Args: llm: The language model to use for predictions. classes (List[str]): The list of valid class labels. \"\"\" self.llm = llm self.classes = classes def predict( self, prompts: List[str], xs: np.ndarray, ) -> np.ndarray: \"\"\" Predict classes for given prompts and input data. This method generates predictions using the language model and then extracts the predicted class from the model's output. Args: prompts (List[str]): The list of prompts to use for prediction. xs (np.ndarray): The input data array. Returns: np.ndarray: A 2D array of predicted classes, with shape (len(prompts), len(xs)). Note: The method concatenates each prompt with each input data point, passes it to the language model, and then extracts the first word in the response that matches a class in self.classes. \"\"\" if isinstance(prompts, str): prompts = [prompts] preds = self.llm.get_response([prompt + \"\\n\" + x for prompt in prompts for x in xs]) response = [] for pred in preds: predicted_class = \"\" for word in pred.split(\" \"): word = \"\".join([c for c in word if c.isalpha()]) if word in self.classes: predicted_class = word break response.append(predicted_class) response = np.array(response).reshape(len(prompts), len(xs)) return response __init__(llm, classes, *args, **kwargs) Initialize the Classificator. Parameters: Name Type Description Default llm The language model to use for predictions. required classes List [ str ] The list of valid class labels. required Source code in promptolution\\predictors\\classificator.py def __init__(self, llm, classes, *args, **kwargs): \"\"\" Initialize the Classificator. Args: llm: The language model to use for predictions. classes (List[str]): The list of valid class labels. \"\"\" self.llm = llm self.classes = classes predict(prompts, xs) Predict classes for given prompts and input data. This method generates predictions using the language model and then extracts the predicted class from the model's output. Parameters: Name Type Description Default prompts List [ str ] The list of prompts to use for prediction. required xs ndarray The input data array. required Returns: Type Description ndarray np.ndarray: A 2D array of predicted classes, with shape (len(prompts), len(xs)). Note The method concatenates each prompt with each input data point, passes it to the language model, and then extracts the first word in the response that matches a class in self.classes. Source code in promptolution\\predictors\\classificator.py def predict( self, prompts: List[str], xs: np.ndarray, ) -> np.ndarray: \"\"\" Predict classes for given prompts and input data. This method generates predictions using the language model and then extracts the predicted class from the model's output. Args: prompts (List[str]): The list of prompts to use for prediction. xs (np.ndarray): The input data array. Returns: np.ndarray: A 2D array of predicted classes, with shape (len(prompts), len(xs)). Note: The method concatenates each prompt with each input data point, passes it to the language model, and then extracts the first word in the response that matches a class in self.classes. \"\"\" if isinstance(prompts, str): prompts = [prompts] preds = self.llm.get_response([prompt + \"\\n\" + x for prompt in prompts for x in xs]) response = [] for pred in preds: predicted_class = \"\" for word in pred.split(\" \"): word = \"\".join([c for c in word if c.isalpha()]) if word in self.classes: predicted_class = word break response.append(predicted_class) response = np.array(response).reshape(len(prompts), len(xs)) return response","title":"Predictors"},{"location":"api/predictors/#predictors","text":"This module contains predictor implementations for various tasks.","title":"Predictors"},{"location":"api/predictors/#promptolution.predictors.get_predictor","text":"Factory function to create and return a predictor instance based on the provided name. This function supports two types of predictors: 1. DummyPredictor: A mock predictor for testing purposes. 2. Classificator: A real predictor using a language model for classification tasks. Parameters: Name Type Description Default name str Identifier for the predictor to use. Special case: - \"dummy\" for DummyPredictor - Any other string for Classificator with the specified LLM required *args Variable length argument list passed to the predictor constructor. () **kwargs Arbitrary keyword arguments passed to the predictor constructor. {} Returns: Type Description An instance of DummyPredictor or Classificator based on the name. Notes For non-dummy predictors, this function calls get_llm to obtain the language model. The batch_size for the language model is currently commented out and not used. Examples: >>> dummy_pred = get_predictor(\"dummy\", classes=[\"A\", \"B\", \"C\"]) >>> real_pred = get_predictor(\"gpt-3.5-turbo\", classes=[\"positive\", \"negative\"]) Source code in promptolution\\predictors\\__init__.py def get_predictor(name, *args, **kwargs): \"\"\" Factory function to create and return a predictor instance based on the provided name. This function supports two types of predictors: 1. DummyPredictor: A mock predictor for testing purposes. 2. Classificator: A real predictor using a language model for classification tasks. Args: name (str): Identifier for the predictor to use. Special case: - \"dummy\" for DummyPredictor - Any other string for Classificator with the specified LLM *args: Variable length argument list passed to the predictor constructor. **kwargs: Arbitrary keyword arguments passed to the predictor constructor. Returns: An instance of DummyPredictor or Classificator based on the name. Notes: - For non-dummy predictors, this function calls get_llm to obtain the language model. - The batch_size for the language model is currently commented out and not used. Examples: >>> dummy_pred = get_predictor(\"dummy\", classes=[\"A\", \"B\", \"C\"]) >>> real_pred = get_predictor(\"gpt-3.5-turbo\", classes=[\"positive\", \"negative\"]) \"\"\" if name == \"dummy\": return DummyPredictor(\"\", *args, **kwargs) downstream_llm = get_llm(name) # , batch_size=config.downstream_bs) return Classificator(downstream_llm, *args, **kwargs)","title":"get_predictor"},{"location":"api/predictors/#promptolution.predictors.base_predictor","text":"","title":"base_predictor"},{"location":"api/predictors/#promptolution.predictors.base_predictor.BasePredictor","text":"Abstract base class for predictors in the promptolution library. This class defines the interface that all concrete predictor implementations should follow. Attributes: Name Type Description model_id str Identifier for the model used by the predictor. classes List [ str ] List of possible class labels for classification tasks. Methods: Name Description predict An abstract method that should be implemented by subclasses to make predictions based on prompts and input data. Source code in promptolution\\predictors\\base_predictor.py class BasePredictor: \"\"\" Abstract base class for predictors in the promptolution library. This class defines the interface that all concrete predictor implementations should follow. Attributes: model_id (str): Identifier for the model used by the predictor. classes (List[str]): List of possible class labels for classification tasks. Methods: predict: An abstract method that should be implemented by subclasses to make predictions based on prompts and input data. \"\"\" def __init__(self, model_id, classes, *args, **kwargs): \"\"\" Initialize the BasePredictor. Args: model_id (str): Identifier for the model to use. classes (List[str]): List of possible class labels. *args: Variable length argument list. **kwargs: Arbitrary keyword arguments. \"\"\" self.model_id = model_id self.classes = classes @abstractmethod def predict( self, prompts: List[str], xs: np.ndarray, ) -> np.ndarray: \"\"\" Abstract method to make predictions based on prompts and input data. Args: prompts (List[str]): List of prompts to use for prediction. xs (np.ndarray): Array of input data. Returns: np.ndarray: Array of predictions. Raises: NotImplementedError: If not implemented by a subclass. \"\"\" raise NotImplementedError","title":"BasePredictor"},{"location":"api/predictors/#promptolution.predictors.base_predictor.BasePredictor.__init__","text":"Initialize the BasePredictor. Parameters: Name Type Description Default model_id str Identifier for the model to use. required classes List [ str ] List of possible class labels. required *args Variable length argument list. () **kwargs Arbitrary keyword arguments. {} Source code in promptolution\\predictors\\base_predictor.py def __init__(self, model_id, classes, *args, **kwargs): \"\"\" Initialize the BasePredictor. Args: model_id (str): Identifier for the model to use. classes (List[str]): List of possible class labels. *args: Variable length argument list. **kwargs: Arbitrary keyword arguments. \"\"\" self.model_id = model_id self.classes = classes","title":"__init__"},{"location":"api/predictors/#promptolution.predictors.base_predictor.BasePredictor.predict","text":"Abstract method to make predictions based on prompts and input data. Parameters: Name Type Description Default prompts List [ str ] List of prompts to use for prediction. required xs ndarray Array of input data. required Returns: Type Description ndarray np.ndarray: Array of predictions. Raises: Type Description NotImplementedError If not implemented by a subclass. Source code in promptolution\\predictors\\base_predictor.py @abstractmethod def predict( self, prompts: List[str], xs: np.ndarray, ) -> np.ndarray: \"\"\" Abstract method to make predictions based on prompts and input data. Args: prompts (List[str]): List of prompts to use for prediction. xs (np.ndarray): Array of input data. Returns: np.ndarray: Array of predictions. Raises: NotImplementedError: If not implemented by a subclass. \"\"\" raise NotImplementedError","title":"predict"},{"location":"api/predictors/#promptolution.predictors.base_predictor.DummyPredictor","text":"Bases: BasePredictor A dummy predictor implementation for testing purposes. This predictor generates random predictions from the list of possible classes. Attributes: Name Type Description model_id str Always set to \"dummy\". classes List [ str ] List of possible class labels. Methods: Name Description predict Generates random predictions for the given prompts and input data. Source code in promptolution\\predictors\\base_predictor.py class DummyPredictor(BasePredictor): \"\"\" A dummy predictor implementation for testing purposes. This predictor generates random predictions from the list of possible classes. Attributes: model_id (str): Always set to \"dummy\". classes (List[str]): List of possible class labels. Methods: predict: Generates random predictions for the given prompts and input data. \"\"\" def __init__(self, model_id, classes, *args, **kwargs): self.model_id = \"dummy\" self.classes = classes def predict( self, prompts: List[str], xs: np.ndarray, ) -> np.ndarray: \"\"\" Generate random predictions for the given prompts and input data. Args: prompts (List[str]): List of prompts (ignored in this implementation). xs (np.ndarray): Array of input data (only the length is used). Returns: np.ndarray: 2D array of random predictions, shape (len(prompts), len(xs)). \"\"\" return np.array([np.random.choice(self.classes, len(xs)) for _ in prompts])","title":"DummyPredictor"},{"location":"api/predictors/#promptolution.predictors.base_predictor.DummyPredictor.predict","text":"Generate random predictions for the given prompts and input data. Parameters: Name Type Description Default prompts List [ str ] List of prompts (ignored in this implementation). required xs ndarray Array of input data (only the length is used). required Returns: Type Description ndarray np.ndarray: 2D array of random predictions, shape (len(prompts), len(xs)). Source code in promptolution\\predictors\\base_predictor.py def predict( self, prompts: List[str], xs: np.ndarray, ) -> np.ndarray: \"\"\" Generate random predictions for the given prompts and input data. Args: prompts (List[str]): List of prompts (ignored in this implementation). xs (np.ndarray): Array of input data (only the length is used). Returns: np.ndarray: 2D array of random predictions, shape (len(prompts), len(xs)). \"\"\" return np.array([np.random.choice(self.classes, len(xs)) for _ in prompts])","title":"predict"},{"location":"api/predictors/#promptolution.predictors.classificator","text":"","title":"classificator"},{"location":"api/predictors/#promptolution.predictors.classificator.Classificator","text":"Bases: BasePredictor A predictor class for classification tasks using language models. This class takes a language model and a list of classes, and provides a method to predict classes for given prompts and input data. Attributes: Name Type Description llm The language model used for generating predictions. classes List [ str ] The list of valid class labels. Inherits from BasePredictor: The base class for predictors in the promptolution library. Source code in promptolution\\predictors\\classificator.py class Classificator(BasePredictor): \"\"\" A predictor class for classification tasks using language models. This class takes a language model and a list of classes, and provides a method to predict classes for given prompts and input data. Attributes: llm: The language model used for generating predictions. classes (List[str]): The list of valid class labels. Inherits from: BasePredictor: The base class for predictors in the promptolution library. \"\"\" def __init__(self, llm, classes, *args, **kwargs): \"\"\" Initialize the Classificator. Args: llm: The language model to use for predictions. classes (List[str]): The list of valid class labels. \"\"\" self.llm = llm self.classes = classes def predict( self, prompts: List[str], xs: np.ndarray, ) -> np.ndarray: \"\"\" Predict classes for given prompts and input data. This method generates predictions using the language model and then extracts the predicted class from the model's output. Args: prompts (List[str]): The list of prompts to use for prediction. xs (np.ndarray): The input data array. Returns: np.ndarray: A 2D array of predicted classes, with shape (len(prompts), len(xs)). Note: The method concatenates each prompt with each input data point, passes it to the language model, and then extracts the first word in the response that matches a class in self.classes. \"\"\" if isinstance(prompts, str): prompts = [prompts] preds = self.llm.get_response([prompt + \"\\n\" + x for prompt in prompts for x in xs]) response = [] for pred in preds: predicted_class = \"\" for word in pred.split(\" \"): word = \"\".join([c for c in word if c.isalpha()]) if word in self.classes: predicted_class = word break response.append(predicted_class) response = np.array(response).reshape(len(prompts), len(xs)) return response","title":"Classificator"},{"location":"api/predictors/#promptolution.predictors.classificator.Classificator.__init__","text":"Initialize the Classificator. Parameters: Name Type Description Default llm The language model to use for predictions. required classes List [ str ] The list of valid class labels. required Source code in promptolution\\predictors\\classificator.py def __init__(self, llm, classes, *args, **kwargs): \"\"\" Initialize the Classificator. Args: llm: The language model to use for predictions. classes (List[str]): The list of valid class labels. \"\"\" self.llm = llm self.classes = classes","title":"__init__"},{"location":"api/predictors/#promptolution.predictors.classificator.Classificator.predict","text":"Predict classes for given prompts and input data. This method generates predictions using the language model and then extracts the predicted class from the model's output. Parameters: Name Type Description Default prompts List [ str ] The list of prompts to use for prediction. required xs ndarray The input data array. required Returns: Type Description ndarray np.ndarray: A 2D array of predicted classes, with shape (len(prompts), len(xs)). Note The method concatenates each prompt with each input data point, passes it to the language model, and then extracts the first word in the response that matches a class in self.classes. Source code in promptolution\\predictors\\classificator.py def predict( self, prompts: List[str], xs: np.ndarray, ) -> np.ndarray: \"\"\" Predict classes for given prompts and input data. This method generates predictions using the language model and then extracts the predicted class from the model's output. Args: prompts (List[str]): The list of prompts to use for prediction. xs (np.ndarray): The input data array. Returns: np.ndarray: A 2D array of predicted classes, with shape (len(prompts), len(xs)). Note: The method concatenates each prompt with each input data point, passes it to the language model, and then extracts the first word in the response that matches a class in self.classes. \"\"\" if isinstance(prompts, str): prompts = [prompts] preds = self.llm.get_response([prompt + \"\\n\" + x for prompt in prompts for x in xs]) response = [] for pred in preds: predicted_class = \"\" for word in pred.split(\" \"): word = \"\".join([c for c in word if c.isalpha()]) if word in self.classes: predicted_class = word break response.append(predicted_class) response = np.array(response).reshape(len(prompts), len(xs)) return response","title":"predict"},{"location":"api/predictors/#base-predictor","text":"","title":"Base Predictor"},{"location":"api/predictors/#promptolution.predictors.base_predictor.BasePredictor","text":"Abstract base class for predictors in the promptolution library. This class defines the interface that all concrete predictor implementations should follow. Attributes: Name Type Description model_id str Identifier for the model used by the predictor. classes List [ str ] List of possible class labels for classification tasks. Methods: Name Description predict An abstract method that should be implemented by subclasses to make predictions based on prompts and input data. Source code in promptolution\\predictors\\base_predictor.py class BasePredictor: \"\"\" Abstract base class for predictors in the promptolution library. This class defines the interface that all concrete predictor implementations should follow. Attributes: model_id (str): Identifier for the model used by the predictor. classes (List[str]): List of possible class labels for classification tasks. Methods: predict: An abstract method that should be implemented by subclasses to make predictions based on prompts and input data. \"\"\" def __init__(self, model_id, classes, *args, **kwargs): \"\"\" Initialize the BasePredictor. Args: model_id (str): Identifier for the model to use. classes (List[str]): List of possible class labels. *args: Variable length argument list. **kwargs: Arbitrary keyword arguments. \"\"\" self.model_id = model_id self.classes = classes @abstractmethod def predict( self, prompts: List[str], xs: np.ndarray, ) -> np.ndarray: \"\"\" Abstract method to make predictions based on prompts and input data. Args: prompts (List[str]): List of prompts to use for prediction. xs (np.ndarray): Array of input data. Returns: np.ndarray: Array of predictions. Raises: NotImplementedError: If not implemented by a subclass. \"\"\" raise NotImplementedError","title":"BasePredictor"},{"location":"api/predictors/#promptolution.predictors.base_predictor.BasePredictor.__init__","text":"Initialize the BasePredictor. Parameters: Name Type Description Default model_id str Identifier for the model to use. required classes List [ str ] List of possible class labels. required *args Variable length argument list. () **kwargs Arbitrary keyword arguments. {} Source code in promptolution\\predictors\\base_predictor.py def __init__(self, model_id, classes, *args, **kwargs): \"\"\" Initialize the BasePredictor. Args: model_id (str): Identifier for the model to use. classes (List[str]): List of possible class labels. *args: Variable length argument list. **kwargs: Arbitrary keyword arguments. \"\"\" self.model_id = model_id self.classes = classes","title":"__init__"},{"location":"api/predictors/#promptolution.predictors.base_predictor.BasePredictor.predict","text":"Abstract method to make predictions based on prompts and input data. Parameters: Name Type Description Default prompts List [ str ] List of prompts to use for prediction. required xs ndarray Array of input data. required Returns: Type Description ndarray np.ndarray: Array of predictions. Raises: Type Description NotImplementedError If not implemented by a subclass. Source code in promptolution\\predictors\\base_predictor.py @abstractmethod def predict( self, prompts: List[str], xs: np.ndarray, ) -> np.ndarray: \"\"\" Abstract method to make predictions based on prompts and input data. Args: prompts (List[str]): List of prompts to use for prediction. xs (np.ndarray): Array of input data. Returns: np.ndarray: Array of predictions. Raises: NotImplementedError: If not implemented by a subclass. \"\"\" raise NotImplementedError","title":"predict"},{"location":"api/predictors/#promptolution.predictors.base_predictor.DummyPredictor","text":"Bases: BasePredictor A dummy predictor implementation for testing purposes. This predictor generates random predictions from the list of possible classes. Attributes: Name Type Description model_id str Always set to \"dummy\". classes List [ str ] List of possible class labels. Methods: Name Description predict Generates random predictions for the given prompts and input data. Source code in promptolution\\predictors\\base_predictor.py class DummyPredictor(BasePredictor): \"\"\" A dummy predictor implementation for testing purposes. This predictor generates random predictions from the list of possible classes. Attributes: model_id (str): Always set to \"dummy\". classes (List[str]): List of possible class labels. Methods: predict: Generates random predictions for the given prompts and input data. \"\"\" def __init__(self, model_id, classes, *args, **kwargs): self.model_id = \"dummy\" self.classes = classes def predict( self, prompts: List[str], xs: np.ndarray, ) -> np.ndarray: \"\"\" Generate random predictions for the given prompts and input data. Args: prompts (List[str]): List of prompts (ignored in this implementation). xs (np.ndarray): Array of input data (only the length is used). Returns: np.ndarray: 2D array of random predictions, shape (len(prompts), len(xs)). \"\"\" return np.array([np.random.choice(self.classes, len(xs)) for _ in prompts])","title":"DummyPredictor"},{"location":"api/predictors/#promptolution.predictors.base_predictor.DummyPredictor.predict","text":"Generate random predictions for the given prompts and input data. Parameters: Name Type Description Default prompts List [ str ] List of prompts (ignored in this implementation). required xs ndarray Array of input data (only the length is used). required Returns: Type Description ndarray np.ndarray: 2D array of random predictions, shape (len(prompts), len(xs)). Source code in promptolution\\predictors\\base_predictor.py def predict( self, prompts: List[str], xs: np.ndarray, ) -> np.ndarray: \"\"\" Generate random predictions for the given prompts and input data. Args: prompts (List[str]): List of prompts (ignored in this implementation). xs (np.ndarray): Array of input data (only the length is used). Returns: np.ndarray: 2D array of random predictions, shape (len(prompts), len(xs)). \"\"\" return np.array([np.random.choice(self.classes, len(xs)) for _ in prompts])","title":"predict"},{"location":"api/predictors/#classifier","text":"","title":"Classifier"},{"location":"api/predictors/#promptolution.predictors.classificator.Classificator","text":"Bases: BasePredictor A predictor class for classification tasks using language models. This class takes a language model and a list of classes, and provides a method to predict classes for given prompts and input data. Attributes: Name Type Description llm The language model used for generating predictions. classes List [ str ] The list of valid class labels. Inherits from BasePredictor: The base class for predictors in the promptolution library. Source code in promptolution\\predictors\\classificator.py class Classificator(BasePredictor): \"\"\" A predictor class for classification tasks using language models. This class takes a language model and a list of classes, and provides a method to predict classes for given prompts and input data. Attributes: llm: The language model used for generating predictions. classes (List[str]): The list of valid class labels. Inherits from: BasePredictor: The base class for predictors in the promptolution library. \"\"\" def __init__(self, llm, classes, *args, **kwargs): \"\"\" Initialize the Classificator. Args: llm: The language model to use for predictions. classes (List[str]): The list of valid class labels. \"\"\" self.llm = llm self.classes = classes def predict( self, prompts: List[str], xs: np.ndarray, ) -> np.ndarray: \"\"\" Predict classes for given prompts and input data. This method generates predictions using the language model and then extracts the predicted class from the model's output. Args: prompts (List[str]): The list of prompts to use for prediction. xs (np.ndarray): The input data array. Returns: np.ndarray: A 2D array of predicted classes, with shape (len(prompts), len(xs)). Note: The method concatenates each prompt with each input data point, passes it to the language model, and then extracts the first word in the response that matches a class in self.classes. \"\"\" if isinstance(prompts, str): prompts = [prompts] preds = self.llm.get_response([prompt + \"\\n\" + x for prompt in prompts for x in xs]) response = [] for pred in preds: predicted_class = \"\" for word in pred.split(\" \"): word = \"\".join([c for c in word if c.isalpha()]) if word in self.classes: predicted_class = word break response.append(predicted_class) response = np.array(response).reshape(len(prompts), len(xs)) return response","title":"Classificator"},{"location":"api/predictors/#promptolution.predictors.classificator.Classificator.__init__","text":"Initialize the Classificator. Parameters: Name Type Description Default llm The language model to use for predictions. required classes List [ str ] The list of valid class labels. required Source code in promptolution\\predictors\\classificator.py def __init__(self, llm, classes, *args, **kwargs): \"\"\" Initialize the Classificator. Args: llm: The language model to use for predictions. classes (List[str]): The list of valid class labels. \"\"\" self.llm = llm self.classes = classes","title":"__init__"},{"location":"api/predictors/#promptolution.predictors.classificator.Classificator.predict","text":"Predict classes for given prompts and input data. This method generates predictions using the language model and then extracts the predicted class from the model's output. Parameters: Name Type Description Default prompts List [ str ] The list of prompts to use for prediction. required xs ndarray The input data array. required Returns: Type Description ndarray np.ndarray: A 2D array of predicted classes, with shape (len(prompts), len(xs)). Note The method concatenates each prompt with each input data point, passes it to the language model, and then extracts the first word in the response that matches a class in self.classes. Source code in promptolution\\predictors\\classificator.py def predict( self, prompts: List[str], xs: np.ndarray, ) -> np.ndarray: \"\"\" Predict classes for given prompts and input data. This method generates predictions using the language model and then extracts the predicted class from the model's output. Args: prompts (List[str]): The list of prompts to use for prediction. xs (np.ndarray): The input data array. Returns: np.ndarray: A 2D array of predicted classes, with shape (len(prompts), len(xs)). Note: The method concatenates each prompt with each input data point, passes it to the language model, and then extracts the first word in the response that matches a class in self.classes. \"\"\" if isinstance(prompts, str): prompts = [prompts] preds = self.llm.get_response([prompt + \"\\n\" + x for prompt in prompts for x in xs]) response = [] for pred in preds: predicted_class = \"\" for word in pred.split(\" \"): word = \"\".join([c for c in word if c.isalpha()]) if word in self.classes: predicted_class = word break response.append(predicted_class) response = np.array(response).reshape(len(prompts), len(xs)) return response","title":"predict"},{"location":"api/tasks/","text":"Tasks This module contains task-specific implementations. get_tasks(config, split='dev') Create and return a list of task instances based on the provided configuration. This function supports creating multiple tasks, including a special 'dummy' task for testing purposes and classification tasks based on JSON descriptions. Parameters: Name Type Description Default config Configuration object containing task settings. Expected attributes: - task_name (str): Comma-separated list of task names. - ds_path (str): Path to the dataset directory. - random_seed (int): Seed for random number generation. required split Literal ['dev', 'test'] Dataset split to use. Defaults to \"dev\". 'dev' Returns: Type Description List [ BaseTask ] List[BaseTask]: A list of instantiated task objects. Raises: Type Description FileNotFoundError If the task description file is not found. JSONDecodeError If the task description file is not valid JSON. Notes The 'dummy' task is a special case that creates a DummyTask instance. For all other tasks, a ClassificationTask instance is created. The task description is loaded from a 'description.json' file in the dataset path. Source code in promptolution\\tasks\\__init__.py def get_tasks(config, split: Literal[\"dev\", \"test\"] = \"dev\") -> List[BaseTask]: \"\"\" Create and return a list of task instances based on the provided configuration. This function supports creating multiple tasks, including a special 'dummy' task for testing purposes and classification tasks based on JSON descriptions. Args: config: Configuration object containing task settings. Expected attributes: - task_name (str): Comma-separated list of task names. - ds_path (str): Path to the dataset directory. - random_seed (int): Seed for random number generation. split (Literal[\"dev\", \"test\"], optional): Dataset split to use. Defaults to \"dev\". Returns: List[BaseTask]: A list of instantiated task objects. Raises: FileNotFoundError: If the task description file is not found. json.JSONDecodeError: If the task description file is not valid JSON. Notes: - The 'dummy' task is a special case that creates a DummyTask instance. - For all other tasks, a ClassificationTask instance is created. - The task description is loaded from a 'description.json' file in the dataset path. \"\"\" task_names = config.task_name.split(\",\") task_list = [] for task_name in task_names: task_description_path = Path(config.ds_path) / Path(\"description.json\") task_description = json.loads(task_description_path.read_text()) if task_name == \"dummy\": task = DummyTask() task_list.append(task) continue task = ClassificationTask(task_name, task_description, split=split, seed=config.random_seed) task_list.append(task) return task_list base_task BaseTask Bases: ABC Abstract base class for tasks in the promptolution library. This class defines the interface that all concrete task implementations should follow. Methods: Name Description evaluate An abstract method that should be implemented by subclasses to evaluate prompts using a given predictor. Source code in promptolution\\tasks\\base_task.py class BaseTask(ABC): \"\"\" Abstract base class for tasks in the promptolution library. This class defines the interface that all concrete task implementations should follow. Methods: evaluate: An abstract method that should be implemented by subclasses to evaluate prompts using a given predictor. \"\"\" def __init__(self, *args, **kwargs): pass @abstractmethod def evaluate(self, prompts: List[str], predictor) -> np.ndarray: \"\"\" Abstract method to evaluate prompts using a given predictor. Args: prompts (List[str]): List of prompts to evaluate. predictor: The predictor to use for evaluation. Returns: np.ndarray: Array of evaluation scores for each prompt. Raises: NotImplementedError: If not implemented by a subclass. \"\"\" raise NotImplementedError evaluate(prompts, predictor) abstractmethod Abstract method to evaluate prompts using a given predictor. Parameters: Name Type Description Default prompts List [ str ] List of prompts to evaluate. required predictor The predictor to use for evaluation. required Returns: Type Description ndarray np.ndarray: Array of evaluation scores for each prompt. Raises: Type Description NotImplementedError If not implemented by a subclass. Source code in promptolution\\tasks\\base_task.py @abstractmethod def evaluate(self, prompts: List[str], predictor) -> np.ndarray: \"\"\" Abstract method to evaluate prompts using a given predictor. Args: prompts (List[str]): List of prompts to evaluate. predictor: The predictor to use for evaluation. Returns: np.ndarray: Array of evaluation scores for each prompt. Raises: NotImplementedError: If not implemented by a subclass. \"\"\" raise NotImplementedError DummyTask Bases: BaseTask A dummy task implementation for testing purposes. This task generates random evaluation scores for given prompts. Attributes: Name Type Description task_id str Identifier for the task, set to \"dummy\". dataset_json None Placeholder for dataset information. initial_population List [ str ] List of initial prompts. description str Description of the dummy task. xs ndarray Array of dummy input data. ys ndarray Array of dummy labels. classes List [ str ] List of possible class labels. Source code in promptolution\\tasks\\base_task.py class DummyTask(BaseTask): \"\"\" A dummy task implementation for testing purposes. This task generates random evaluation scores for given prompts. Attributes: task_id (str): Identifier for the task, set to \"dummy\". dataset_json (None): Placeholder for dataset information. initial_population (List[str]): List of initial prompts. description (str): Description of the dummy task. xs (np.ndarray): Array of dummy input data. ys (np.ndarray): Array of dummy labels. classes (List[str]): List of possible class labels. \"\"\" def __init__(self): self.task_id = \"dummy\" self.dataset_json = None self.initial_population = [\"Some\", \"initial\", \"prompts\", \"that\", \"will\", \"do\", \"the\", \"trick\"] self.description = \"This is a dummy task for testing purposes.\" self.xs = np.array([\"This is a test\", \"This is another test\", \"This is a third test\"]) self.ys = np.array([\"positive\", \"negative\", \"positive\"]) self.classes = [\"negative\", \"positive\"] def evaluate(self, prompts: List[str], predictor) -> np.ndarray: \"\"\" Generate random evaluation scores for the given prompts. Args: prompts (List[str]): List of prompts to evaluate. predictor: The predictor to use for evaluation (ignored in this implementation). Returns: np.ndarray: Array of random evaluation scores, one for each prompt. \"\"\" return np.array([np.random.rand()] * len(prompts)) evaluate(prompts, predictor) Generate random evaluation scores for the given prompts. Parameters: Name Type Description Default prompts List [ str ] List of prompts to evaluate. required predictor The predictor to use for evaluation (ignored in this implementation). required Returns: Type Description ndarray np.ndarray: Array of random evaluation scores, one for each prompt. Source code in promptolution\\tasks\\base_task.py def evaluate(self, prompts: List[str], predictor) -> np.ndarray: \"\"\" Generate random evaluation scores for the given prompts. Args: prompts (List[str]): List of prompts to evaluate. predictor: The predictor to use for evaluation (ignored in this implementation). Returns: np.ndarray: Array of random evaluation scores, one for each prompt. \"\"\" return np.array([np.random.rand()] * len(prompts)) classification_tasks ClassificationTask Bases: BaseTask A class representing a classification task in the promptolution library. This class handles the loading and management of classification datasets, as well as the evaluation of predictors on these datasets. Attributes: Name Type Description task_id str Unique identifier for the task. dataset_json Dict Dictionary containing dataset information. description Optional [ str ] Description of the task. initial_population Optional [ List [ str ]] Initial set of prompts. xs Optional [ ndarray ] Input data for the task. ys Optional [ ndarray ] Ground truth labels for the task. classes Optional [ List ] List of possible class labels. split Literal ['dev', 'test'] Dataset split to use. seed int Random seed for reproducibility. Inherits from BaseTask: The base class for tasks in the promptolution library. Source code in promptolution\\tasks\\classification_tasks.py class ClassificationTask(BaseTask): \"\"\" A class representing a classification task in the promptolution library. This class handles the loading and management of classification datasets, as well as the evaluation of predictors on these datasets. Attributes: task_id (str): Unique identifier for the task. dataset_json (Dict): Dictionary containing dataset information. description (Optional[str]): Description of the task. initial_population (Optional[List[str]]): Initial set of prompts. xs (Optional[np.ndarray]): Input data for the task. ys (Optional[np.ndarray]): Ground truth labels for the task. classes (Optional[List]): List of possible class labels. split (Literal[\"dev\", \"test\"]): Dataset split to use. seed (int): Random seed for reproducibility. Inherits from: BaseTask: The base class for tasks in the promptolution library. \"\"\" def __init__(self, task_id: str, dataset_json: Dict, seed: int = 42, split: Literal[\"dev\", \"test\"] = \"dev\"): \"\"\" Initialize the ClassificationTask. Args: task_id (str): Unique identifier for the task. dataset_json (Dict): Dictionary containing dataset information. seed (int, optional): Random seed for reproducibility. Defaults to 42. split (Literal[\"dev\", \"test\"], optional): Dataset split to use. Defaults to \"dev\". \"\"\" self.task_id: str = task_id self.dataset_json: Dict = dataset_json self.description: Optional[str] = None self.initial_population: Optional[List[str]] = None self.xs: Optional[np.ndarray] = np.array([]) self.ys: Optional[np.ndarray] = None self.classes: Optional[List] = None self.split: Literal[\"dev\", \"test\"] = split self._parse_task() self.reset_seed(seed) def __str__(self): return self.task_id def _parse_task(self): \"\"\" Parse the task data from the provided dataset JSON. This method loads the task description, classes, initial prompts, and the dataset split (dev or test) into the class attributes. \"\"\" task_path = Path(self.dataset_json[\"path\"]) self.description = self.dataset_json[\"description\"] self.classes = self.dataset_json[\"classes\"] with open(task_path / Path(self.dataset_json[\"init_prompts\"]), \"r\", encoding=\"utf-8\") as file: lines = file.readlines() self.initial_population = [line.strip() for line in lines] seed = Path(self.dataset_json[\"seed\"]) split = Path(self.split + \".txt\") with open(task_path / seed / split, \"r\", encoding=\"utf-8\") as file: lines = file.readlines() lines = [line.strip() for line in lines] xs = [] ys = [] for line in lines: x, y = line.split(\"\\t\") xs.append(x) ys.append(self.classes[int(y)]) self.xs = np.array(xs) self.ys = np.array(ys) def evaluate( self, prompts: List[str], predictor: BasePredictor, n_samples: int = 20, subsample: bool = True ) -> np.ndarray: \"\"\" Evaluate a set of prompts using a given predictor. Args: prompts (List[str]): List of prompts to evaluate. predictor (BasePredictor): Predictor to use for evaluation. n_samples (int, optional): Number of samples to use if subsampling. Defaults to 20. subsample (bool, optional): Whether to use subsampling. Defaults to True. Returns: np.ndarray: Array of accuracy scores for each prompt. \"\"\" if isinstance(prompts, str): prompts = [prompts] # Randomly select a subsample of n_samples if subsample: indices = np.random.choice(len(self.xs), n_samples, replace=False) else: indices = np.arange(len(self.xs)) xs_subsample = self.xs[indices] ys_subsample = self.ys[indices] # Make predictions on the subsample preds = predictor.predict(prompts, xs_subsample) # Calculate accuracy: number of correct predictions / total number of predictions per prompt return np.mean(preds == ys_subsample, axis=1) def reset_seed(self, seed: int = None): if seed is not None: self.seed = seed np.random.seed(self.seed) __init__(task_id, dataset_json, seed=42, split='dev') Initialize the ClassificationTask. Parameters: Name Type Description Default task_id str Unique identifier for the task. required dataset_json Dict Dictionary containing dataset information. required seed int Random seed for reproducibility. Defaults to 42. 42 split Literal ['dev', 'test'] Dataset split to use. Defaults to \"dev\". 'dev' Source code in promptolution\\tasks\\classification_tasks.py def __init__(self, task_id: str, dataset_json: Dict, seed: int = 42, split: Literal[\"dev\", \"test\"] = \"dev\"): \"\"\" Initialize the ClassificationTask. Args: task_id (str): Unique identifier for the task. dataset_json (Dict): Dictionary containing dataset information. seed (int, optional): Random seed for reproducibility. Defaults to 42. split (Literal[\"dev\", \"test\"], optional): Dataset split to use. Defaults to \"dev\". \"\"\" self.task_id: str = task_id self.dataset_json: Dict = dataset_json self.description: Optional[str] = None self.initial_population: Optional[List[str]] = None self.xs: Optional[np.ndarray] = np.array([]) self.ys: Optional[np.ndarray] = None self.classes: Optional[List] = None self.split: Literal[\"dev\", \"test\"] = split self._parse_task() self.reset_seed(seed) evaluate(prompts, predictor, n_samples=20, subsample=True) Evaluate a set of prompts using a given predictor. Parameters: Name Type Description Default prompts List [ str ] List of prompts to evaluate. required predictor BasePredictor Predictor to use for evaluation. required n_samples int Number of samples to use if subsampling. Defaults to 20. 20 subsample bool Whether to use subsampling. Defaults to True. True Returns: Type Description ndarray np.ndarray: Array of accuracy scores for each prompt. Source code in promptolution\\tasks\\classification_tasks.py def evaluate( self, prompts: List[str], predictor: BasePredictor, n_samples: int = 20, subsample: bool = True ) -> np.ndarray: \"\"\" Evaluate a set of prompts using a given predictor. Args: prompts (List[str]): List of prompts to evaluate. predictor (BasePredictor): Predictor to use for evaluation. n_samples (int, optional): Number of samples to use if subsampling. Defaults to 20. subsample (bool, optional): Whether to use subsampling. Defaults to True. Returns: np.ndarray: Array of accuracy scores for each prompt. \"\"\" if isinstance(prompts, str): prompts = [prompts] # Randomly select a subsample of n_samples if subsample: indices = np.random.choice(len(self.xs), n_samples, replace=False) else: indices = np.arange(len(self.xs)) xs_subsample = self.xs[indices] ys_subsample = self.ys[indices] # Make predictions on the subsample preds = predictor.predict(prompts, xs_subsample) # Calculate accuracy: number of correct predictions / total number of predictions per prompt return np.mean(preds == ys_subsample, axis=1) Base Task BaseTask Bases: ABC Abstract base class for tasks in the promptolution library. This class defines the interface that all concrete task implementations should follow. Methods: Name Description evaluate An abstract method that should be implemented by subclasses to evaluate prompts using a given predictor. Source code in promptolution\\tasks\\base_task.py class BaseTask(ABC): \"\"\" Abstract base class for tasks in the promptolution library. This class defines the interface that all concrete task implementations should follow. Methods: evaluate: An abstract method that should be implemented by subclasses to evaluate prompts using a given predictor. \"\"\" def __init__(self, *args, **kwargs): pass @abstractmethod def evaluate(self, prompts: List[str], predictor) -> np.ndarray: \"\"\" Abstract method to evaluate prompts using a given predictor. Args: prompts (List[str]): List of prompts to evaluate. predictor: The predictor to use for evaluation. Returns: np.ndarray: Array of evaluation scores for each prompt. Raises: NotImplementedError: If not implemented by a subclass. \"\"\" raise NotImplementedError evaluate(prompts, predictor) abstractmethod Abstract method to evaluate prompts using a given predictor. Parameters: Name Type Description Default prompts List [ str ] List of prompts to evaluate. required predictor The predictor to use for evaluation. required Returns: Type Description ndarray np.ndarray: Array of evaluation scores for each prompt. Raises: Type Description NotImplementedError If not implemented by a subclass. Source code in promptolution\\tasks\\base_task.py @abstractmethod def evaluate(self, prompts: List[str], predictor) -> np.ndarray: \"\"\" Abstract method to evaluate prompts using a given predictor. Args: prompts (List[str]): List of prompts to evaluate. predictor: The predictor to use for evaluation. Returns: np.ndarray: Array of evaluation scores for each prompt. Raises: NotImplementedError: If not implemented by a subclass. \"\"\" raise NotImplementedError DummyTask Bases: BaseTask A dummy task implementation for testing purposes. This task generates random evaluation scores for given prompts. Attributes: Name Type Description task_id str Identifier for the task, set to \"dummy\". dataset_json None Placeholder for dataset information. initial_population List [ str ] List of initial prompts. description str Description of the dummy task. xs ndarray Array of dummy input data. ys ndarray Array of dummy labels. classes List [ str ] List of possible class labels. Source code in promptolution\\tasks\\base_task.py class DummyTask(BaseTask): \"\"\" A dummy task implementation for testing purposes. This task generates random evaluation scores for given prompts. Attributes: task_id (str): Identifier for the task, set to \"dummy\". dataset_json (None): Placeholder for dataset information. initial_population (List[str]): List of initial prompts. description (str): Description of the dummy task. xs (np.ndarray): Array of dummy input data. ys (np.ndarray): Array of dummy labels. classes (List[str]): List of possible class labels. \"\"\" def __init__(self): self.task_id = \"dummy\" self.dataset_json = None self.initial_population = [\"Some\", \"initial\", \"prompts\", \"that\", \"will\", \"do\", \"the\", \"trick\"] self.description = \"This is a dummy task for testing purposes.\" self.xs = np.array([\"This is a test\", \"This is another test\", \"This is a third test\"]) self.ys = np.array([\"positive\", \"negative\", \"positive\"]) self.classes = [\"negative\", \"positive\"] def evaluate(self, prompts: List[str], predictor) -> np.ndarray: \"\"\" Generate random evaluation scores for the given prompts. Args: prompts (List[str]): List of prompts to evaluate. predictor: The predictor to use for evaluation (ignored in this implementation). Returns: np.ndarray: Array of random evaluation scores, one for each prompt. \"\"\" return np.array([np.random.rand()] * len(prompts)) evaluate(prompts, predictor) Generate random evaluation scores for the given prompts. Parameters: Name Type Description Default prompts List [ str ] List of prompts to evaluate. required predictor The predictor to use for evaluation (ignored in this implementation). required Returns: Type Description ndarray np.ndarray: Array of random evaluation scores, one for each prompt. Source code in promptolution\\tasks\\base_task.py def evaluate(self, prompts: List[str], predictor) -> np.ndarray: \"\"\" Generate random evaluation scores for the given prompts. Args: prompts (List[str]): List of prompts to evaluate. predictor: The predictor to use for evaluation (ignored in this implementation). Returns: np.ndarray: Array of random evaluation scores, one for each prompt. \"\"\" return np.array([np.random.rand()] * len(prompts)) Classification Tasks ClassificationTask Bases: BaseTask A class representing a classification task in the promptolution library. This class handles the loading and management of classification datasets, as well as the evaluation of predictors on these datasets. Attributes: Name Type Description task_id str Unique identifier for the task. dataset_json Dict Dictionary containing dataset information. description Optional [ str ] Description of the task. initial_population Optional [ List [ str ]] Initial set of prompts. xs Optional [ ndarray ] Input data for the task. ys Optional [ ndarray ] Ground truth labels for the task. classes Optional [ List ] List of possible class labels. split Literal ['dev', 'test'] Dataset split to use. seed int Random seed for reproducibility. Inherits from BaseTask: The base class for tasks in the promptolution library. Source code in promptolution\\tasks\\classification_tasks.py class ClassificationTask(BaseTask): \"\"\" A class representing a classification task in the promptolution library. This class handles the loading and management of classification datasets, as well as the evaluation of predictors on these datasets. Attributes: task_id (str): Unique identifier for the task. dataset_json (Dict): Dictionary containing dataset information. description (Optional[str]): Description of the task. initial_population (Optional[List[str]]): Initial set of prompts. xs (Optional[np.ndarray]): Input data for the task. ys (Optional[np.ndarray]): Ground truth labels for the task. classes (Optional[List]): List of possible class labels. split (Literal[\"dev\", \"test\"]): Dataset split to use. seed (int): Random seed for reproducibility. Inherits from: BaseTask: The base class for tasks in the promptolution library. \"\"\" def __init__(self, task_id: str, dataset_json: Dict, seed: int = 42, split: Literal[\"dev\", \"test\"] = \"dev\"): \"\"\" Initialize the ClassificationTask. Args: task_id (str): Unique identifier for the task. dataset_json (Dict): Dictionary containing dataset information. seed (int, optional): Random seed for reproducibility. Defaults to 42. split (Literal[\"dev\", \"test\"], optional): Dataset split to use. Defaults to \"dev\". \"\"\" self.task_id: str = task_id self.dataset_json: Dict = dataset_json self.description: Optional[str] = None self.initial_population: Optional[List[str]] = None self.xs: Optional[np.ndarray] = np.array([]) self.ys: Optional[np.ndarray] = None self.classes: Optional[List] = None self.split: Literal[\"dev\", \"test\"] = split self._parse_task() self.reset_seed(seed) def __str__(self): return self.task_id def _parse_task(self): \"\"\" Parse the task data from the provided dataset JSON. This method loads the task description, classes, initial prompts, and the dataset split (dev or test) into the class attributes. \"\"\" task_path = Path(self.dataset_json[\"path\"]) self.description = self.dataset_json[\"description\"] self.classes = self.dataset_json[\"classes\"] with open(task_path / Path(self.dataset_json[\"init_prompts\"]), \"r\", encoding=\"utf-8\") as file: lines = file.readlines() self.initial_population = [line.strip() for line in lines] seed = Path(self.dataset_json[\"seed\"]) split = Path(self.split + \".txt\") with open(task_path / seed / split, \"r\", encoding=\"utf-8\") as file: lines = file.readlines() lines = [line.strip() for line in lines] xs = [] ys = [] for line in lines: x, y = line.split(\"\\t\") xs.append(x) ys.append(self.classes[int(y)]) self.xs = np.array(xs) self.ys = np.array(ys) def evaluate( self, prompts: List[str], predictor: BasePredictor, n_samples: int = 20, subsample: bool = True ) -> np.ndarray: \"\"\" Evaluate a set of prompts using a given predictor. Args: prompts (List[str]): List of prompts to evaluate. predictor (BasePredictor): Predictor to use for evaluation. n_samples (int, optional): Number of samples to use if subsampling. Defaults to 20. subsample (bool, optional): Whether to use subsampling. Defaults to True. Returns: np.ndarray: Array of accuracy scores for each prompt. \"\"\" if isinstance(prompts, str): prompts = [prompts] # Randomly select a subsample of n_samples if subsample: indices = np.random.choice(len(self.xs), n_samples, replace=False) else: indices = np.arange(len(self.xs)) xs_subsample = self.xs[indices] ys_subsample = self.ys[indices] # Make predictions on the subsample preds = predictor.predict(prompts, xs_subsample) # Calculate accuracy: number of correct predictions / total number of predictions per prompt return np.mean(preds == ys_subsample, axis=1) def reset_seed(self, seed: int = None): if seed is not None: self.seed = seed np.random.seed(self.seed) __init__(task_id, dataset_json, seed=42, split='dev') Initialize the ClassificationTask. Parameters: Name Type Description Default task_id str Unique identifier for the task. required dataset_json Dict Dictionary containing dataset information. required seed int Random seed for reproducibility. Defaults to 42. 42 split Literal ['dev', 'test'] Dataset split to use. Defaults to \"dev\". 'dev' Source code in promptolution\\tasks\\classification_tasks.py def __init__(self, task_id: str, dataset_json: Dict, seed: int = 42, split: Literal[\"dev\", \"test\"] = \"dev\"): \"\"\" Initialize the ClassificationTask. Args: task_id (str): Unique identifier for the task. dataset_json (Dict): Dictionary containing dataset information. seed (int, optional): Random seed for reproducibility. Defaults to 42. split (Literal[\"dev\", \"test\"], optional): Dataset split to use. Defaults to \"dev\". \"\"\" self.task_id: str = task_id self.dataset_json: Dict = dataset_json self.description: Optional[str] = None self.initial_population: Optional[List[str]] = None self.xs: Optional[np.ndarray] = np.array([]) self.ys: Optional[np.ndarray] = None self.classes: Optional[List] = None self.split: Literal[\"dev\", \"test\"] = split self._parse_task() self.reset_seed(seed) evaluate(prompts, predictor, n_samples=20, subsample=True) Evaluate a set of prompts using a given predictor. Parameters: Name Type Description Default prompts List [ str ] List of prompts to evaluate. required predictor BasePredictor Predictor to use for evaluation. required n_samples int Number of samples to use if subsampling. Defaults to 20. 20 subsample bool Whether to use subsampling. Defaults to True. True Returns: Type Description ndarray np.ndarray: Array of accuracy scores for each prompt. Source code in promptolution\\tasks\\classification_tasks.py def evaluate( self, prompts: List[str], predictor: BasePredictor, n_samples: int = 20, subsample: bool = True ) -> np.ndarray: \"\"\" Evaluate a set of prompts using a given predictor. Args: prompts (List[str]): List of prompts to evaluate. predictor (BasePredictor): Predictor to use for evaluation. n_samples (int, optional): Number of samples to use if subsampling. Defaults to 20. subsample (bool, optional): Whether to use subsampling. Defaults to True. Returns: np.ndarray: Array of accuracy scores for each prompt. \"\"\" if isinstance(prompts, str): prompts = [prompts] # Randomly select a subsample of n_samples if subsample: indices = np.random.choice(len(self.xs), n_samples, replace=False) else: indices = np.arange(len(self.xs)) xs_subsample = self.xs[indices] ys_subsample = self.ys[indices] # Make predictions on the subsample preds = predictor.predict(prompts, xs_subsample) # Calculate accuracy: number of correct predictions / total number of predictions per prompt return np.mean(preds == ys_subsample, axis=1)","title":"Tasks"},{"location":"api/tasks/#tasks","text":"This module contains task-specific implementations.","title":"Tasks"},{"location":"api/tasks/#promptolution.tasks.get_tasks","text":"Create and return a list of task instances based on the provided configuration. This function supports creating multiple tasks, including a special 'dummy' task for testing purposes and classification tasks based on JSON descriptions. Parameters: Name Type Description Default config Configuration object containing task settings. Expected attributes: - task_name (str): Comma-separated list of task names. - ds_path (str): Path to the dataset directory. - random_seed (int): Seed for random number generation. required split Literal ['dev', 'test'] Dataset split to use. Defaults to \"dev\". 'dev' Returns: Type Description List [ BaseTask ] List[BaseTask]: A list of instantiated task objects. Raises: Type Description FileNotFoundError If the task description file is not found. JSONDecodeError If the task description file is not valid JSON. Notes The 'dummy' task is a special case that creates a DummyTask instance. For all other tasks, a ClassificationTask instance is created. The task description is loaded from a 'description.json' file in the dataset path. Source code in promptolution\\tasks\\__init__.py def get_tasks(config, split: Literal[\"dev\", \"test\"] = \"dev\") -> List[BaseTask]: \"\"\" Create and return a list of task instances based on the provided configuration. This function supports creating multiple tasks, including a special 'dummy' task for testing purposes and classification tasks based on JSON descriptions. Args: config: Configuration object containing task settings. Expected attributes: - task_name (str): Comma-separated list of task names. - ds_path (str): Path to the dataset directory. - random_seed (int): Seed for random number generation. split (Literal[\"dev\", \"test\"], optional): Dataset split to use. Defaults to \"dev\". Returns: List[BaseTask]: A list of instantiated task objects. Raises: FileNotFoundError: If the task description file is not found. json.JSONDecodeError: If the task description file is not valid JSON. Notes: - The 'dummy' task is a special case that creates a DummyTask instance. - For all other tasks, a ClassificationTask instance is created. - The task description is loaded from a 'description.json' file in the dataset path. \"\"\" task_names = config.task_name.split(\",\") task_list = [] for task_name in task_names: task_description_path = Path(config.ds_path) / Path(\"description.json\") task_description = json.loads(task_description_path.read_text()) if task_name == \"dummy\": task = DummyTask() task_list.append(task) continue task = ClassificationTask(task_name, task_description, split=split, seed=config.random_seed) task_list.append(task) return task_list","title":"get_tasks"},{"location":"api/tasks/#promptolution.tasks.base_task","text":"","title":"base_task"},{"location":"api/tasks/#promptolution.tasks.base_task.BaseTask","text":"Bases: ABC Abstract base class for tasks in the promptolution library. This class defines the interface that all concrete task implementations should follow. Methods: Name Description evaluate An abstract method that should be implemented by subclasses to evaluate prompts using a given predictor. Source code in promptolution\\tasks\\base_task.py class BaseTask(ABC): \"\"\" Abstract base class for tasks in the promptolution library. This class defines the interface that all concrete task implementations should follow. Methods: evaluate: An abstract method that should be implemented by subclasses to evaluate prompts using a given predictor. \"\"\" def __init__(self, *args, **kwargs): pass @abstractmethod def evaluate(self, prompts: List[str], predictor) -> np.ndarray: \"\"\" Abstract method to evaluate prompts using a given predictor. Args: prompts (List[str]): List of prompts to evaluate. predictor: The predictor to use for evaluation. Returns: np.ndarray: Array of evaluation scores for each prompt. Raises: NotImplementedError: If not implemented by a subclass. \"\"\" raise NotImplementedError","title":"BaseTask"},{"location":"api/tasks/#promptolution.tasks.base_task.BaseTask.evaluate","text":"Abstract method to evaluate prompts using a given predictor. Parameters: Name Type Description Default prompts List [ str ] List of prompts to evaluate. required predictor The predictor to use for evaluation. required Returns: Type Description ndarray np.ndarray: Array of evaluation scores for each prompt. Raises: Type Description NotImplementedError If not implemented by a subclass. Source code in promptolution\\tasks\\base_task.py @abstractmethod def evaluate(self, prompts: List[str], predictor) -> np.ndarray: \"\"\" Abstract method to evaluate prompts using a given predictor. Args: prompts (List[str]): List of prompts to evaluate. predictor: The predictor to use for evaluation. Returns: np.ndarray: Array of evaluation scores for each prompt. Raises: NotImplementedError: If not implemented by a subclass. \"\"\" raise NotImplementedError","title":"evaluate"},{"location":"api/tasks/#promptolution.tasks.base_task.DummyTask","text":"Bases: BaseTask A dummy task implementation for testing purposes. This task generates random evaluation scores for given prompts. Attributes: Name Type Description task_id str Identifier for the task, set to \"dummy\". dataset_json None Placeholder for dataset information. initial_population List [ str ] List of initial prompts. description str Description of the dummy task. xs ndarray Array of dummy input data. ys ndarray Array of dummy labels. classes List [ str ] List of possible class labels. Source code in promptolution\\tasks\\base_task.py class DummyTask(BaseTask): \"\"\" A dummy task implementation for testing purposes. This task generates random evaluation scores for given prompts. Attributes: task_id (str): Identifier for the task, set to \"dummy\". dataset_json (None): Placeholder for dataset information. initial_population (List[str]): List of initial prompts. description (str): Description of the dummy task. xs (np.ndarray): Array of dummy input data. ys (np.ndarray): Array of dummy labels. classes (List[str]): List of possible class labels. \"\"\" def __init__(self): self.task_id = \"dummy\" self.dataset_json = None self.initial_population = [\"Some\", \"initial\", \"prompts\", \"that\", \"will\", \"do\", \"the\", \"trick\"] self.description = \"This is a dummy task for testing purposes.\" self.xs = np.array([\"This is a test\", \"This is another test\", \"This is a third test\"]) self.ys = np.array([\"positive\", \"negative\", \"positive\"]) self.classes = [\"negative\", \"positive\"] def evaluate(self, prompts: List[str], predictor) -> np.ndarray: \"\"\" Generate random evaluation scores for the given prompts. Args: prompts (List[str]): List of prompts to evaluate. predictor: The predictor to use for evaluation (ignored in this implementation). Returns: np.ndarray: Array of random evaluation scores, one for each prompt. \"\"\" return np.array([np.random.rand()] * len(prompts))","title":"DummyTask"},{"location":"api/tasks/#promptolution.tasks.base_task.DummyTask.evaluate","text":"Generate random evaluation scores for the given prompts. Parameters: Name Type Description Default prompts List [ str ] List of prompts to evaluate. required predictor The predictor to use for evaluation (ignored in this implementation). required Returns: Type Description ndarray np.ndarray: Array of random evaluation scores, one for each prompt. Source code in promptolution\\tasks\\base_task.py def evaluate(self, prompts: List[str], predictor) -> np.ndarray: \"\"\" Generate random evaluation scores for the given prompts. Args: prompts (List[str]): List of prompts to evaluate. predictor: The predictor to use for evaluation (ignored in this implementation). Returns: np.ndarray: Array of random evaluation scores, one for each prompt. \"\"\" return np.array([np.random.rand()] * len(prompts))","title":"evaluate"},{"location":"api/tasks/#promptolution.tasks.classification_tasks","text":"","title":"classification_tasks"},{"location":"api/tasks/#promptolution.tasks.classification_tasks.ClassificationTask","text":"Bases: BaseTask A class representing a classification task in the promptolution library. This class handles the loading and management of classification datasets, as well as the evaluation of predictors on these datasets. Attributes: Name Type Description task_id str Unique identifier for the task. dataset_json Dict Dictionary containing dataset information. description Optional [ str ] Description of the task. initial_population Optional [ List [ str ]] Initial set of prompts. xs Optional [ ndarray ] Input data for the task. ys Optional [ ndarray ] Ground truth labels for the task. classes Optional [ List ] List of possible class labels. split Literal ['dev', 'test'] Dataset split to use. seed int Random seed for reproducibility. Inherits from BaseTask: The base class for tasks in the promptolution library. Source code in promptolution\\tasks\\classification_tasks.py class ClassificationTask(BaseTask): \"\"\" A class representing a classification task in the promptolution library. This class handles the loading and management of classification datasets, as well as the evaluation of predictors on these datasets. Attributes: task_id (str): Unique identifier for the task. dataset_json (Dict): Dictionary containing dataset information. description (Optional[str]): Description of the task. initial_population (Optional[List[str]]): Initial set of prompts. xs (Optional[np.ndarray]): Input data for the task. ys (Optional[np.ndarray]): Ground truth labels for the task. classes (Optional[List]): List of possible class labels. split (Literal[\"dev\", \"test\"]): Dataset split to use. seed (int): Random seed for reproducibility. Inherits from: BaseTask: The base class for tasks in the promptolution library. \"\"\" def __init__(self, task_id: str, dataset_json: Dict, seed: int = 42, split: Literal[\"dev\", \"test\"] = \"dev\"): \"\"\" Initialize the ClassificationTask. Args: task_id (str): Unique identifier for the task. dataset_json (Dict): Dictionary containing dataset information. seed (int, optional): Random seed for reproducibility. Defaults to 42. split (Literal[\"dev\", \"test\"], optional): Dataset split to use. Defaults to \"dev\". \"\"\" self.task_id: str = task_id self.dataset_json: Dict = dataset_json self.description: Optional[str] = None self.initial_population: Optional[List[str]] = None self.xs: Optional[np.ndarray] = np.array([]) self.ys: Optional[np.ndarray] = None self.classes: Optional[List] = None self.split: Literal[\"dev\", \"test\"] = split self._parse_task() self.reset_seed(seed) def __str__(self): return self.task_id def _parse_task(self): \"\"\" Parse the task data from the provided dataset JSON. This method loads the task description, classes, initial prompts, and the dataset split (dev or test) into the class attributes. \"\"\" task_path = Path(self.dataset_json[\"path\"]) self.description = self.dataset_json[\"description\"] self.classes = self.dataset_json[\"classes\"] with open(task_path / Path(self.dataset_json[\"init_prompts\"]), \"r\", encoding=\"utf-8\") as file: lines = file.readlines() self.initial_population = [line.strip() for line in lines] seed = Path(self.dataset_json[\"seed\"]) split = Path(self.split + \".txt\") with open(task_path / seed / split, \"r\", encoding=\"utf-8\") as file: lines = file.readlines() lines = [line.strip() for line in lines] xs = [] ys = [] for line in lines: x, y = line.split(\"\\t\") xs.append(x) ys.append(self.classes[int(y)]) self.xs = np.array(xs) self.ys = np.array(ys) def evaluate( self, prompts: List[str], predictor: BasePredictor, n_samples: int = 20, subsample: bool = True ) -> np.ndarray: \"\"\" Evaluate a set of prompts using a given predictor. Args: prompts (List[str]): List of prompts to evaluate. predictor (BasePredictor): Predictor to use for evaluation. n_samples (int, optional): Number of samples to use if subsampling. Defaults to 20. subsample (bool, optional): Whether to use subsampling. Defaults to True. Returns: np.ndarray: Array of accuracy scores for each prompt. \"\"\" if isinstance(prompts, str): prompts = [prompts] # Randomly select a subsample of n_samples if subsample: indices = np.random.choice(len(self.xs), n_samples, replace=False) else: indices = np.arange(len(self.xs)) xs_subsample = self.xs[indices] ys_subsample = self.ys[indices] # Make predictions on the subsample preds = predictor.predict(prompts, xs_subsample) # Calculate accuracy: number of correct predictions / total number of predictions per prompt return np.mean(preds == ys_subsample, axis=1) def reset_seed(self, seed: int = None): if seed is not None: self.seed = seed np.random.seed(self.seed)","title":"ClassificationTask"},{"location":"api/tasks/#promptolution.tasks.classification_tasks.ClassificationTask.__init__","text":"Initialize the ClassificationTask. Parameters: Name Type Description Default task_id str Unique identifier for the task. required dataset_json Dict Dictionary containing dataset information. required seed int Random seed for reproducibility. Defaults to 42. 42 split Literal ['dev', 'test'] Dataset split to use. Defaults to \"dev\". 'dev' Source code in promptolution\\tasks\\classification_tasks.py def __init__(self, task_id: str, dataset_json: Dict, seed: int = 42, split: Literal[\"dev\", \"test\"] = \"dev\"): \"\"\" Initialize the ClassificationTask. Args: task_id (str): Unique identifier for the task. dataset_json (Dict): Dictionary containing dataset information. seed (int, optional): Random seed for reproducibility. Defaults to 42. split (Literal[\"dev\", \"test\"], optional): Dataset split to use. Defaults to \"dev\". \"\"\" self.task_id: str = task_id self.dataset_json: Dict = dataset_json self.description: Optional[str] = None self.initial_population: Optional[List[str]] = None self.xs: Optional[np.ndarray] = np.array([]) self.ys: Optional[np.ndarray] = None self.classes: Optional[List] = None self.split: Literal[\"dev\", \"test\"] = split self._parse_task() self.reset_seed(seed)","title":"__init__"},{"location":"api/tasks/#promptolution.tasks.classification_tasks.ClassificationTask.evaluate","text":"Evaluate a set of prompts using a given predictor. Parameters: Name Type Description Default prompts List [ str ] List of prompts to evaluate. required predictor BasePredictor Predictor to use for evaluation. required n_samples int Number of samples to use if subsampling. Defaults to 20. 20 subsample bool Whether to use subsampling. Defaults to True. True Returns: Type Description ndarray np.ndarray: Array of accuracy scores for each prompt. Source code in promptolution\\tasks\\classification_tasks.py def evaluate( self, prompts: List[str], predictor: BasePredictor, n_samples: int = 20, subsample: bool = True ) -> np.ndarray: \"\"\" Evaluate a set of prompts using a given predictor. Args: prompts (List[str]): List of prompts to evaluate. predictor (BasePredictor): Predictor to use for evaluation. n_samples (int, optional): Number of samples to use if subsampling. Defaults to 20. subsample (bool, optional): Whether to use subsampling. Defaults to True. Returns: np.ndarray: Array of accuracy scores for each prompt. \"\"\" if isinstance(prompts, str): prompts = [prompts] # Randomly select a subsample of n_samples if subsample: indices = np.random.choice(len(self.xs), n_samples, replace=False) else: indices = np.arange(len(self.xs)) xs_subsample = self.xs[indices] ys_subsample = self.ys[indices] # Make predictions on the subsample preds = predictor.predict(prompts, xs_subsample) # Calculate accuracy: number of correct predictions / total number of predictions per prompt return np.mean(preds == ys_subsample, axis=1)","title":"evaluate"},{"location":"api/tasks/#base-task","text":"","title":"Base Task"},{"location":"api/tasks/#promptolution.tasks.base_task.BaseTask","text":"Bases: ABC Abstract base class for tasks in the promptolution library. This class defines the interface that all concrete task implementations should follow. Methods: Name Description evaluate An abstract method that should be implemented by subclasses to evaluate prompts using a given predictor. Source code in promptolution\\tasks\\base_task.py class BaseTask(ABC): \"\"\" Abstract base class for tasks in the promptolution library. This class defines the interface that all concrete task implementations should follow. Methods: evaluate: An abstract method that should be implemented by subclasses to evaluate prompts using a given predictor. \"\"\" def __init__(self, *args, **kwargs): pass @abstractmethod def evaluate(self, prompts: List[str], predictor) -> np.ndarray: \"\"\" Abstract method to evaluate prompts using a given predictor. Args: prompts (List[str]): List of prompts to evaluate. predictor: The predictor to use for evaluation. Returns: np.ndarray: Array of evaluation scores for each prompt. Raises: NotImplementedError: If not implemented by a subclass. \"\"\" raise NotImplementedError","title":"BaseTask"},{"location":"api/tasks/#promptolution.tasks.base_task.BaseTask.evaluate","text":"Abstract method to evaluate prompts using a given predictor. Parameters: Name Type Description Default prompts List [ str ] List of prompts to evaluate. required predictor The predictor to use for evaluation. required Returns: Type Description ndarray np.ndarray: Array of evaluation scores for each prompt. Raises: Type Description NotImplementedError If not implemented by a subclass. Source code in promptolution\\tasks\\base_task.py @abstractmethod def evaluate(self, prompts: List[str], predictor) -> np.ndarray: \"\"\" Abstract method to evaluate prompts using a given predictor. Args: prompts (List[str]): List of prompts to evaluate. predictor: The predictor to use for evaluation. Returns: np.ndarray: Array of evaluation scores for each prompt. Raises: NotImplementedError: If not implemented by a subclass. \"\"\" raise NotImplementedError","title":"evaluate"},{"location":"api/tasks/#promptolution.tasks.base_task.DummyTask","text":"Bases: BaseTask A dummy task implementation for testing purposes. This task generates random evaluation scores for given prompts. Attributes: Name Type Description task_id str Identifier for the task, set to \"dummy\". dataset_json None Placeholder for dataset information. initial_population List [ str ] List of initial prompts. description str Description of the dummy task. xs ndarray Array of dummy input data. ys ndarray Array of dummy labels. classes List [ str ] List of possible class labels. Source code in promptolution\\tasks\\base_task.py class DummyTask(BaseTask): \"\"\" A dummy task implementation for testing purposes. This task generates random evaluation scores for given prompts. Attributes: task_id (str): Identifier for the task, set to \"dummy\". dataset_json (None): Placeholder for dataset information. initial_population (List[str]): List of initial prompts. description (str): Description of the dummy task. xs (np.ndarray): Array of dummy input data. ys (np.ndarray): Array of dummy labels. classes (List[str]): List of possible class labels. \"\"\" def __init__(self): self.task_id = \"dummy\" self.dataset_json = None self.initial_population = [\"Some\", \"initial\", \"prompts\", \"that\", \"will\", \"do\", \"the\", \"trick\"] self.description = \"This is a dummy task for testing purposes.\" self.xs = np.array([\"This is a test\", \"This is another test\", \"This is a third test\"]) self.ys = np.array([\"positive\", \"negative\", \"positive\"]) self.classes = [\"negative\", \"positive\"] def evaluate(self, prompts: List[str], predictor) -> np.ndarray: \"\"\" Generate random evaluation scores for the given prompts. Args: prompts (List[str]): List of prompts to evaluate. predictor: The predictor to use for evaluation (ignored in this implementation). Returns: np.ndarray: Array of random evaluation scores, one for each prompt. \"\"\" return np.array([np.random.rand()] * len(prompts))","title":"DummyTask"},{"location":"api/tasks/#promptolution.tasks.base_task.DummyTask.evaluate","text":"Generate random evaluation scores for the given prompts. Parameters: Name Type Description Default prompts List [ str ] List of prompts to evaluate. required predictor The predictor to use for evaluation (ignored in this implementation). required Returns: Type Description ndarray np.ndarray: Array of random evaluation scores, one for each prompt. Source code in promptolution\\tasks\\base_task.py def evaluate(self, prompts: List[str], predictor) -> np.ndarray: \"\"\" Generate random evaluation scores for the given prompts. Args: prompts (List[str]): List of prompts to evaluate. predictor: The predictor to use for evaluation (ignored in this implementation). Returns: np.ndarray: Array of random evaluation scores, one for each prompt. \"\"\" return np.array([np.random.rand()] * len(prompts))","title":"evaluate"},{"location":"api/tasks/#classification-tasks","text":"","title":"Classification Tasks"},{"location":"api/tasks/#promptolution.tasks.classification_tasks.ClassificationTask","text":"Bases: BaseTask A class representing a classification task in the promptolution library. This class handles the loading and management of classification datasets, as well as the evaluation of predictors on these datasets. Attributes: Name Type Description task_id str Unique identifier for the task. dataset_json Dict Dictionary containing dataset information. description Optional [ str ] Description of the task. initial_population Optional [ List [ str ]] Initial set of prompts. xs Optional [ ndarray ] Input data for the task. ys Optional [ ndarray ] Ground truth labels for the task. classes Optional [ List ] List of possible class labels. split Literal ['dev', 'test'] Dataset split to use. seed int Random seed for reproducibility. Inherits from BaseTask: The base class for tasks in the promptolution library. Source code in promptolution\\tasks\\classification_tasks.py class ClassificationTask(BaseTask): \"\"\" A class representing a classification task in the promptolution library. This class handles the loading and management of classification datasets, as well as the evaluation of predictors on these datasets. Attributes: task_id (str): Unique identifier for the task. dataset_json (Dict): Dictionary containing dataset information. description (Optional[str]): Description of the task. initial_population (Optional[List[str]]): Initial set of prompts. xs (Optional[np.ndarray]): Input data for the task. ys (Optional[np.ndarray]): Ground truth labels for the task. classes (Optional[List]): List of possible class labels. split (Literal[\"dev\", \"test\"]): Dataset split to use. seed (int): Random seed for reproducibility. Inherits from: BaseTask: The base class for tasks in the promptolution library. \"\"\" def __init__(self, task_id: str, dataset_json: Dict, seed: int = 42, split: Literal[\"dev\", \"test\"] = \"dev\"): \"\"\" Initialize the ClassificationTask. Args: task_id (str): Unique identifier for the task. dataset_json (Dict): Dictionary containing dataset information. seed (int, optional): Random seed for reproducibility. Defaults to 42. split (Literal[\"dev\", \"test\"], optional): Dataset split to use. Defaults to \"dev\". \"\"\" self.task_id: str = task_id self.dataset_json: Dict = dataset_json self.description: Optional[str] = None self.initial_population: Optional[List[str]] = None self.xs: Optional[np.ndarray] = np.array([]) self.ys: Optional[np.ndarray] = None self.classes: Optional[List] = None self.split: Literal[\"dev\", \"test\"] = split self._parse_task() self.reset_seed(seed) def __str__(self): return self.task_id def _parse_task(self): \"\"\" Parse the task data from the provided dataset JSON. This method loads the task description, classes, initial prompts, and the dataset split (dev or test) into the class attributes. \"\"\" task_path = Path(self.dataset_json[\"path\"]) self.description = self.dataset_json[\"description\"] self.classes = self.dataset_json[\"classes\"] with open(task_path / Path(self.dataset_json[\"init_prompts\"]), \"r\", encoding=\"utf-8\") as file: lines = file.readlines() self.initial_population = [line.strip() for line in lines] seed = Path(self.dataset_json[\"seed\"]) split = Path(self.split + \".txt\") with open(task_path / seed / split, \"r\", encoding=\"utf-8\") as file: lines = file.readlines() lines = [line.strip() for line in lines] xs = [] ys = [] for line in lines: x, y = line.split(\"\\t\") xs.append(x) ys.append(self.classes[int(y)]) self.xs = np.array(xs) self.ys = np.array(ys) def evaluate( self, prompts: List[str], predictor: BasePredictor, n_samples: int = 20, subsample: bool = True ) -> np.ndarray: \"\"\" Evaluate a set of prompts using a given predictor. Args: prompts (List[str]): List of prompts to evaluate. predictor (BasePredictor): Predictor to use for evaluation. n_samples (int, optional): Number of samples to use if subsampling. Defaults to 20. subsample (bool, optional): Whether to use subsampling. Defaults to True. Returns: np.ndarray: Array of accuracy scores for each prompt. \"\"\" if isinstance(prompts, str): prompts = [prompts] # Randomly select a subsample of n_samples if subsample: indices = np.random.choice(len(self.xs), n_samples, replace=False) else: indices = np.arange(len(self.xs)) xs_subsample = self.xs[indices] ys_subsample = self.ys[indices] # Make predictions on the subsample preds = predictor.predict(prompts, xs_subsample) # Calculate accuracy: number of correct predictions / total number of predictions per prompt return np.mean(preds == ys_subsample, axis=1) def reset_seed(self, seed: int = None): if seed is not None: self.seed = seed np.random.seed(self.seed)","title":"ClassificationTask"},{"location":"api/tasks/#promptolution.tasks.classification_tasks.ClassificationTask.__init__","text":"Initialize the ClassificationTask. Parameters: Name Type Description Default task_id str Unique identifier for the task. required dataset_json Dict Dictionary containing dataset information. required seed int Random seed for reproducibility. Defaults to 42. 42 split Literal ['dev', 'test'] Dataset split to use. Defaults to \"dev\". 'dev' Source code in promptolution\\tasks\\classification_tasks.py def __init__(self, task_id: str, dataset_json: Dict, seed: int = 42, split: Literal[\"dev\", \"test\"] = \"dev\"): \"\"\" Initialize the ClassificationTask. Args: task_id (str): Unique identifier for the task. dataset_json (Dict): Dictionary containing dataset information. seed (int, optional): Random seed for reproducibility. Defaults to 42. split (Literal[\"dev\", \"test\"], optional): Dataset split to use. Defaults to \"dev\". \"\"\" self.task_id: str = task_id self.dataset_json: Dict = dataset_json self.description: Optional[str] = None self.initial_population: Optional[List[str]] = None self.xs: Optional[np.ndarray] = np.array([]) self.ys: Optional[np.ndarray] = None self.classes: Optional[List] = None self.split: Literal[\"dev\", \"test\"] = split self._parse_task() self.reset_seed(seed)","title":"__init__"},{"location":"api/tasks/#promptolution.tasks.classification_tasks.ClassificationTask.evaluate","text":"Evaluate a set of prompts using a given predictor. Parameters: Name Type Description Default prompts List [ str ] List of prompts to evaluate. required predictor BasePredictor Predictor to use for evaluation. required n_samples int Number of samples to use if subsampling. Defaults to 20. 20 subsample bool Whether to use subsampling. Defaults to True. True Returns: Type Description ndarray np.ndarray: Array of accuracy scores for each prompt. Source code in promptolution\\tasks\\classification_tasks.py def evaluate( self, prompts: List[str], predictor: BasePredictor, n_samples: int = 20, subsample: bool = True ) -> np.ndarray: \"\"\" Evaluate a set of prompts using a given predictor. Args: prompts (List[str]): List of prompts to evaluate. predictor (BasePredictor): Predictor to use for evaluation. n_samples (int, optional): Number of samples to use if subsampling. Defaults to 20. subsample (bool, optional): Whether to use subsampling. Defaults to True. Returns: np.ndarray: Array of accuracy scores for each prompt. \"\"\" if isinstance(prompts, str): prompts = [prompts] # Randomly select a subsample of n_samples if subsample: indices = np.random.choice(len(self.xs), n_samples, replace=False) else: indices = np.arange(len(self.xs)) xs_subsample = self.xs[indices] ys_subsample = self.ys[indices] # Make predictions on the subsample preds = predictor.predict(prompts, xs_subsample) # Calculate accuracy: number of correct predictions / total number of predictions per prompt return np.mean(preds == ys_subsample, axis=1)","title":"evaluate"}]}